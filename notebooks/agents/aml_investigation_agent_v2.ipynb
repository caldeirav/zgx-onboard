{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç AML Investigation Agent (Gemma + GRPO)\n",
    "\n",
    "An autonomous AI Agent for investigating financial transaction graphs to identify money laundering patterns using FunctionGemma function-calling format and GRPO reinforcement learning.\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "| Component | Technology | Description |\n",
    "|-----------|------------|-------------|\n",
    "| **Base Model** | Gemma (Gemma-2-2B-IT) | XML-style function calling format |\n",
    "| **Fine-Tuning** | Unsloth + LoRA/QLoRA | 4-bit quantization, 2x faster training |\n",
    "| **Data Processing** | Polars | High-performance dataframes |\n",
    "| **Graph Analysis** | NetworkX | Transaction network traversal |\n",
    "| **Agent Framework** | LangGraph + MemorySaver | Stateful exploration with checkpointing |\n",
    "| **RL Training** | TRL GRPOTrainer | Group Relative Policy Optimization |\n",
    "| **Observability** | MLflow Tracing | Full agent trace logging |\n",
    "| **Evaluation** | Gemini LLM-as-Judge | Strategy quality scoring |\n",
    "\n",
    "## Evaluation Flow\n",
    "\n",
    "This notebook implements a **three-stage evaluation** to measure training impact:\n",
    "\n",
    "| Stage | Model State | Purpose |\n",
    "|-------|-------------|---------|\n",
    "| **1. Baseline** | Pre-trained Gemma-2-2B-IT | Measure zero-shot performance |\n",
    "| **2. Post-SFT** | After Supervised Fine-Tuning | Measure SFT improvement |\n",
    "| **3. Post-GRPO** | After RL Training | Measure GRPO improvement |\n",
    "\n",
    "## Gemma Tool Format (with Internal Reasoning)\n",
    "\n",
    "```xml\n",
    "<thinking>\n",
    "[Internal reasoning about investigation strategy]\n",
    "</thinking>\n",
    "<start_function_call>call:function_name{param: value}</start_function_call>\n",
    "<start_function_output>call:function_name{result: value}</start_function_output>\n",
    "```\n",
    "\n",
    "## Investigation Tools\n",
    "\n",
    "| Tool | Description |\n",
    "|------|-------------|\n",
    "| `get_account_summary` | Get account metadata and risk assessment |\n",
    "| `get_recent_transactions` | Get top-5 recent transaction flows |\n",
    "| `check_sanctions_list` | Verify against OFAC watchlist |\n",
    "| `submit_sar` | Terminal action - Submit Suspicious Activity Report |\n",
    "\n",
    "## Win Condition\n",
    "`submit_sar` on an entity that is **both sanctioned AND reachable via a laundering path** from the seed account.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# AML INVESTIGATION AGENT - Setup & Dependencies\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Any, Tuple, Optional, Annotated, Literal, TypedDict\n",
    "import operator\n",
    "from pathlib import Path\n",
    "\n",
    "# Numerical & Data Processing\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# ML & Deep Learning\n",
    "import torch\n",
    "\n",
    "# Environment\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Model Configuration\n",
    "MODEL_NAME = \"google/gemma-2-2b-it\"  # FunctionGemma compatible base\n",
    "GEMINI_MODEL = \"gemini-2.0-flash\"    # LLM-as-Judge\n",
    "\n",
    "# Agent Configuration\n",
    "MAX_STEPS = 50                        # Max steps per investigation\n",
    "MAX_HISTORY_TURNS = 6                 # Conversation history limit\n",
    "\n",
    "# Training Configuration\n",
    "SFT_EPOCHS = 3\n",
    "SFT_LEARNING_RATE = 2e-4\n",
    "GRPO_EPOCHS = 1\n",
    "GRPO_LEARNING_RATE = 5e-6\n",
    "\n",
    "# LoRA Configuration (per design doc Section 3.3)\n",
    "LORA_R = 32                           # Higher rank for complex reasoning\n",
    "LORA_ALPHA = 64\n",
    "LORA_TARGET_MODULES = [\n",
    "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "    \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "]\n",
    "\n",
    "# Evaluation Configuration\n",
    "EVAL_EPISODES = 10                    # Episodes per evaluation stage\n",
    "\n",
    "# Random Seed\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Paths (relative to notebook location)\n",
    "# Notebook is at: notebooks/agents/aml_investigation_agent_v2.ipynb\n",
    "# Project root is 2 levels up\n",
    "NOTEBOOK_DIR = Path(\".\").resolve()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent.parent  # Go up from notebooks/agents to project root\n",
    "DATA_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"outputs\"\n",
    "\n",
    "# Ensure directories exist\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dataset Selection\n",
    "DATASET_SIZE = \"Small\"   # Options: \"Small\", \"Medium\", \"Large\"\n",
    "DATASET_PREFIX = \"LI\"    # Options: \"LI\" (Low Illicit), \"HI\" (High Illicit)\n",
    "\n",
    "# ============================================================================\n",
    "# RESULTS STORAGE - For comparison across training stages\n",
    "# ============================================================================\n",
    "\n",
    "evaluation_results = {\n",
    "    \"baseline\": None,\n",
    "    \"post_sft\": None,\n",
    "    \"post_grpo\": None,\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üîç AML INVESTIGATION AGENT - Configuration\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Model:           {MODEL_NAME}\")\n",
    "print(f\"  Judge:           {GEMINI_MODEL}\")\n",
    "print(f\"  Dataset:         {DATASET_PREFIX}-{DATASET_SIZE}\")\n",
    "print(f\"  Max Steps:       {MAX_STEPS}\")\n",
    "print(f\"  Eval Episodes:   {EVAL_EPISODES}\")\n",
    "print(f\"  LoRA Rank:       {LORA_R}\")\n",
    "print(f\"  LoRA Alpha:      {LORA_ALPHA}\")\n",
    "print(f\"  Random Seed:     {RANDOM_SEED}\")\n",
    "print(f\"  Project Root:    {PROJECT_ROOT}\")\n",
    "print(f\"  Models Dir:      {MODELS_DIR}\")\n",
    "print(f\"  GPU Available:   {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU Device:      {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  GPU Memory:      {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading with Polars\n",
    "\n",
    "Download the IBM AML dataset from Kaggle (if not already present) and load using Polars for high-performance data manipulation.\n",
    "\n",
    "**Dataset**: [IBM Transactions for Anti Money Laundering (AML)](https://www.kaggle.com/datasets/ealtman2019/ibm-transactions-for-anti-money-laundering-aml/data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DOWNLOAD DATASET FROM KAGGLE - IBM AML Transactions Dataset\n",
    "# Dataset: https://www.kaggle.com/datasets/ealtman2019/ibm-transactions-for-anti-money-laundering-aml\n",
    "# ============================================================================\n",
    "\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "# Kaggle dataset identifier\n",
    "KAGGLE_DATASET = \"ealtman2019/ibm-transactions-for-anti-money-laundering-aml\"\n",
    "\n",
    "# Check if required files exist\n",
    "trans_file = DATA_DIR / f\"{DATASET_PREFIX}-{DATASET_SIZE}_Trans.csv\"\n",
    "accounts_file = DATA_DIR / f\"{DATASET_PREFIX}-{DATASET_SIZE}_accounts.csv\"\n",
    "patterns_file = DATA_DIR / f\"{DATASET_PREFIX}-{DATASET_SIZE}_Patterns.txt\"\n",
    "\n",
    "files_exist = trans_file.exists() and accounts_file.exists() and patterns_file.exists()\n",
    "\n",
    "if files_exist:\n",
    "    print(f\"‚úì Dataset already exists at {DATA_DIR}\")\n",
    "    print(f\"  - Transactions: {trans_file.name}\")\n",
    "    print(f\"  - Accounts: {accounts_file.name}\")\n",
    "    print(f\"  - Patterns: {patterns_file.name}\")\n",
    "else:\n",
    "    print(f\"üì• Dataset not found. Downloading from Kaggle...\")\n",
    "    print(f\"   Dataset: {KAGGLE_DATASET}\")\n",
    "    \n",
    "    # Ensure data directory exists\n",
    "    DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Import and authenticate Kaggle API\n",
    "        from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "        \n",
    "        api = KaggleApi()\n",
    "        api.authenticate()\n",
    "        \n",
    "        print(f\"   ‚úì Kaggle API authenticated\")\n",
    "        \n",
    "        # Download dataset\n",
    "        print(f\"   Downloading dataset to {DATA_DIR}...\")\n",
    "        api.dataset_download_files(\n",
    "            dataset=KAGGLE_DATASET,\n",
    "            path=str(DATA_DIR),\n",
    "            unzip=True,\n",
    "            quiet=False\n",
    "        )\n",
    "        \n",
    "        print(f\"   ‚úì Download complete!\")\n",
    "        \n",
    "        # List downloaded files\n",
    "        print(f\"\\n   Downloaded files:\")\n",
    "        for f in sorted(DATA_DIR.glob(\"*\")):\n",
    "            size_mb = f.stat().st_size / (1024 * 1024)\n",
    "            print(f\"     - {f.name} ({size_mb:.1f} MB)\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"   ‚ùå Kaggle package not installed.\")\n",
    "        print(\"   Run: pip install kaggle\")\n",
    "        print(\"   Then set up ~/.kaggle/kaggle.json with your API credentials\")\n",
    "        raise ImportError(\"Please install kaggle package: pip install kaggle\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error downloading dataset: {e}\")\n",
    "        print(\"\\n   Manual download instructions:\")\n",
    "        print(f\"   1. Visit: https://www.kaggle.com/datasets/{KAGGLE_DATASET}\")\n",
    "        print(f\"   2. Download and extract to: {DATA_DIR}\")\n",
    "        raise\n",
    "\n",
    "# Verify files exist after download\n",
    "assert trans_file.exists(), f\"Transaction file not found: {trans_file}\"\n",
    "assert accounts_file.exists(), f\"Accounts file not found: {accounts_file}\"\n",
    "assert patterns_file.exists(), f\"Patterns file not found: {patterns_file}\"\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"‚úì DATASET READY: {DATASET_PREFIX}-{DATASET_SIZE}\")\n",
    "print(f\"{'=' * 60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA LOADING - IBM AML Dataset with Polars\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üìä Loading IBM AML Dataset with Polars...\")\n",
    "\n",
    "# Load transactions\n",
    "raw_trans_pl = pl.read_csv(\n",
    "    trans_file,\n",
    "    new_columns=[\n",
    "        'timestamp', 'from_bank', 'from_account', 'to_bank', 'to_account',\n",
    "        'amount_received', 'receiving_currency', 'amount_paid',\n",
    "        'payment_currency', 'payment_format', 'is_laundering'\n",
    "    ],\n",
    "    skip_rows=1\n",
    ")\n",
    "print(f\"‚úì Loaded {len(raw_trans_pl):,} transactions\")\n",
    "\n",
    "# Load accounts\n",
    "raw_accounts_pl = pl.read_csv(accounts_file)\n",
    "print(f\"‚úì Loaded {len(raw_accounts_pl):,} accounts\")\n",
    "\n",
    "# Process transactions - Create unique account IDs\n",
    "transactions_pl = raw_trans_pl.with_columns([\n",
    "    (pl.col('from_bank').cast(pl.Utf8) + '-' + pl.col('from_account').cast(pl.Utf8)).alias('from_account_id'),\n",
    "    (pl.col('to_bank').cast(pl.Utf8) + '-' + pl.col('to_account').cast(pl.Utf8)).alias('to_account_id'),\n",
    "    (pl.lit('TXN-') + pl.arange(0, pl.len()).cast(pl.Utf8)).alias('transaction_id'),\n",
    "    pl.col('is_laundering').cast(pl.Int32),\n",
    "]).select([\n",
    "    'transaction_id',\n",
    "    pl.col('from_account_id').alias('from_account'),\n",
    "    pl.col('to_account_id').alias('to_account'),\n",
    "    pl.col('amount_received').alias('amount'),\n",
    "    pl.col('receiving_currency').alias('currency'),\n",
    "    'timestamp', 'is_laundering', 'payment_format',\n",
    "])\n",
    "\n",
    "# Identify laundering destinations for sanctioned marking\n",
    "laundering_dests = set(\n",
    "    transactions_pl.filter(pl.col('is_laundering') == 1)['to_account'].unique().to_list()\n",
    ")\n",
    "\n",
    "# Process accounts - Add risk scores and sanctioned flags\n",
    "accounts_pl = raw_accounts_pl.rename({\n",
    "    'Bank Name': 'bank_name', 'Bank ID': 'bank_id',\n",
    "    'Account Number': 'account_number', 'Entity ID': 'entity_id', 'Entity Name': 'entity_name'\n",
    "}).with_columns([\n",
    "    (pl.col('bank_id').cast(pl.Utf8) + '-' + pl.col('account_number').cast(pl.Utf8)).alias('account_id'),\n",
    "    pl.when(pl.col('entity_name').str.contains('Corporation')).then(pl.lit('Corporate'))\n",
    "        .when(pl.col('entity_name').str.contains('Partnership')).then(pl.lit('Partnership'))\n",
    "        .when(pl.col('entity_name').str.contains('Sole Proprietorship')).then(pl.lit('Individual'))\n",
    "        .otherwise(pl.lit('Unknown')).alias('account_type'),\n",
    "])\n",
    "\n",
    "# Add sanctioned flag (30% of laundering destinations) and risk scores\n",
    "account_ids = accounts_pl['account_id'].to_list()\n",
    "accounts_pl = accounts_pl.with_columns([\n",
    "    pl.Series('is_sanctioned', [acc in laundering_dests and random.random() < 0.3 for acc in account_ids]),\n",
    "    pl.Series('risk_score', [round(random.uniform(0.1, 0.9), 2) for _ in range(len(account_ids))]),\n",
    "]).select(['account_id', 'bank_id', 'bank_name', 'entity_id', 'entity_name', 'account_type', 'risk_score', 'is_sanctioned'])\n",
    "\n",
    "# Convert to pandas for NetworkX\n",
    "transactions_df = transactions_pl.to_pandas()\n",
    "accounts_df = accounts_pl.to_pandas()\n",
    "\n",
    "# Summary\n",
    "n_accounts, n_transactions = len(accounts_df), len(transactions_df)\n",
    "n_laundering = int(transactions_df['is_laundering'].sum())\n",
    "n_sanctioned = int(accounts_df['is_sanctioned'].sum())\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"üìä DATASET SUMMARY: {DATASET_PREFIX}-{DATASET_SIZE}\")\n",
    "print(f\"{'=' * 60}\")\n",
    "print(f\"  Accounts:             {n_accounts:>12,}\")\n",
    "print(f\"  Transactions:         {n_transactions:>12,}\")\n",
    "print(f\"  Laundering Txns:      {n_laundering:>12,} ({n_laundering/n_transactions*100:.2f}%)\")\n",
    "print(f\"  Sanctioned Accounts:  {n_sanctioned:>12,}\")\n",
    "print(f\"{'=' * 60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PARSE LAUNDERING PATTERNS - Extract Pattern Seeds for Training\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class LaunderingPattern:\n",
    "    \"\"\"Represents a single money laundering pattern from the dataset.\"\"\"\n",
    "    pattern_type: str\n",
    "    pattern_info: str\n",
    "    transactions: List[dict]\n",
    "    accounts_involved: set\n",
    "    \n",
    "    @property\n",
    "    def seed_account(self) -> str:\n",
    "        return self.transactions[0].get('from_account', '') if self.transactions else ''\n",
    "    \n",
    "    @property\n",
    "    def terminal_account(self) -> str:\n",
    "        return self.transactions[-1].get('to_account', '') if self.transactions else ''\n",
    "    \n",
    "    @property\n",
    "    def total_amount(self) -> float:\n",
    "        return sum(t.get('amount', 0) for t in self.transactions)\n",
    "    \n",
    "    @property\n",
    "    def hop_count(self) -> int:\n",
    "        return len(self.transactions)\n",
    "\n",
    "\n",
    "def parse_patterns_file(filepath: Path) -> List[LaunderingPattern]:\n",
    "    \"\"\"Parse patterns file to extract laundering patterns.\"\"\"\n",
    "    patterns = []\n",
    "    current_pattern = None\n",
    "    \n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            \n",
    "            if line.startswith('BEGIN LAUNDERING ATTEMPT'):\n",
    "                match = re.match(r'BEGIN LAUNDERING ATTEMPT - (\\w+(?:-\\w+)?):?\\s*(.*)', line)\n",
    "                if match:\n",
    "                    current_pattern = LaunderingPattern(\n",
    "                        pattern_type=match.group(1),\n",
    "                        pattern_info=match.group(2).strip() if match.group(2) else \"\",\n",
    "                        transactions=[], accounts_involved=set()\n",
    "                    )\n",
    "            \n",
    "            elif line.startswith('END LAUNDERING ATTEMPT'):\n",
    "                if current_pattern and current_pattern.transactions:\n",
    "                    patterns.append(current_pattern)\n",
    "                current_pattern = None\n",
    "            \n",
    "            elif current_pattern and line and not line.startswith('BEGIN') and not line.startswith('END'):\n",
    "                parts = line.split(',')\n",
    "                if len(parts) >= 7:\n",
    "                    try:\n",
    "                        from_account = f\"{parts[1].strip()}-{parts[2].strip()}\"\n",
    "                        to_account = f\"{parts[3].strip()}-{parts[4].strip()}\"\n",
    "                        amount = float(parts[5].strip())\n",
    "                        \n",
    "                        current_pattern.transactions.append({\n",
    "                            'timestamp': parts[0].strip(),\n",
    "                            'from_account': from_account, 'to_account': to_account,\n",
    "                            'amount': amount, 'currency': parts[6].strip(),\n",
    "                        })\n",
    "                        current_pattern.accounts_involved.add(from_account)\n",
    "                        current_pattern.accounts_involved.add(to_account)\n",
    "                    except (ValueError, IndexError):\n",
    "                        pass\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "\n",
    "# Parse patterns\n",
    "laundering_patterns = parse_patterns_file(patterns_file)\n",
    "\n",
    "# Statistics\n",
    "pattern_types = {}\n",
    "for p in laundering_patterns:\n",
    "    pattern_types[p.pattern_type] = pattern_types.get(p.pattern_type, 0) + 1\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"üîó LAUNDERING PATTERNS PARSED\")\n",
    "print(f\"{'=' * 60}\")\n",
    "print(f\"  Total Patterns: {len(laundering_patterns):,}\")\n",
    "for ptype, count in sorted(pattern_types.items(), key=lambda x: -x[1]):\n",
    "    print(f\"    - {ptype:<20} {count:>6,}\")\n",
    "print(f\"{'=' * 60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Financial Environment with MLflow-Instrumented Tools\n",
    "\n",
    "Build the transaction graph with NetworkX and create the `FinancialEnvironment` class with MLflow-traced tool functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FINANCIAL ENVIRONMENT - NetworkX Graph with MLflow-Traced Tools\n",
    "# ============================================================================\n",
    "\n",
    "import mlflow\n",
    "\n",
    "mlflow.set_experiment(\"AML_Investigation_Agent_v2\")\n",
    "print(\"‚úì MLflow experiment: AML_Investigation_Agent_v2\")\n",
    "\n",
    "\n",
    "@dataclass \n",
    "class FinancialEnvironment:\n",
    "    \"\"\"Financial investigation environment with path-validated SAR evaluation.\"\"\"\n",
    "    graph: nx.DiGraph = field(default_factory=nx.DiGraph)\n",
    "    accounts: Dict[str, dict] = field(default_factory=dict)\n",
    "    laundering_targets: List[str] = field(default_factory=list)\n",
    "    all_sanctioned: set = field(default_factory=set)\n",
    "    laundering_destinations: set = field(default_factory=set)\n",
    "    transitive_illicit: set = field(default_factory=set)\n",
    "    current_start_account: str = \"\"\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataframes(cls, transactions_df: pd.DataFrame, accounts_df: pd.DataFrame) -> 'FinancialEnvironment':\n",
    "        env = cls()\n",
    "        \n",
    "        for _, row in accounts_df.iterrows():\n",
    "            env.accounts[row['account_id']] = row.to_dict()\n",
    "            env.graph.add_node(row['account_id'], **row.to_dict())\n",
    "        \n",
    "        for _, row in transactions_df.iterrows():\n",
    "            env.graph.add_edge(\n",
    "                row['from_account'], row['to_account'],\n",
    "                transaction_id=row['transaction_id'], amount=row['amount'],\n",
    "                currency=row.get('currency', 'USD'), timestamp=row['timestamp'],\n",
    "                is_laundering=row['is_laundering']\n",
    "            )\n",
    "        \n",
    "        env.all_sanctioned = set(accounts_df[accounts_df['is_sanctioned']]['account_id'])\n",
    "        laundering_txns = transactions_df[transactions_df['is_laundering'] == 1]\n",
    "        env.laundering_destinations = set(laundering_txns['to_account'].unique())\n",
    "        env._compute_transitive_illicit()\n",
    "        env.laundering_targets = list(env.all_sanctioned & env.laundering_destinations)\n",
    "        \n",
    "        return env\n",
    "    \n",
    "    def _compute_transitive_illicit(self):\n",
    "        self.transitive_illicit = set()\n",
    "        laundering_sources = set()\n",
    "        for u, v, data in self.graph.edges(data=True):\n",
    "            if data.get('is_laundering', 0) == 1:\n",
    "                laundering_sources.add(u)\n",
    "                self.transitive_illicit.add(u)\n",
    "                self.transitive_illicit.add(v)\n",
    "        \n",
    "        for source in laundering_sources:\n",
    "            visited = {source}\n",
    "            queue = [source]\n",
    "            while queue:\n",
    "                node = queue.pop(0)\n",
    "                for neighbor in self.graph.successors(node):\n",
    "                    edge_data = self.graph.edges[node, neighbor]\n",
    "                    if edge_data.get('is_laundering', 0) == 1 and neighbor not in visited:\n",
    "                        visited.add(neighbor)\n",
    "                        self.transitive_illicit.add(neighbor)\n",
    "                        queue.append(neighbor)\n",
    "    \n",
    "    def is_on_laundering_path(self, entity_id: str, max_depth: int = 10) -> bool:\n",
    "        if not self.current_start_account:\n",
    "            return False\n",
    "        try:\n",
    "            for path in nx.all_simple_paths(self.graph, self.current_start_account, entity_id, cutoff=max_depth):\n",
    "                if all(self.graph.edges[path[i], path[i+1]].get('is_laundering', 0) == 1 for i in range(len(path)-1)):\n",
    "                    return True\n",
    "            return False\n",
    "        except (nx.NetworkXNoPath, nx.NodeNotFound):\n",
    "            return False\n",
    "    \n",
    "    @mlflow.trace(span_type=\"TOOL\")\n",
    "    def get_account_summary(self, account_id: str) -> dict:\n",
    "        if account_id not in self.accounts:\n",
    "            return {\"error\": f\"Account {account_id} not found\"}\n",
    "        acc = self.accounts[account_id]\n",
    "        return {\n",
    "            \"account_id\": account_id, \"account_type\": acc.get('account_type', 'Unknown'),\n",
    "            \"entity_name\": acc.get('entity_name', 'Unknown'), \"bank_name\": acc.get('bank_name', 'Unknown'),\n",
    "            \"risk_score\": round(acc.get('risk_score', 0), 2), \"is_sanctioned\": acc.get('is_sanctioned', False),\n",
    "            \"transitive_illicit\": account_id in self.transitive_illicit,\n",
    "        }\n",
    "    \n",
    "    @mlflow.trace(span_type=\"TOOL\")\n",
    "    def get_recent_transactions(self, account_id: str, direction: str = \"outgoing\", limit: int = 5) -> List[dict]:\n",
    "        if account_id not in self.graph:\n",
    "            return []\n",
    "        edges = list(self.graph.out_edges(account_id, data=True) if direction == \"outgoing\" \n",
    "                     else self.graph.in_edges(account_id, data=True))\n",
    "        edges = sorted(edges, key=lambda e: e[2].get('amount', 0), reverse=True)[:limit]\n",
    "        \n",
    "        results = []\n",
    "        for edge in edges:\n",
    "            target = edge[1] if direction == \"outgoing\" else edge[0]\n",
    "            results.append({\n",
    "                \"counterparty\": target, \"amount\": round(edge[2].get('amount', 0), 2),\n",
    "                \"currency\": edge[2].get('currency', 'USD'), \"is_laundering\": edge[2].get('is_laundering', 0),\n",
    "                \"high_risk_indicator\": target in self.transitive_illicit,\n",
    "            })\n",
    "        return results\n",
    "    \n",
    "    @mlflow.trace(span_type=\"TOOL\")\n",
    "    def check_sanctions_list(self, entity_id: str) -> dict:\n",
    "        is_sanctioned = entity_id in self.all_sanctioned\n",
    "        return {\"entity_id\": entity_id, \"on_sanctions_list\": is_sanctioned, \"list_type\": \"OFAC SDN\" if is_sanctioned else None}\n",
    "    \n",
    "    @mlflow.trace(span_type=\"TOOL\")\n",
    "    def submit_sar(self, entity_id: str, reason: str) -> dict:\n",
    "        is_sanctioned = entity_id in self.all_sanctioned\n",
    "        is_primary = entity_id in self.laundering_targets\n",
    "        on_path = self.is_on_laundering_path(entity_id)\n",
    "        correct = is_primary or (is_sanctioned and on_path)\n",
    "        \n",
    "        if is_primary:\n",
    "            eval_reason = \"PRIMARY_TARGET: sanctioned + receives laundering directly\"\n",
    "        elif is_sanctioned and on_path:\n",
    "            eval_reason = f\"VALID: sanctioned + on laundering path from {self.current_start_account}\"\n",
    "        elif is_sanctioned:\n",
    "            eval_reason = \"INVALID: sanctioned but NOT on laundering path from start\"\n",
    "        else:\n",
    "            eval_reason = \"INVALID: entity is not sanctioned\"\n",
    "        \n",
    "        return {\n",
    "            \"entity_id\": entity_id, \"reason\": reason, \"report_id\": f\"SAR-{uuid.uuid4().hex[:8].upper()}\",\n",
    "            \"correct_identification\": correct, \"is_sanctioned\": is_sanctioned,\n",
    "            \"is_primary_target\": is_primary, \"on_laundering_path\": on_path, \"evaluation_reason\": eval_reason,\n",
    "        }\n",
    "    \n",
    "    def reset_investigation(self, start_account: str):\n",
    "        self.current_start_account = start_account\n",
    "\n",
    "\n",
    "# Build environment\n",
    "env = FinancialEnvironment.from_dataframes(transactions_df, accounts_df)\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"üè¶ FINANCIAL ENVIRONMENT BUILT\")\n",
    "print(f\"{'=' * 60}\")\n",
    "print(f\"  Graph Nodes:          {env.graph.number_of_nodes():>12,}\")\n",
    "print(f\"  Graph Edges:          {env.graph.number_of_edges():>12,}\")\n",
    "print(f\"  Sanctioned Accounts:  {len(env.all_sanctioned):>12,}\")\n",
    "print(f\"  Primary Targets:      {len(env.laundering_targets):>12,}\")\n",
    "print(f\"  Transitive Illicit:   {len(env.transitive_illicit):>12,}\")\n",
    "print(f\"{'=' * 60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. FunctionGemma Tool Format and Parsing\n",
    "\n",
    "Implement the FunctionGemma XML-style function calling format with internal reasoning (`<thinking>` tags) and robust parsing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FUNCTIONGEMMA TOOL FORMAT & PARSING (with Internal Reasoning)\n",
    "# Ref: https://ai.google.dev/gemma/docs/functiongemma\n",
    "# Ref: https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/FunctionGemma_(270M).ipynb\n",
    "# ============================================================================\n",
    "\n",
    "TOOL_DECLARATIONS = \"\"\"<start_function_declaration>\n",
    "{\"name\": \"get_account_summary\", \"description\": \"Get account metadata and risk assessment for an account ID\",\n",
    " \"parameters\": {\"type\": \"object\", \"properties\": {\"account_id\": {\"type\": \"string\", \"description\": \"The unique account identifier\"}}, \"required\": [\"account_id\"]}}\n",
    "</start_function_declaration>\n",
    "\n",
    "<start_function_declaration>\n",
    "{\"name\": \"get_recent_transactions\", \"description\": \"Get the top-5 recent transactions by amount for an account\",\n",
    " \"parameters\": {\"type\": \"object\", \"properties\": {\"account_id\": {\"type\": \"string\", \"description\": \"The account to get transactions for\"}, \"direction\": {\"type\": \"string\", \"enum\": [\"outgoing\", \"incoming\"]}}, \"required\": [\"account_id\"]}}\n",
    "</start_function_declaration>\n",
    "\n",
    "<start_function_declaration>\n",
    "{\"name\": \"check_sanctions_list\", \"description\": \"Check if an entity is on the OFAC sanctions list\",\n",
    " \"parameters\": {\"type\": \"object\", \"properties\": {\"entity_id\": {\"type\": \"string\", \"description\": \"The account/entity ID to check\"}}, \"required\": [\"entity_id\"]}}\n",
    "</start_function_declaration>\n",
    "\n",
    "<start_function_declaration>\n",
    "{\"name\": \"submit_sar\", \"description\": \"Submit a Suspicious Activity Report - TERMINAL ACTION\",\n",
    " \"parameters\": {\"type\": \"object\", \"properties\": {\"entity_id\": {\"type\": \"string\", \"description\": \"The sanctioned entity to report\"}, \"reason\": {\"type\": \"string\", \"description\": \"Justification for the SAR\"}}, \"required\": [\"entity_id\", \"reason\"]}}\n",
    "</start_function_declaration>\"\"\"\n",
    "\n",
    "# Investigation prompt with internal reasoning (FunctionGemma doesn't support thinking by default)\n",
    "INVESTIGATION_PROMPT = \"\"\"You are an expert AML investigator. Investigate financial transaction networks to identify money laundering.\n",
    "\n",
    "STRATEGY:\n",
    "1. START: Get account summary of seed account\n",
    "2. EXPLORE: Get recent transactions to find money flows\n",
    "3. FOLLOW: Investigate high-amount counterparties\n",
    "4. VERIFY: Check sanctions list for suspicious entities\n",
    "5. REPORT: Submit SAR only after confirming sanctioned status\n",
    "\n",
    "{tool_declarations}\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "You MUST first provide your internal reasoning in <thinking> tags, then make a function call.\n",
    "\n",
    "<thinking>\n",
    "[Your reasoning about what to do next and why]\n",
    "</thinking>\n",
    "<start_function_call>call:function_name{{param: value}}</start_function_call>\n",
    "\"\"\"\n",
    "\n",
    "# Tool mapping to environment functions\n",
    "TOOL_MAPPING = {\n",
    "    \"get_account_summary\": lambda args: env.get_account_summary(args.get(\"account_id\", \"\")),\n",
    "    \"get_recent_transactions\": lambda args: env.get_recent_transactions(args.get(\"account_id\", \"\"), args.get(\"direction\", \"outgoing\")),\n",
    "    \"check_sanctions_list\": lambda args: env.check_sanctions_list(args.get(\"entity_id\", \"\")),\n",
    "    \"submit_sar\": lambda args: env.submit_sar(args.get(\"entity_id\", \"\"), args.get(\"reason\", \"Suspicious activity\")),\n",
    "}\n",
    "\n",
    "VALID_TOOLS = set(TOOL_MAPPING.keys())\n",
    "\n",
    "# Tool name corrections for hallucination handling\n",
    "TOOL_NAME_CORRECTIONS = {\n",
    "    \"get_account\": \"get_account_summary\",\n",
    "    \"get_transactions\": \"get_recent_transactions\",\n",
    "    \"check_sanctions\": \"check_sanctions_list\",\n",
    "    \"submit_report\": \"submit_sar\",\n",
    "}\n",
    "\n",
    "def harden_tool_call(call: dict) -> dict:\n",
    "    \"\"\"Correct common tool name hallucinations.\"\"\"\n",
    "    if not call:\n",
    "        return call\n",
    "    name = call.get(\"name\", \"\").lower().strip()\n",
    "    return {\"name\": TOOL_NAME_CORRECTIONS.get(name, name), \"arguments\": call.get(\"arguments\", {})}\n",
    "\n",
    "def extract_thinking(text: str) -> Optional[str]:\n",
    "    \"\"\"Extract internal reasoning from <thinking> tags.\"\"\"\n",
    "    match = re.search(r\"<thinking>(.*?)</thinking>\", text, re.DOTALL | re.IGNORECASE)\n",
    "    return match.group(1).strip() if match else None\n",
    "\n",
    "def extract_function_call(text: str) -> Optional[dict]:\n",
    "    \"\"\"Extract FunctionGemma-style function call from model output.\"\"\"\n",
    "    _ = extract_thinking(text)  # Extract thinking for logging\n",
    "    match = re.search(r\"<start_function_call>call:(\\w+)\\{(.*?)\\}</start_function_call>\", text, re.DOTALL)\n",
    "    if match:\n",
    "        name = match.group(1)\n",
    "        args = {}\n",
    "        for arg_match in re.finditer(r\"(\\w+):\\s*([^,}]+)\", match.group(2)):\n",
    "            args[arg_match.group(1)] = arg_match.group(2).strip().strip('\"\\'')\n",
    "        return harden_tool_call({\"name\": name, \"arguments\": args})\n",
    "    return None\n",
    "\n",
    "def format_function_output(tool_name: str, result: Any) -> str:\n",
    "    \"\"\"Format tool result in FunctionGemma output format.\"\"\"\n",
    "    return f\"<start_function_output>call:{tool_name}{{{json.dumps(result, default=str)}}}</start_function_output>\"\n",
    "\n",
    "def execute_tool_call(call: dict) -> Tuple[str, Any]:\n",
    "    \"\"\"Execute a tool call and return (tool_name, result).\"\"\"\n",
    "    if not call:\n",
    "        return (\"error\", {\"error\": \"No valid tool call\"})\n",
    "    tool_name = call.get(\"name\", \"\")\n",
    "    if tool_name not in TOOL_MAPPING:\n",
    "        return (\"error\", {\"error\": f\"Unknown tool: {tool_name}\"})\n",
    "    try:\n",
    "        return (tool_name, TOOL_MAPPING[tool_name](call.get(\"arguments\", {})))\n",
    "    except Exception as e:\n",
    "        return (\"error\", {\"error\": str(e)})\n",
    "\n",
    "print(\"‚úì FunctionGemma tool format configured (with <thinking> support)\")\n",
    "print(f\"  Available tools: {list(VALID_TOOLS)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Base Model with Unsloth\n",
    "\n",
    "Load FunctionGemma (Gemma-2-2B-IT) with Unsloth for optimized 4-bit inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LOAD MODEL WITH UNSLOTH - Optimized 4-bit Loading\n",
    "# ============================================================================\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "print(\"üì• Loading model with Unsloth...\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=8192,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"cuda:0\",  # Explicit GPU mapping to prevent CPU offload\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"ü§ñ MODEL LOADED\")\n",
    "print(f\"{'=' * 60}\")\n",
    "print(f\"  Model:            {MODEL_NAME}\")\n",
    "print(f\"  Parameters:       {model.num_parameters():,}\")\n",
    "print(f\"  Max Seq Length:   8,192\")\n",
    "print(f\"  Quantization:     4-bit\")\n",
    "print(f\"  Device:           {next(model.parameters()).device}\")\n",
    "print(f\"{'=' * 60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Agent State and Execution Logic\n",
    "\n",
    "Define the `InvestigationState`, `InvestigationEpisode`, and agent execution functions with reward calculation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# AGENT STATE AND EXECUTION LOGIC\n",
    "# ============================================================================\n",
    "\n",
    "class InvestigationState(TypedDict):\n",
    "    \"\"\"Complete state for the investigation agent.\"\"\"\n",
    "    start_account: str\n",
    "    accounts_analyzed: Dict[str, dict]\n",
    "    entities_checked: Dict[str, bool]\n",
    "    risk_indicators: List[str]\n",
    "    investigation_path: List[str]\n",
    "    total_amount_traced: float\n",
    "    current_strategy: str\n",
    "    messages: List[dict]\n",
    "    steps: List[dict]\n",
    "    step_count: int\n",
    "    terminated: bool\n",
    "    success: bool\n",
    "    final_result: dict\n",
    "\n",
    "\n",
    "def create_initial_state(start_account: str) -> InvestigationState:\n",
    "    return InvestigationState(\n",
    "        start_account=start_account, accounts_analyzed={}, entities_checked={},\n",
    "        risk_indicators=[], investigation_path=[], total_amount_traced=0.0,\n",
    "        current_strategy=\"explore\", messages=[], steps=[], step_count=0,\n",
    "        terminated=False, success=False, final_result={},\n",
    "    )\n",
    "\n",
    "\n",
    "def build_prompt(state: InvestigationState) -> str:\n",
    "    system_prompt = INVESTIGATION_PROMPT.format(tool_declarations=TOOL_DECLARATIONS)\n",
    "    prompt = f\"<start_of_turn>user\\n{system_prompt}\\n\\nINVESTIGATION TARGET: {state['start_account']}\\n\\n\"\n",
    "    prompt += f\"CURRENT STATUS:\\n- Accounts analyzed: {len(state['accounts_analyzed'])}\\n\"\n",
    "    prompt += f\"- Path: {' ‚Üí '.join(state['investigation_path'][-5:]) if state['investigation_path'] else '(none)'}\\n\"\n",
    "    prompt += f\"- Amount traced: ${state['total_amount_traced']:,.2f}\\n\"\n",
    "    if state['risk_indicators']:\n",
    "        prompt += f\"- Risk indicators: {'; '.join(state['risk_indicators'][-3:])}\\n\"\n",
    "    prompt += \"\\nBegin investigation.<end_of_turn>\\n\"\n",
    "    \n",
    "    for msg in state['messages'][-MAX_HISTORY_TURNS*2:]:\n",
    "        if msg['role'] == 'assistant':\n",
    "            prompt += f\"<start_of_turn>model\\n{msg['content']}<end_of_turn>\\n\"\n",
    "        elif msg['role'] == 'tool':\n",
    "            prompt += f\"<start_of_turn>user\\n{msg['content']}<end_of_turn>\\n\"\n",
    "    prompt += \"<start_of_turn>model\\n\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def update_state_from_result(state: InvestigationState, tool_name: str, args: dict, result: Any) -> InvestigationState:\n",
    "    new_state = dict(state)\n",
    "    new_state['accounts_analyzed'] = dict(state['accounts_analyzed'])\n",
    "    new_state['entities_checked'] = dict(state['entities_checked'])\n",
    "    new_state['risk_indicators'] = list(state['risk_indicators'])\n",
    "    new_state['investigation_path'] = list(state['investigation_path'])\n",
    "    \n",
    "    if tool_name == \"get_account_summary\":\n",
    "        acc_id = args.get(\"account_id\", \"\")\n",
    "        if isinstance(result, dict) and \"error\" not in result:\n",
    "            new_state['accounts_analyzed'][acc_id] = result\n",
    "            if acc_id not in new_state['investigation_path']:\n",
    "                new_state['investigation_path'].append(acc_id)\n",
    "            if result.get(\"risk_score\", 0) > 0.7:\n",
    "                new_state['risk_indicators'].append(f\"HIGH_RISK:{acc_id}\")\n",
    "            if result.get(\"is_sanctioned\"):\n",
    "                new_state['risk_indicators'].append(f\"SANCTIONED:{acc_id}\")\n",
    "            if result.get(\"transitive_illicit\"):\n",
    "                new_state['risk_indicators'].append(f\"ILLICIT_PATH:{acc_id}\")\n",
    "    \n",
    "    elif tool_name == \"get_recent_transactions\":\n",
    "        if isinstance(result, list):\n",
    "            for txn in result:\n",
    "                new_state['total_amount_traced'] += txn.get(\"amount\", 0)\n",
    "                counterparty = txn.get(\"counterparty\")\n",
    "                if counterparty and counterparty not in new_state['investigation_path']:\n",
    "                    new_state['investigation_path'].append(counterparty)\n",
    "                if txn.get(\"high_risk_indicator\"):\n",
    "                    new_state['risk_indicators'].append(f\"ILLICIT_TXN:{counterparty}\")\n",
    "    \n",
    "    elif tool_name == \"check_sanctions_list\":\n",
    "        entity_id = args.get(\"entity_id\", \"\")\n",
    "        is_sanctioned = result.get(\"on_sanctions_list\", False) if isinstance(result, dict) else False\n",
    "        new_state['entities_checked'][entity_id] = is_sanctioned\n",
    "        if is_sanctioned:\n",
    "            new_state['risk_indicators'].append(f\"SANCTIONED:{entity_id}\")\n",
    "    \n",
    "    return new_state\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class InvestigationEpisode:\n",
    "    \"\"\"Records an investigation episode for evaluation and training.\"\"\"\n",
    "    start_account: str\n",
    "    steps: List[dict] = field(default_factory=list)\n",
    "    terminated: bool = False\n",
    "    success: bool = False\n",
    "    final_result: dict = field(default_factory=dict)\n",
    "    total_reward: float = 0.0\n",
    "    \n",
    "    def add_step(self, tool_name: str, args: dict, result: Any, reward: float = 0.0):\n",
    "        self.steps.append({\"step\": len(self.steps) + 1, \"tool_name\": tool_name, \"arguments\": args, \"result\": result, \"reward\": reward})\n",
    "        self.total_reward += reward\n",
    "\n",
    "\n",
    "def calculate_reward(tool_name: str, args: dict, result: Any, state: InvestigationState) -> float:\n",
    "    \"\"\"GRPO reward function based on design doc Section 4.2.\"\"\"\n",
    "    reward = -0.1  # Base step penalty\n",
    "    \n",
    "    if tool_name == \"get_account_summary\":\n",
    "        if isinstance(result, dict) and result.get(\"transitive_illicit\"):\n",
    "            reward += 0.5  # R_Discovery\n",
    "    elif tool_name == \"get_recent_transactions\":\n",
    "        if any(\"HIGH_RISK\" in r or \"ILLICIT\" in r for r in state['risk_indicators']):\n",
    "            reward += 0.3  # R_Logic\n",
    "        if isinstance(result, list):\n",
    "            for txn in result:\n",
    "                if txn.get(\"high_risk_indicator\"):\n",
    "                    reward += 0.2\n",
    "    elif tool_name == \"check_sanctions_list\":\n",
    "        if isinstance(result, dict) and result.get(\"on_sanctions_list\"):\n",
    "            reward += 0.5\n",
    "    elif tool_name == \"submit_sar\":\n",
    "        if isinstance(result, dict):\n",
    "            if result.get(\"correct_identification\"):\n",
    "                reward += 2.0  # R_Outcome\n",
    "            else:\n",
    "                reward -= 1.0\n",
    "    return reward\n",
    "\n",
    "\n",
    "print(\"‚úì Agent state and execution logic defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# AGENT EXECUTION - Run Investigation Episode with Detailed Logging\n",
    "# ============================================================================\n",
    "\n",
    "def run_investigation(\n",
    "    start_account: str, \n",
    "    model, \n",
    "    tokenizer, \n",
    "    max_steps: int = MAX_STEPS, \n",
    "    verbose: bool = False,\n",
    "    show_thinking: bool = True,\n",
    "    show_memory: bool = True,\n",
    "    show_raw_response: bool = False\n",
    ") -> InvestigationEpisode:\n",
    "    \"\"\"\n",
    "    Run a complete investigation episode using the FunctionGemma agent.\n",
    "    \n",
    "    Args:\n",
    "        verbose: Show step-by-step execution details\n",
    "        show_thinking: Display agent's <thinking> reasoning\n",
    "        show_memory: Display memory context (accounts analyzed, risk indicators)\n",
    "        show_raw_response: Display raw model output (for debugging)\n",
    "    \"\"\"\n",
    "    env.reset_investigation(start_account)\n",
    "    state = create_initial_state(start_account)\n",
    "    episode = InvestigationEpisode(start_account=start_account)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'‚ïê' * 70}\")\n",
    "        print(f\"üîç INVESTIGATION START\")\n",
    "        print(f\"{'‚ïê' * 70}\")\n",
    "        print(f\"  Target Account: {start_account}\")\n",
    "        print(f\"  Max Steps: {max_steps}\")\n",
    "        print(f\"{'‚îÄ' * 70}\")\n",
    "    \n",
    "    for step_num in range(max_steps):\n",
    "        # Build prompt from current state\n",
    "        prompt = build_prompt(state)\n",
    "        prompt_tokens = len(tokenizer.encode(prompt))\n",
    "        \n",
    "        if verbose and show_memory:\n",
    "            print(f\"\\n‚îå‚îÄ STEP {step_num + 1} {'‚îÄ' * 55}‚îê\")\n",
    "            print(f\"‚îÇ üìä MEMORY CONTEXT:\")\n",
    "            print(f\"‚îÇ   Accounts Analyzed: {len(state['accounts_analyzed'])}\")\n",
    "            print(f\"‚îÇ   Path: {' ‚Üí '.join(state['investigation_path'][-4:]) if state['investigation_path'] else '(empty)'}\")\n",
    "            print(f\"‚îÇ   Amount Traced: ${state['total_amount_traced']:,.2f}\")\n",
    "            print(f\"‚îÇ   Risk Indicators: {len(state['risk_indicators'])} ({state['risk_indicators'][-3:] if state['risk_indicators'] else 'none'})\")\n",
    "            print(f\"‚îÇ   Strategy: {state['current_strategy'].upper()}\")\n",
    "            print(f\"‚îÇ   Prompt Tokens: {prompt_tokens:,}\")\n",
    "        \n",
    "        # Generate model response\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=7000).to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=512,  # Increased for thinking\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=False)\n",
    "        if \"<end_of_turn>\" in response:\n",
    "            response = response.split(\"<end_of_turn>\")[0]\n",
    "        \n",
    "        # Extract thinking and function call\n",
    "        thinking = extract_thinking(response)\n",
    "        tool_call = extract_function_call(response)\n",
    "        used_fallback = False\n",
    "        \n",
    "        if verbose and show_raw_response:\n",
    "            print(f\"‚îÇ üî§ RAW RESPONSE:\")\n",
    "            for line in response[:500].split('\\n'):\n",
    "                print(f\"‚îÇ   {line[:65]}\")\n",
    "            if len(response) > 500:\n",
    "                print(f\"‚îÇ   ... (truncated, {len(response)} chars total)\")\n",
    "        \n",
    "        if verbose and show_thinking and thinking:\n",
    "            print(f\"‚îÇ üí≠ AGENT THINKING:\")\n",
    "            for line in thinking.split('\\n')[:5]:\n",
    "                print(f\"‚îÇ   {line[:65]}\")\n",
    "        \n",
    "        if not tool_call:\n",
    "            used_fallback = True\n",
    "            # Smart fallback strategy that makes real progress\n",
    "            \n",
    "            # Check if we found a sanctioned entity - submit SAR immediately\n",
    "            sanctioned_found = [entity for entity, is_sanct in state['entities_checked'].items() if is_sanct]\n",
    "            if sanctioned_found:\n",
    "                tool_call = {\"name\": \"submit_sar\", \"arguments\": {\n",
    "                    \"entity_id\": sanctioned_found[0], \n",
    "                    \"reason\": f\"Sanctioned entity found on laundering path from {start_account}\"\n",
    "                }}\n",
    "            # Step 0: Start with account summary\n",
    "            elif state['step_count'] == 0 or not state['accounts_analyzed']:\n",
    "                tool_call = {\"name\": \"get_account_summary\", \"arguments\": {\"account_id\": start_account}}\n",
    "            # Step 1: Get transactions to find connected accounts\n",
    "            elif len(state['accounts_analyzed']) == 1 and not any(\"TXN\" in r for r in state['risk_indicators']):\n",
    "                tool_call = {\"name\": \"get_recent_transactions\", \"arguments\": {\"account_id\": start_account}}\n",
    "            else:\n",
    "                # Priority: Check sanctions on accounts in path (high-risk first)\n",
    "                unchecked = [a for a in state['investigation_path'] if a not in state['entities_checked']]\n",
    "                \n",
    "                # Prioritize high-risk accounts\n",
    "                high_risk_unchecked = [a for a in unchecked if any(f\"HIGH_RISK:{a}\" in r or f\"ILLICIT\" in r for r in state['risk_indicators'])]\n",
    "                \n",
    "                if high_risk_unchecked:\n",
    "                    tool_call = {\"name\": \"check_sanctions_list\", \"arguments\": {\"entity_id\": high_risk_unchecked[0]}}\n",
    "                elif unchecked:\n",
    "                    tool_call = {\"name\": \"check_sanctions_list\", \"arguments\": {\"entity_id\": unchecked[0]}}\n",
    "                else:\n",
    "                    # Explore next account in path\n",
    "                    unexplored = [a for a in state['investigation_path'] if a not in state['accounts_analyzed']]\n",
    "                    if unexplored:\n",
    "                        tool_call = {\"name\": \"get_account_summary\", \"arguments\": {\"account_id\": unexplored[0]}}\n",
    "                    elif state['investigation_path']:\n",
    "                        # Get transactions from last explored account\n",
    "                        for acc in reversed(state['investigation_path'][-5:]):\n",
    "                            if acc in state['accounts_analyzed']:\n",
    "                                tool_call = {\"name\": \"get_recent_transactions\", \"arguments\": {\"account_id\": acc}}\n",
    "                                break\n",
    "                    \n",
    "                    if not tool_call:\n",
    "                        if verbose:\n",
    "                            print(f\"‚îÇ ‚ö†Ô∏è No valid action - terminating\")\n",
    "                        break\n",
    "        \n",
    "        # Execute tool call\n",
    "        tool_name, result = execute_tool_call(tool_call)\n",
    "        args = tool_call.get(\"arguments\", {})\n",
    "        \n",
    "        if verbose:\n",
    "            fallback_indicator = \" (FALLBACK)\" if used_fallback else \"\"\n",
    "            print(f\"‚îÇ üîß TOOL CALL{fallback_indicator}:\")\n",
    "            print(f\"‚îÇ   Function: {tool_name}\")\n",
    "            print(f\"‚îÇ   Arguments: {json.dumps(args, default=str)[:60]}\")\n",
    "            \n",
    "            # Show result summary based on tool type\n",
    "            if tool_name == \"get_account_summary\" and isinstance(result, dict):\n",
    "                illicit = \"üî¥ ILLICIT\" if result.get(\"transitive_illicit\") else \"\"\n",
    "                sanct = \"‚ö†Ô∏è SANCTIONED\" if result.get(\"is_sanctioned\") else \"\"\n",
    "                print(f\"‚îÇ   Result: {result.get('account_type', 'Unknown')} | Risk: {result.get('risk_score', 0):.2f} {illicit} {sanct}\")\n",
    "            elif tool_name == \"get_recent_transactions\" and isinstance(result, list):\n",
    "                print(f\"‚îÇ   Result: {len(result)} transactions found\")\n",
    "                for txn in result[:3]:\n",
    "                    risk = \"üî¥\" if txn.get(\"high_risk_indicator\") else \"\"\n",
    "                    print(f\"‚îÇ     ‚Üí {txn.get('counterparty', '?')[:25]}: ${txn.get('amount', 0):,.2f} {risk}\")\n",
    "            elif tool_name == \"check_sanctions_list\" and isinstance(result, dict):\n",
    "                status = \"‚ö†Ô∏è ON SANCTIONS LIST\" if result.get(\"on_sanctions_list\") else \"‚úì Clear\"\n",
    "                print(f\"‚îÇ   Result: {status}\")\n",
    "            elif tool_name == \"submit_sar\" and isinstance(result, dict):\n",
    "                status = \"‚úÖ CORRECT\" if result.get(\"correct_identification\") else \"‚ùå INCORRECT\"\n",
    "                print(f\"‚îÇ   Result: {status}\")\n",
    "                print(f\"‚îÇ   Reason: {result.get('evaluation_reason', '')[:50]}\")\n",
    "            elif isinstance(result, dict) and \"error\" in result:\n",
    "                print(f\"‚îÇ   Result: ‚ö†Ô∏è ERROR - {result.get('error', '')[:50]}\")\n",
    "        \n",
    "        # Calculate reward\n",
    "        reward = calculate_reward(tool_name, args, result, state)\n",
    "        \n",
    "        if verbose:\n",
    "            reward_color = \"üü¢\" if reward > 0 else (\"üî¥\" if reward < 0 else \"‚ö™\")\n",
    "            print(f\"‚îÇ üí∞ REWARD: {reward_color} {reward:+.2f}\")\n",
    "            print(f\"‚îî{'‚îÄ' * 68}‚îò\")\n",
    "        \n",
    "        # Record step\n",
    "        episode.add_step(tool_name, args, result, reward)\n",
    "        \n",
    "        # Update state\n",
    "        state = update_state_from_result(state, tool_name, args, result)\n",
    "        state['step_count'] = step_num + 1\n",
    "        state['messages'] = list(state['messages']) + [\n",
    "            {\"role\": \"assistant\", \"content\": response},\n",
    "            {\"role\": \"tool\", \"content\": format_function_output(tool_name, result)},\n",
    "        ]\n",
    "        \n",
    "        # Check for terminal action (SAR submission)\n",
    "        if tool_name == \"submit_sar\":\n",
    "            episode.terminated = True\n",
    "            episode.success = result.get(\"correct_identification\", False)\n",
    "            episode.final_result = result\n",
    "            break\n",
    "    \n",
    "    if not episode.terminated:\n",
    "        episode.terminated = True\n",
    "        episode.success = False\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'‚ïê' * 70}\")\n",
    "        print(f\"üìã INVESTIGATION COMPLETE\")\n",
    "        print(f\"{'‚ïê' * 70}\")\n",
    "        print(f\"  Result: {'‚úÖ SUCCESS' if episode.success else '‚ùå FAILED'}\")\n",
    "        print(f\"  Steps: {len(episode.steps)}\")\n",
    "        print(f\"  Total Reward: {episode.total_reward:+.2f}\")\n",
    "        if episode.final_result:\n",
    "            print(f\"  SAR Reason: {episode.final_result.get('evaluation_reason', 'N/A')}\")\n",
    "        print(f\"{'‚ïê' * 70}\")\n",
    "    \n",
    "    return episode\n",
    "\n",
    "\n",
    "print(\"‚úì Agent execution function defined (verbose mode available)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. LLM-as-Judge Evaluation with Gemini\n",
    "\n",
    "Evaluate agent performance using Gemini as an LLM judge with integrated MLflow tracing.\n",
    "\n",
    "**Rubric:**\n",
    "1. **Strategy Quality** (0-10): Did the agent prioritize high-value/high-risk transfers?\n",
    "2. **Decision Persistence** (0-10): Did it pivot correctly after hitting dead ends?\n",
    "3. **Outcome** (0-10): Did the agent correctly identify a sanctioned entity?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LLM-AS-JUDGE EVALUATION - Gemini-based Scoring\n",
    "# Using google.genai library (google.generativeai is deprecated)\n",
    "# ============================================================================\n",
    "\n",
    "from google import genai\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\", \"\")\n",
    "if GEMINI_API_KEY:\n",
    "    gemini_client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "    print(\"‚úì Gemini API configured (google.genai)\")\n",
    "else:\n",
    "    gemini_client = None\n",
    "    print(\"‚ö†Ô∏è GEMINI_API_KEY not set - LLM-as-Judge will be disabled\")\n",
    "\n",
    "\n",
    "JUDGE_PROMPT = \"\"\"You are an expert evaluator of AML investigations.\n",
    "\n",
    "## Investigation Trace\n",
    "{trace}\n",
    "\n",
    "## Evaluation Rubric\n",
    "1. **Strategy Quality** (0-10): Did the agent prioritize high-value and high-risk transfers?\n",
    "2. **Decision Persistence** (0-10): Did the agent correctly pivot after hitting dead ends?\n",
    "3. **Outcome Quality** (0-10): Did the agent correctly identify a sanctioned entity?\n",
    "\n",
    "## Response Format\n",
    "Provide your evaluation as JSON:\n",
    "{{\"strategy_score\": <0-10>, \"persistence_score\": <0-10>, \"outcome_score\": <0-10>, \"overall_score\": <0-10>, \"reasoning\": \"<brief explanation>\"}}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@mlflow.trace(span_type=\"LLM_JUDGE\")\n",
    "def evaluate_episode_with_llm(episode: InvestigationEpisode) -> dict:\n",
    "    \"\"\"Evaluate an investigation episode using Gemini as LLM judge.\"\"\"\n",
    "    default_scores = {\n",
    "        \"strategy_score\": 0, \n",
    "        \"persistence_score\": 0, \n",
    "        \"outcome_score\": 10 if episode.success else 0,\n",
    "        \"overall_score\": 5 if episode.success else 0, \n",
    "        \"reasoning\": \"Default score\"\n",
    "    }\n",
    "    \n",
    "    if not gemini_client:\n",
    "        default_scores[\"reasoning\"] = \"LLM evaluation disabled (no API key)\"\n",
    "        return default_scores\n",
    "    \n",
    "    # Build investigation trace\n",
    "    trace_lines = [f\"Start Account: {episode.start_account}\", \"\"]\n",
    "    for step in episode.steps:\n",
    "        tool_name = step.get('tool_name', 'unknown')\n",
    "        trace_lines.append(f\"Step {step['step']}: {tool_name}\")\n",
    "        trace_lines.append(f\"  Args: {json.dumps(step.get('arguments', {}), default=str)[:80]}\")\n",
    "        \n",
    "        # More detailed result summary based on tool type\n",
    "        result = step.get('result', {})\n",
    "        if tool_name == \"get_account_summary\" and isinstance(result, dict):\n",
    "            trace_lines.append(f\"  Result: Type={result.get('account_type', 'Unknown')}, Risk={result.get('risk_score', 0):.2f}, Illicit={result.get('transitive_illicit', False)}\")\n",
    "        elif tool_name == \"get_recent_transactions\" and isinstance(result, list):\n",
    "            trace_lines.append(f\"  Result: {len(result)} transactions (high_risk: {sum(1 for t in result if t.get('high_risk_indicator'))})\")\n",
    "        elif tool_name == \"check_sanctions_list\" and isinstance(result, dict):\n",
    "            trace_lines.append(f\"  Result: Sanctioned={result.get('on_sanctions_list', False)}\")\n",
    "        elif tool_name == \"submit_sar\" and isinstance(result, dict):\n",
    "            trace_lines.append(f\"  Result: Correct={result.get('correct_identification', False)}, Reason={result.get('evaluation_reason', 'N/A')[:50]}\")\n",
    "        else:\n",
    "            trace_lines.append(f\"  Result: {json.dumps(result, default=str)[:100]}\")\n",
    "        trace_lines.append(f\"  Reward: {step.get('reward', 0):+.2f}\")\n",
    "    \n",
    "    trace_lines.append(f\"\\nFinal: {'SUCCESS' if episode.success else 'FAILED'} | Steps: {len(episode.steps)} | Total Reward: {episode.total_reward:+.2f}\")\n",
    "    \n",
    "    trace_text = \"\\n\".join(trace_lines)\n",
    "    \n",
    "    try:\n",
    "        response = gemini_client.models.generate_content(\n",
    "            model=GEMINI_MODEL, \n",
    "            contents=JUDGE_PROMPT.format(trace=trace_text)\n",
    "        )\n",
    "        \n",
    "        # Extract JSON from response\n",
    "        json_match = re.search(r'\\{[^{}]*\\}', response.text, re.DOTALL)\n",
    "        if json_match:\n",
    "            scores = json.loads(json_match.group())\n",
    "            # Ensure all required fields are present\n",
    "            for key in [\"strategy_score\", \"persistence_score\", \"outcome_score\", \"overall_score\"]:\n",
    "                if key not in scores:\n",
    "                    scores[key] = 0\n",
    "            if \"reasoning\" not in scores:\n",
    "                scores[\"reasoning\"] = \"No reasoning provided\"\n",
    "            return scores\n",
    "        else:\n",
    "            default_scores[\"reasoning\"] = f\"Failed to parse JSON from: {response.text[:100]}\"\n",
    "            return default_scores\n",
    "            \n",
    "    except Exception as e:\n",
    "        default_scores[\"reasoning\"] = f\"API error: {str(e)[:50]}\"\n",
    "        return default_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EVALUATION FUNCTION - Run Multiple Episodes with Detailed Logging\n",
    "# ============================================================================\n",
    "\n",
    "def run_evaluation(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    stage_name: str, \n",
    "    n_episodes: int = EVAL_EPISODES, \n",
    "    use_llm_judge: bool = True, \n",
    "    verbose: bool = True,\n",
    "    show_episode_details: bool = True,  # Show step-by-step for each episode\n",
    "    show_thinking: bool = True,\n",
    "    show_memory: bool = True,\n",
    "    show_raw_response: bool = False     # Show raw model output for debugging\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run evaluation episodes and collect metrics. Returns DataFrame with results.\n",
    "    \n",
    "    Args:\n",
    "        verbose: Show progress and summaries\n",
    "        show_episode_details: Show step-by-step details for each episode\n",
    "        show_thinking: Show agent's <thinking> reasoning\n",
    "        show_memory: Show memory context at each step\n",
    "    \"\"\"\n",
    "    # Use pattern seed accounts for guaranteed laundering paths\n",
    "    eval_seeds = [p.seed_account for p in random.sample(laundering_patterns, min(n_episodes, len(laundering_patterns))) if p.seed_account]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(f\"\\n{'‚ïê' * 70}\")\n",
    "    print(f\"üß™ EVALUATION: {stage_name.upper()}\")\n",
    "    print(f\"{'‚ïê' * 70}\")\n",
    "    print(f\"  Episodes: {n_episodes}\")\n",
    "    print(f\"  Max Steps: {MAX_STEPS}\")\n",
    "    print(f\"  LLM Judge: {'Enabled' if use_llm_judge and gemini_client else 'Disabled'}\")\n",
    "    print(f\"{'‚ïê' * 70}\")\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"eval_{stage_name}\"):\n",
    "        for i, seed in enumerate(eval_seeds[:n_episodes]):\n",
    "            print(f\"\\n{'‚ñì' * 70}\")\n",
    "            print(f\"  EPISODE {i+1}/{n_episodes}\")\n",
    "            print(f\"  Seed: {seed}\")\n",
    "            print(f\"{'‚ñì' * 70}\")\n",
    "            \n",
    "            # Run investigation with verbose output if requested\n",
    "            episode = run_investigation(\n",
    "                seed, model, tokenizer, \n",
    "                verbose=show_episode_details,\n",
    "                show_thinking=show_thinking,\n",
    "                show_memory=show_memory,\n",
    "                show_raw_response=show_raw_response\n",
    "            )\n",
    "            \n",
    "            # LLM Judge evaluation\n",
    "            if use_llm_judge and gemini_client:\n",
    "                print(f\"\\n  ü§ñ LLM-AS-JUDGE EVALUATION...\")\n",
    "                llm_scores = evaluate_episode_with_llm(episode)\n",
    "                print(f\"     Strategy:    {llm_scores.get('strategy_score', 0)}/10\")\n",
    "                print(f\"     Persistence: {llm_scores.get('persistence_score', 0)}/10\")\n",
    "                print(f\"     Outcome:     {llm_scores.get('outcome_score', 0)}/10\")\n",
    "                print(f\"     Overall:     {llm_scores.get('overall_score', 0)}/10\")\n",
    "                if llm_scores.get('reasoning'):\n",
    "                    print(f\"     Reasoning:   {llm_scores.get('reasoning', '')[:60]}...\")\n",
    "            else:\n",
    "                llm_scores = {\n",
    "                    \"strategy_score\": 0, \n",
    "                    \"persistence_score\": 0, \n",
    "                    \"outcome_score\": 10 if episode.success else 0, \n",
    "                    \"overall_score\": 5 if episode.success else 0,\n",
    "                    \"reasoning\": \"LLM judge disabled\"\n",
    "                }\n",
    "            \n",
    "            result = {\n",
    "                \"seed_account\": seed, \n",
    "                \"success\": episode.success, \n",
    "                \"steps\": len(episode.steps),\n",
    "                \"total_reward\": episode.total_reward, \n",
    "                **llm_scores\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            # Episode summary\n",
    "            print(f\"\\n  ‚îå‚îÄ EPISODE {i+1} RESULT {'‚îÄ' * 44}‚îê\")\n",
    "            print(f\"  ‚îÇ Outcome:      {'‚úÖ SUCCESS' if episode.success else '‚ùå FAILED':<20} ‚îÇ\")\n",
    "            print(f\"  ‚îÇ Steps:        {len(episode.steps):<20} ‚îÇ\")\n",
    "            print(f\"  ‚îÇ Total Reward: {episode.total_reward:+.2f}{' ' * 17} ‚îÇ\")\n",
    "            print(f\"  ‚îÇ LLM Score:    {llm_scores.get('overall_score', 0)}/10{' ' * 16} ‚îÇ\")\n",
    "            print(f\"  ‚îî{'‚îÄ' * 53}‚îò\")\n",
    "        \n",
    "        df = pd.DataFrame(results)\n",
    "        \n",
    "        # Log to MLflow\n",
    "        mlflow.log_metrics({\n",
    "            f\"{stage_name}_success_rate\": df['success'].mean(),\n",
    "            f\"{stage_name}_avg_steps\": df['steps'].mean(),\n",
    "            f\"{stage_name}_avg_reward\": df['total_reward'].mean(),\n",
    "            f\"{stage_name}_avg_score\": df['overall_score'].mean(),\n",
    "        })\n",
    "    \n",
    "    # Final Summary\n",
    "    print(f\"\\n{'‚ïê' * 70}\")\n",
    "    print(f\"üìä {stage_name.upper()} - FINAL SUMMARY\")\n",
    "    print(f\"{'‚ïê' * 70}\")\n",
    "    print(f\"  Episodes Run:     {len(results)}\")\n",
    "    print(f\"  Success Rate:     {df['success'].mean()*100:.1f}% ({df['success'].sum()}/{len(results)})\")\n",
    "    print(f\"  Avg Steps:        {df['steps'].mean():.1f}\")\n",
    "    print(f\"  Avg Reward:       {df['total_reward'].mean():+.2f}\")\n",
    "    print(f\"  Avg LLM Score:    {df['overall_score'].mean():.1f}/10\")\n",
    "    print(f\"{'‚îÄ' * 70}\")\n",
    "    print(f\"  Score Breakdown:\")\n",
    "    print(f\"    Strategy:       {df['strategy_score'].mean():.1f}/10\")\n",
    "    print(f\"    Persistence:    {df['persistence_score'].mean():.1f}/10\")\n",
    "    print(f\"    Outcome:        {df['outcome_score'].mean():.1f}/10\")\n",
    "    print(f\"{'‚ïê' * 70}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"‚úì Evaluation framework configured (detailed logging enabled)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. STAGE 1: Baseline Evaluation (Pre-Training)\n",
    "\n",
    "Run evaluation on the **base Gemma-2-2B-IT model** before any fine-tuning to establish baseline performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CLEANUP - Remove Previous Training Outputs (for notebook re-runs)\n",
    "# ============================================================================\n",
    "\n",
    "import shutil\n",
    "\n",
    "def cleanup_training_outputs():\n",
    "    \"\"\"Remove previous training outputs to ensure clean re-runs.\"\"\"\n",
    "    cleanup_dirs = [\n",
    "        MODELS_DIR / \"sft_output\",\n",
    "        MODELS_DIR / \"sft_adapter\", \n",
    "        MODELS_DIR / \"grpo_output\",\n",
    "        MODELS_DIR / \"grpo_adapter\",\n",
    "        MODELS_DIR / \"aml_agent_final\",\n",
    "    ]\n",
    "    \n",
    "    print(\"üßπ Cleaning up previous training outputs...\")\n",
    "    \n",
    "    for dir_path in cleanup_dirs:\n",
    "        if dir_path.exists():\n",
    "            try:\n",
    "                shutil.rmtree(dir_path)\n",
    "                print(f\"   ‚úì Removed: {dir_path.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö† Could not remove {dir_path.name}: {e}\")\n",
    "        else:\n",
    "            print(f\"   - Not found: {dir_path.name} (skipping)\")\n",
    "    \n",
    "    print(\"‚úì Cleanup complete\\n\")\n",
    "\n",
    "# Run cleanup\n",
    "cleanup_training_outputs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STAGE 1: BASELINE EVALUATION - Pre-Training Performance\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üîç STAGE 1: BASELINE EVALUATION\")\n",
    "print(\"   Testing base Gemma-2-2B-IT model (no fine-tuning)\")\n",
    "\n",
    "# Initialize results dictionary for all stages\n",
    "all_results = {}\n",
    "\n",
    "# Ensure model is in inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Configuration for evaluation verbosity\n",
    "# Set show_raw_response=True to see what the model actually generates\n",
    "SHOW_EPISODE_DETAILS = True  # Show step-by-step execution\n",
    "SHOW_THINKING = True         # Show agent's <thinking> reasoning\n",
    "SHOW_MEMORY = True           # Show memory context\n",
    "SHOW_RAW_RESPONSE = False    # Show raw model output (useful for debugging)\n",
    "\n",
    "# Run baseline evaluation with detailed logging\n",
    "baseline_results = run_evaluation(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    stage_name=\"Baseline\",\n",
    "    n_episodes=EVAL_EPISODES,\n",
    "    use_llm_judge=bool(gemini_client),\n",
    "    verbose=True,\n",
    "    show_episode_details=SHOW_EPISODE_DETAILS,\n",
    "    show_thinking=SHOW_THINKING,\n",
    "    show_memory=SHOW_MEMORY\n",
    ")\n",
    "\n",
    "# Store results for comparison\n",
    "all_results[\"Baseline\"] = baseline_results\n",
    "\n",
    "print(\"\\nüìä Baseline Results Summary:\")\n",
    "print(baseline_results[['success', 'steps', 'total_reward', 'overall_score']].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. SFT Training Data Generation\n",
    "\n",
    "Generate Supervised Fine-Tuning samples from laundering patterns with internal reasoning (`<thinking>` tags).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SFT TRAINING DATA GENERATION - Multi-turn Conversations with Thinking\n",
    "# ============================================================================\n",
    "\n",
    "def generate_sft_sample(pattern: LaunderingPattern, include_backtrack: bool = False) -> List[dict]:\n",
    "    \"\"\"Generate a multi-turn SFT training sample with <thinking> tags.\"\"\"\n",
    "    messages = []\n",
    "    seed_account = pattern.seed_account\n",
    "    terminal_account = pattern.terminal_account\n",
    "    \n",
    "    if not seed_account or not terminal_account:\n",
    "        return []\n",
    "    \n",
    "    system_prompt = INVESTIGATION_PROMPT.format(tool_declarations=TOOL_DECLARATIONS)\n",
    "    messages.append({\"role\": \"user\", \"content\": f\"{system_prompt}\\n\\nINVESTIGATION TARGET: {seed_account}\\n\\nBegin investigation.\"})\n",
    "    \n",
    "    # Step 1: Get account summary with thinking\n",
    "    messages.append({\"role\": \"assistant\", \"content\": f\"\"\"<thinking>\n",
    "I need to start by getting the account summary for {seed_account} to understand its risk profile.\n",
    "</thinking>\n",
    "<start_function_call>call:get_account_summary{{account_id: {seed_account}}}</start_function_call>\"\"\"})\n",
    "    messages.append({\"role\": \"user\", \"content\": f\"TOOL RESULT: {json.dumps({'account_id': seed_account, 'account_type': 'Corporate', 'risk_score': 0.75, 'transitive_illicit': True})}\"})\n",
    "    \n",
    "    # Step 2: Get transactions with thinking\n",
    "    messages.append({\"role\": \"assistant\", \"content\": f\"\"\"<thinking>\n",
    "The account shows transitive_illicit=True. I should trace outgoing transactions to follow the money flow.\n",
    "</thinking>\n",
    "<start_function_call>call:get_recent_transactions{{account_id: {seed_account}, direction: outgoing}}</start_function_call>\"\"\"})\n",
    "    txn_results = [{\"counterparty\": t['to_account'], \"amount\": t['amount'], \"high_risk_indicator\": True} for t in pattern.transactions[:3]]\n",
    "    messages.append({\"role\": \"user\", \"content\": f\"TOOL RESULT: {json.dumps(txn_results)}\"})\n",
    "    \n",
    "    # Step 3: Check sanctions with thinking\n",
    "    messages.append({\"role\": \"assistant\", \"content\": f\"\"\"<thinking>\n",
    "I've found high-risk counterparties. Before submitting a SAR, I MUST verify the target is sanctioned.\n",
    "</thinking>\n",
    "<start_function_call>call:check_sanctions_list{{entity_id: {terminal_account}}}</start_function_call>\"\"\"})\n",
    "    messages.append({\"role\": \"user\", \"content\": f'TOOL RESULT: {{\"entity_id\": \"{terminal_account}\", \"on_sanctions_list\": true, \"list_type\": \"OFAC SDN\"}}'})\n",
    "    \n",
    "    # Step 4: Submit SAR with thinking\n",
    "    reason = f\"Identified {pattern.pattern_type} pattern with ${pattern.total_amount:,.2f}\"\n",
    "    messages.append({\"role\": \"assistant\", \"content\": f\"\"\"<thinking>\n",
    "CONFIRMED: {terminal_account} is ON the OFAC sanctions list. I can now submit the SAR with confidence.\n",
    "</thinking>\n",
    "<start_function_call>call:submit_sar{{entity_id: {terminal_account}, reason: {reason}}}</start_function_call>\"\"\"})\n",
    "    \n",
    "    return messages\n",
    "\n",
    "\n",
    "def generate_sft_dataset(patterns: List[LaunderingPattern], max_samples: int = 100) -> List[dict]:\n",
    "    dataset = []\n",
    "    for i, pattern in enumerate(random.sample(patterns, min(max_samples, len(patterns)))):\n",
    "        messages = generate_sft_sample(pattern)\n",
    "        if messages:\n",
    "            dataset.append({\"id\": f\"sft_{i}\", \"pattern_type\": pattern.pattern_type, \"messages\": messages})\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def format_for_unsloth_sft(dataset: List[dict]) -> List[dict]:\n",
    "    formatted = []\n",
    "    for sample in dataset:\n",
    "        text = \"\"\n",
    "        for msg in sample['messages']:\n",
    "            if msg['role'] == 'user':\n",
    "                text += f\"<start_of_turn>user\\n{msg['content']}<end_of_turn>\\n\"\n",
    "            elif msg['role'] == 'assistant':\n",
    "                text += f\"<start_of_turn>model\\n{msg['content']}<end_of_turn>\\n\"\n",
    "        formatted.append({\"text\": text})\n",
    "    return formatted\n",
    "\n",
    "\n",
    "# Generate SFT dataset\n",
    "print(\"üìä Generating SFT training data...\")\n",
    "sft_dataset = generate_sft_dataset(laundering_patterns, max_samples=100)\n",
    "sft_formatted = format_for_unsloth_sft(sft_dataset)\n",
    "\n",
    "print(f\"‚úì Generated {len(sft_formatted)} SFT training samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. SFT Training with Unsloth\n",
    "\n",
    "Fine-tune with LoRA adapters (r=32, alpha=64) targeting attention and MLP layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SFT TRAINING WITH UNSLOTH - LoRA Fine-tuning\n",
    "# ============================================================================\n",
    "\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "print(\"üì• Configuring LoRA adapters for SFT...\")\n",
    "\n",
    "# Get PEFT model with LoRA\n",
    "model_for_training = FastLanguageModel.get_peft_model(\n",
    "    model, r=LORA_R, target_modules=LORA_TARGET_MODULES, lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=0.05, bias=\"none\", use_gradient_checkpointing=\"unsloth\", random_state=RANDOM_SEED,\n",
    ")\n",
    "\n",
    "# Create dataset\n",
    "sft_hf_dataset = Dataset.from_list([{\"text\": s[\"text\"]} for s in sft_formatted])\n",
    "\n",
    "# Training arguments\n",
    "sft_output_dir = MODELS_DIR / \"sft_output\"\n",
    "sft_args = TrainingArguments(\n",
    "    output_dir=str(sft_output_dir), per_device_train_batch_size=2, gradient_accumulation_steps=4,\n",
    "    num_train_epochs=SFT_EPOCHS, learning_rate=SFT_LEARNING_RATE, warmup_ratio=0.1,\n",
    "    logging_steps=10, save_strategy=\"epoch\", fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(), optim=\"adamw_8bit\", seed=RANDOM_SEED, report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "sft_trainer = SFTTrainer(\n",
    "    model=model_for_training, tokenizer=tokenizer, train_dataset=sft_hf_dataset,\n",
    "    args=sft_args, max_seq_length=4096,\n",
    ")\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"üéì STARTING SFT TRAINING\")\n",
    "print(f\"{'=' * 60}\")\n",
    "print(f\"  Epochs: {SFT_EPOCHS} | LR: {SFT_LEARNING_RATE} | LoRA r={LORA_R}\")\n",
    "print(f\"{'=' * 60}\")\n",
    "\n",
    "# Train\n",
    "sft_trainer.train()\n",
    "\n",
    "# Save adapter\n",
    "sft_adapter_path = MODELS_DIR / \"sft_adapter\"\n",
    "model_for_training.save_pretrained(str(sft_adapter_path))\n",
    "tokenizer.save_pretrained(str(sft_adapter_path))\n",
    "\n",
    "print(f\"\\n‚úì SFT training complete! Adapter saved to: {sft_adapter_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. STAGE 2: Post-SFT Evaluation\n",
    "\n",
    "Evaluate the **SFT-tuned model** to measure improvement from supervised fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STAGE 2: POST-SFT EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üîç STAGE 2: POST-SFT EVALUATION\")\n",
    "print(\"   Testing model after Supervised Fine-Tuning\")\n",
    "\n",
    "# Switch to inference mode\n",
    "FastLanguageModel.for_inference(model_for_training)\n",
    "\n",
    "# Run post-SFT evaluation with detailed logging\n",
    "post_sft_results = run_evaluation(\n",
    "    model=model_for_training,\n",
    "    tokenizer=tokenizer,\n",
    "    stage_name=\"Post-SFT\",\n",
    "    n_episodes=EVAL_EPISODES,\n",
    "    use_llm_judge=bool(gemini_client),\n",
    "    verbose=True,\n",
    "    show_episode_details=SHOW_EPISODE_DETAILS,\n",
    "    show_thinking=SHOW_THINKING,\n",
    "    show_memory=SHOW_MEMORY\n",
    ")\n",
    "\n",
    "# Store results for comparison\n",
    "all_results[\"Post-SFT\"] = post_sft_results\n",
    "\n",
    "# Show improvement over baseline\n",
    "baseline_sr = all_results[\"Baseline\"]['success'].mean()\n",
    "post_sft_sr = post_sft_results['success'].mean()\n",
    "print(f\"\\nüìà SFT Improvement: {baseline_sr*100:.1f}% ‚Üí {post_sft_sr*100:.1f}% ({(post_sft_sr-baseline_sr)*100:+.1f}%)\")\n",
    "\n",
    "print(\"\\nüìä Post-SFT Results Summary:\")\n",
    "print(post_sft_results[['success', 'steps', 'total_reward', 'overall_score']].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. GRPO Training - Reinforcement Learning\n",
    "\n",
    "Train with **Group Relative Policy Optimization (GRPO)** using TRL:\n",
    "- **R_Discovery (+0.5)**: Discovering transitive_illicit nodes\n",
    "- **R_Logic (+0.3)**: Correct tool sequencing (transactions after high-risk)\n",
    "- **R_Outcome (+2.0)**: Correct SAR submission\n",
    "- **R_Efficiency (-0.1)**: Step penalty to prevent loops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GRPO TRAINING - Group Relative Policy Optimization\n",
    "# ============================================================================\n",
    "\n",
    "from trl import GRPOTrainer, GRPOConfig\n",
    "\n",
    "def generate_grpo_prompts(patterns: List[LaunderingPattern], n_prompts: int = 50) -> List[str]:\n",
    "    \"\"\"Generate prompts for GRPO training from laundering patterns.\"\"\"\n",
    "    prompts = []\n",
    "    selected = random.sample(patterns, min(n_prompts, len(patterns)))\n",
    "    \n",
    "    for pattern in selected:\n",
    "        seed = pattern.seed_account\n",
    "        if seed:\n",
    "            system = INVESTIGATION_PROMPT.format(tool_declarations=TOOL_DECLARATIONS)\n",
    "            prompt = f\"<start_of_turn>user\\n{system}\\n\\nINVESTIGATION TARGET: {seed}\\n\\nBegin investigation.<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "            prompts.append(prompt)\n",
    "    \n",
    "    return prompts\n",
    "\n",
    "\n",
    "def grpo_reward_function(completions: List[str], prompts: List[str]) -> List[float]:\n",
    "    \"\"\"\n",
    "    GRPO reward function that evaluates model completions.\n",
    "    Runs each completion through the environment and calculates reward.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    \n",
    "    for completion, prompt in zip(completions, prompts):\n",
    "        # Extract seed account from prompt\n",
    "        match = re.search(r'INVESTIGATION TARGET:\\s*([^\\n]+)', prompt)\n",
    "        if not match:\n",
    "            rewards.append(-1.0)\n",
    "            continue\n",
    "        \n",
    "        seed_account = match.group(1).strip()\n",
    "        total_reward = 0.0\n",
    "        \n",
    "        # Reset environment\n",
    "        env.reset_investigation(seed_account)\n",
    "        state = create_initial_state(seed_account)\n",
    "        \n",
    "        # Extract and execute function call from completion\n",
    "        tool_call = extract_function_call(completion)\n",
    "        \n",
    "        if tool_call:\n",
    "            tool_name, result = execute_tool_call(tool_call)\n",
    "            args = tool_call.get(\"arguments\", {})\n",
    "            total_reward = calculate_reward(tool_name, args, result, state)\n",
    "        else:\n",
    "            total_reward = -0.5  # Penalty for invalid output\n",
    "        \n",
    "        rewards.append(total_reward)\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "\n",
    "# Generate GRPO training prompts\n",
    "print(\"üìä Generating GRPO training prompts...\")\n",
    "grpo_prompts = generate_grpo_prompts(laundering_patterns, n_prompts=50)\n",
    "grpo_dataset = Dataset.from_dict({\"prompt\": grpo_prompts})\n",
    "print(f\"‚úì Generated {len(grpo_prompts)} GRPO training prompts\")\n",
    "\n",
    "# GRPO Training Configuration\n",
    "grpo_output_dir = MODELS_DIR / \"grpo_output\"\n",
    "grpo_config = GRPOConfig(\n",
    "    output_dir=str(grpo_output_dir),\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=GRPO_EPOCHS,\n",
    "    learning_rate=GRPO_LEARNING_RATE,\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"üéØ STARTING GRPO TRAINING\")\n",
    "print(f\"{'=' * 60}\")\n",
    "print(f\"  Epochs:        {GRPO_EPOCHS}\")\n",
    "print(f\"  Learning Rate: {GRPO_LEARNING_RATE}\")\n",
    "print(f\"  Prompts:       {len(grpo_prompts)}\")\n",
    "print(f\"{'=' * 60}\")\n",
    "\n",
    "# Create GRPO Trainer\n",
    "grpo_trainer = GRPOTrainer(\n",
    "    model=model_for_training,\n",
    "    config=grpo_config,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=grpo_dataset,\n",
    "    reward_funcs=grpo_reward_function,\n",
    ")\n",
    "\n",
    "# Train with GRPO\n",
    "grpo_trainer.train()\n",
    "\n",
    "# Save GRPO adapter\n",
    "grpo_adapter_path = MODELS_DIR / \"grpo_adapter\"\n",
    "model_for_training.save_pretrained(str(grpo_adapter_path))\n",
    "tokenizer.save_pretrained(str(grpo_adapter_path))\n",
    "\n",
    "print(f\"\\n‚úì GRPO training complete!\")\n",
    "print(f\"‚úì Adapter saved to: {grpo_adapter_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. STAGE 3: Post-GRPO Evaluation\n",
    "\n",
    "Evaluate the **GRPO-trained model** to measure improvement from reinforcement learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STAGE 3: POST-GRPO EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üîç STAGE 3: POST-GRPO EVALUATION\")\n",
    "print(\"   Testing model after GRPO Reinforcement Learning\")\n",
    "\n",
    "# Switch to inference mode\n",
    "FastLanguageModel.for_inference(model_for_training)\n",
    "\n",
    "# Run post-GRPO evaluation with detailed logging\n",
    "post_grpo_results = run_evaluation(\n",
    "    model=model_for_training,\n",
    "    tokenizer=tokenizer,\n",
    "    stage_name=\"Post-GRPO\",\n",
    "    n_episodes=EVAL_EPISODES,\n",
    "    use_llm_judge=bool(gemini_client),\n",
    "    verbose=True,\n",
    "    show_episode_details=SHOW_EPISODE_DETAILS,\n",
    "    show_thinking=SHOW_THINKING,\n",
    "    show_memory=SHOW_MEMORY\n",
    ")\n",
    "\n",
    "# Store results for comparison\n",
    "all_results[\"Post-GRPO\"] = post_grpo_results\n",
    "\n",
    "print(\"\\nüìä Post-GRPO Results Summary:\")\n",
    "print(post_grpo_results[['success', 'steps', 'total_reward', 'overall_score']].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 14. Final Comparison: Baseline vs SFT vs GRPO\n",
    "\n",
    "Compare metrics across all three training stages:\n",
    "- **Success Rate**: Percentage of correct SAR submissions\n",
    "- **Average Steps**: Efficiency of investigation\n",
    "- **Average Reward**: GRPO reward function score\n",
    "- **LLM-as-Judge Scores**: Strategy, Persistence, Outcome, Overall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FINAL COMPARISON - Baseline vs SFT vs GRPO\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä FINAL COMPARISON: BASELINE vs SFT vs GRPO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Build comparison DataFrame\n",
    "comparison_data = []\n",
    "for stage_name, results_df in all_results.items():\n",
    "    comparison_data.append({\n",
    "        \"Stage\": stage_name,\n",
    "        \"Success Rate (%)\": results_df['success'].mean() * 100,\n",
    "        \"Avg Steps\": results_df['steps'].mean(),\n",
    "        \"Avg Reward\": results_df['total_reward'].mean(),\n",
    "        \"Avg Strategy Score\": results_df['strategy_score'].mean() if 'strategy_score' in results_df else 0,\n",
    "        \"Avg Persistence Score\": results_df['persistence_score'].mean() if 'persistence_score' in results_df else 0,\n",
    "        \"Avg Outcome Score\": results_df['outcome_score'].mean() if 'outcome_score' in results_df else 0,\n",
    "        \"Avg Overall Score\": results_df['overall_score'].mean() if 'overall_score' in results_df else 0,\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.set_index(\"Stage\")\n",
    "\n",
    "# Display comparison table\n",
    "print(\"\\nüìà METRICS COMPARISON:\")\n",
    "print(\"-\" * 80)\n",
    "print(comparison_df.round(2).to_string())\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Calculate improvements\n",
    "if len(comparison_data) >= 2:\n",
    "    baseline = comparison_data[0]\n",
    "    \n",
    "    print(\"\\nüìä IMPROVEMENTS OVER BASELINE:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, stage in enumerate(comparison_data[1:], 1):\n",
    "        stage_name = stage[\"Stage\"]\n",
    "        success_improvement = stage[\"Success Rate (%)\"] - baseline[\"Success Rate (%)\"]\n",
    "        reward_improvement = stage[\"Avg Reward\"] - baseline[\"Avg Reward\"]\n",
    "        score_improvement = stage[\"Avg Overall Score\"] - baseline[\"Avg Overall Score\"]\n",
    "        \n",
    "        print(f\"\\n  {stage_name}:\")\n",
    "        print(f\"    Success Rate: {success_improvement:+.1f}%\")\n",
    "        print(f\"    Avg Reward:   {reward_improvement:+.2f}\")\n",
    "        print(f\"    Overall Score: {score_improvement:+.1f}/10\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Log to MLflow\n",
    "with mlflow.start_run(run_name=\"final_comparison\"):\n",
    "    for stage_name, results_df in all_results.items():\n",
    "        mlflow.log_metrics({\n",
    "            f\"{stage_name}_success_rate\": results_df['success'].mean(),\n",
    "            f\"{stage_name}_avg_steps\": results_df['steps'].mean(),\n",
    "            f\"{stage_name}_avg_reward\": results_df['total_reward'].mean(),\n",
    "            f\"{stage_name}_avg_overall_score\": results_df.get('overall_score', pd.Series([0])).mean(),\n",
    "        })\n",
    "\n",
    "print(\"‚úì Comparison logged to MLflow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATION - Performance Comparison Charts\n",
    "# ============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create comparison visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('AML Investigation Agent: Training Stage Comparison', fontsize=14, fontweight='bold')\n",
    "\n",
    "stages = list(all_results.keys())\n",
    "colors = ['#e74c3c', '#3498db', '#2ecc71'][:len(stages)]\n",
    "\n",
    "# 1. Success Rate\n",
    "ax1 = axes[0, 0]\n",
    "success_rates = [all_results[s]['success'].mean() * 100 for s in stages]\n",
    "bars1 = ax1.bar(stages, success_rates, color=colors, edgecolor='black', linewidth=1.2)\n",
    "ax1.set_ylabel('Success Rate (%)', fontweight='bold')\n",
    "ax1.set_title('SAR Submission Accuracy', fontweight='bold')\n",
    "ax1.set_ylim(0, 100)\n",
    "for bar, val in zip(bars1, success_rates):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2, f'{val:.1f}%', \n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 2. Average Reward\n",
    "ax2 = axes[0, 1]\n",
    "avg_rewards = [all_results[s]['total_reward'].mean() for s in stages]\n",
    "bars2 = ax2.bar(stages, avg_rewards, color=colors, edgecolor='black', linewidth=1.2)\n",
    "ax2.set_ylabel('Average Reward', fontweight='bold')\n",
    "ax2.set_title('GRPO Reward Score', fontweight='bold')\n",
    "ax2.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "for bar, val in zip(bars2, avg_rewards):\n",
    "    ypos = bar.get_height() + 0.1 if val >= 0 else bar.get_height() - 0.3\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, ypos, f'{val:.2f}', \n",
    "             ha='center', va='bottom' if val >= 0 else 'top', fontweight='bold')\n",
    "\n",
    "# 3. Average Steps\n",
    "ax3 = axes[1, 0]\n",
    "avg_steps = [all_results[s]['steps'].mean() for s in stages]\n",
    "bars3 = ax3.bar(stages, avg_steps, color=colors, edgecolor='black', linewidth=1.2)\n",
    "ax3.set_ylabel('Average Steps', fontweight='bold')\n",
    "ax3.set_title('Investigation Efficiency', fontweight='bold')\n",
    "for bar, val in zip(bars3, avg_steps):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3, f'{val:.1f}', \n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 4. LLM-as-Judge Overall Score\n",
    "ax4 = axes[1, 1]\n",
    "if 'overall_score' in all_results[stages[0]].columns:\n",
    "    overall_scores = [all_results[s]['overall_score'].mean() for s in stages]\n",
    "    bars4 = ax4.bar(stages, overall_scores, color=colors, edgecolor='black', linewidth=1.2)\n",
    "    ax4.set_ylabel('Overall Score (0-10)', fontweight='bold')\n",
    "    ax4.set_title('LLM-as-Judge Score', fontweight='bold')\n",
    "    ax4.set_ylim(0, 10)\n",
    "    for bar, val in zip(bars4, overall_scores):\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.2, f'{val:.1f}', \n",
    "                 ha='center', va='bottom', fontweight='bold')\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'LLM-as-Judge\\nNot Available', ha='center', va='center', fontsize=12)\n",
    "    ax4.set_title('LLM-as-Judge Score', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "comparison_chart_path = OUTPUT_DIR / \"training_comparison.png\"\n",
    "plt.savefig(str(comparison_chart_path), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Comparison chart saved to {comparison_chart_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 15. Save Final Model\n",
    "\n",
    "Save the trained model adapters for deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SAVE FINAL MODEL\n",
    "# ============================================================================\n",
    "\n",
    "# Save final adapter to models directory\n",
    "final_model_path = MODELS_DIR / \"aml_agent_final\"\n",
    "model_for_training.save_pretrained(str(final_model_path))\n",
    "tokenizer.save_pretrained(str(final_model_path))\n",
    "\n",
    "# Save comparison results\n",
    "comparison_csv_path = OUTPUT_DIR / \"training_comparison.csv\"\n",
    "comparison_df.to_csv(str(comparison_csv_path))\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"‚úÖ TRAINING COMPLETE\")\n",
    "print(f\"{'=' * 60}\")\n",
    "print(f\"  Final Model:      {final_model_path}\")\n",
    "print(f\"  SFT Adapter:      {sft_adapter_path}\")\n",
    "print(f\"  GRPO Adapter:     {grpo_adapter_path}\")\n",
    "print(f\"  Comparison Chart: {OUTPUT_DIR / 'training_comparison.png'}\")\n",
    "print(f\"  Comparison CSV:   {comparison_csv_path}\")\n",
    "print(f\"{'=' * 60}\")\n",
    "\n",
    "print(f\"\\nüéâ AML Investigation Agent Training Complete!\")\n",
    "print(f\"\\nüìä Final Performance Summary:\")\n",
    "print(comparison_df.round(2).to_string())\n",
    "\n",
    "print(f\"\\nNext Steps:\")\n",
    "print(f\"  1. Load adapter with Unsloth for inference\")\n",
    "print(f\"  2. Deploy agent with LangGraph orchestration\")\n",
    "print(f\"  3. Monitor with MLflow tracing\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
