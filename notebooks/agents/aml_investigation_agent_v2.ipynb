{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç AML Investigation Agent (Qwen3-30B-A3B-Instruct + DPO)\n",
    "\n",
    "An autonomous AI Agent for investigating financial transaction graphs to identify money laundering patterns using **Qwen3-30B-A3B-Instruct-2507** (Mixture of Experts) with native Hermes-style tool calling and **Direct Preference Optimization (DPO)** for trajectory-level learning.\n",
    "\n",
    "## Why DPO over GRPO?\n",
    "\n",
    "Based on research from [Zhang et al. (arXiv:2506.00845)](https://arxiv.org/abs/2506.00845) on graph reasoning with LLMs:\n",
    "\n",
    "- **GRPO (single-step)**: Optimizes individual actions, but AML investigation requires multi-step reasoning\n",
    "- **DPO (trajectory-level)**: Compares full successful vs failed investigation trajectories\n",
    "- **Key finding**: Process-based rewards outperform solution-based by ~24% on graph tasks\n",
    "- **Practical benefit**: DPO uses offline traces, avoiding expensive online rollouts\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "| Component | Technology | Description |\n",
    "|-----------|------------|-------------|\n",
    "| **Base Model** | Qwen3-30B-A3B-Instruct-2507 | MOE architecture with native tool calling (Hermes format) |\n",
    "| **Fine-Tuning** | Unsloth FastModel + LoRA | 4-bit quantization, 2x faster training (MOE optimized) |\n",
    "| **Data Processing** | Polars | High-performance dataframes |\n",
    "| **Graph Analysis** | NetworkX | Transaction network traversal |\n",
    "| **Agent Framework** | Custom Agent Loop + State | Stateful exploration with step tracking |\n",
    "| **Preference Learning** | TRL DPOTrainer | Direct Preference Optimization (trajectory-level) |\n",
    "| **Observability** | MLflow Tracing | Full agent trace logging |\n",
    "| **Evaluation** | Gemini LLM-as-Judge | Strategy quality scoring |\n",
    "\n",
    "## Evaluation Flow\n",
    "\n",
    "This notebook implements a **three-stage evaluation** to measure training impact:\n",
    "\n",
    "| Stage | Model State | Purpose |\n",
    "|-------|-------------|---------|\n",
    "| **1. Baseline** | Pre-trained Qwen3-30B-A3B-Instruct | Measure zero-shot performance |\n",
    "| **2. Post-SFT** | After Supervised Fine-Tuning | Measure SFT improvement |\n",
    "| **3. Post-DPO** | After DPO Training | Measure DPO improvement (trajectory-level) |\n",
    "\n",
    "## Qwen3 Tool Calling Format (Hermes Style)\n",
    "\n",
    "Qwen3 uses the Hermes-style tool calling format with `<tool_call>` XML tags:\n",
    "\n",
    "```\n",
    "<|im_start|>system\n",
    "# Tools\n",
    "\n",
    "You may call one or more functions to assist with the user query.\n",
    "\n",
    "You are provided with function signatures within <tools></tools> XML tags:\n",
    "<tools>\n",
    "{\"type\": \"function\", \"function\": {\"name\": \"tool_name\", \"description\": \"...\", \"parameters\": {...}}}\n",
    "</tools>\n",
    "\n",
    "For each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n",
    "<tool_call>\n",
    "{\"name\": <function-name>, \"arguments\": <args-json-object>}\n",
    "</tool_call><|im_end|>\n",
    "<|im_start|>user\n",
    "{{user query}}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "<tool_call>\n",
    "{\"name\": \"function_name\", \"arguments\": {\"param\": \"value\"}}\n",
    "</tool_call><|im_end|>\n",
    "<|im_start|>user\n",
    "<tool_response>\n",
    "{\"result\": \"...\"}\n",
    "</tool_response><|im_end|>\n",
    "```\n",
    "\n",
    "## Investigation Tools\n",
    "\n",
    "| Tool | Description |\n",
    "|------|-------------|\n",
    "| `get_account_summary` | Get account metadata and risk assessment |\n",
    "| `get_recent_transactions` | Get top-5 recent transaction flows |\n",
    "| `check_sanctions_list` | Verify against OFAC watchlist |\n",
    "| `submit_sar` | Terminal action - Submit Suspicious Activity Report |\n",
    "\n",
    "## Win Condition\n",
    "`submit_sar` on an entity that is **both sanctioned AND reachable via a laundering path** from the seed account.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# AML INVESTIGATION AGENT (Qwen3-30B-A3B-Instruct) - Setup & Dependencies\n",
    "# ============================================================================\n",
    "\n",
    "# CRITICAL: Import Unsloth FIRST before any other ML libraries\n",
    "# This ensures all optimizations are applied correctly\n",
    "# NOTE: For MOE models (like Qwen3-30B-A3B), use FastModel instead of FastLanguageModel\n",
    "#       See: https://unsloth.ai/docs/models/qwen3-how-to-run-and-fine-tune/qwen3-2507\n",
    "from unsloth import FastModel\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Any, Tuple, Optional, Annotated, Literal, TypedDict\n",
    "import operator\n",
    "from pathlib import Path\n",
    "\n",
    "# Numerical & Data Processing\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# ML & Deep Learning (imported AFTER unsloth)\n",
    "import torch\n",
    "\n",
    "# Environment\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Model Configuration\n",
    "# Available Qwen3-2507 models (MOE = Mixture of Experts):\n",
    "#   - \"unsloth/Qwen3-4B-Thinking-2507\"                    # ~3GB VRAM - Dense, small\n",
    "#   - \"unsloth/Qwen3-30B-A3B-Thinking-2507\"               # ~17GB VRAM - MOE (Thinking mode)\n",
    "#   - \"unsloth/Qwen3-30B-A3B-Instruct-2507\"               # ~17GB VRAM - MOE (Instruct mode)\n",
    "#   - \"unsloth/Qwen3-235B-A22B-Thinking-2507\"             # Multi-GPU - MOE (Thinking mode)\n",
    "#   - \"unsloth/Qwen3-235B-A22B-Instruct-2507\"             # Multi-GPU - MOE (Instruct mode)\n",
    "#\n",
    "# IMPORTANT: MOE models require FastModel (not FastLanguageModel)\n",
    "# IMPORTANT: Use Instruct model for faster inference with tool calling\n",
    "# Instruct models are designed for fast, direct responses with tool calling support\n",
    "# Thinking models are VERY slow because they generate long reasoning traces\n",
    "MODEL_NAME = \"unsloth/Qwen3-30B-A3B-Instruct-2507\"\n",
    "IS_THINKING_MODEL = \"Thinking\" in MODEL_NAME  # Auto-detect thinking vs instruct\n",
    "GEMINI_MODEL = \"gemini-2.0-flash\"    # LLM-as-Judge\n",
    "\n",
    "# Qwen3-2507 Recommended Generation Settings (from Unsloth docs)\n",
    "# https://unsloth.ai/docs/models/qwen3-how-to-run-and-fine-tune/qwen3-2507\n",
    "if IS_THINKING_MODEL:\n",
    "    # Thinking model settings\n",
    "    GENERATION_TEMPERATURE = 0.6\n",
    "    GENERATION_TOP_P = 0.95\n",
    "    GENERATION_TOP_K = 20\n",
    "    GENERATION_MIN_P = 0.0\n",
    "else:\n",
    "    # Instruct model settings (recommended for tool calling)\n",
    "    GENERATION_TEMPERATURE = 0.7\n",
    "    GENERATION_TOP_P = 0.8\n",
    "    GENERATION_TOP_K = 20\n",
    "    GENERATION_MIN_P = 0.0\n",
    "\n",
    "# Agent Configuration\n",
    "MAX_STEPS = 50                        # Max steps per investigation\n",
    "MAX_HISTORY_TURNS = 6                 # Conversation history limit\n",
    "\n",
    "# Training Configuration - SFT (Supervised Fine-Tuning)\n",
    "SFT_EPOCHS = 3\n",
    "SFT_LEARNING_RATE = 2e-4\n",
    "\n",
    "# Training Configuration - DPO (Direct Preference Optimization)\n",
    "# DPO uses full episode trajectories to learn preferences between\n",
    "# successful and failed investigations (trajectory-level learning)\n",
    "DPO_EPOCHS = 1\n",
    "DPO_LEARNING_RATE = 5e-7              # Lower LR for DPO stability\n",
    "DPO_BETA = 0.1                        # KL penalty coefficient (controls deviation from reference model)\n",
    "DPO_TRACE_EPISODES = 50               # Episodes to collect for DPO training data\n",
    "DPO_MAX_PAIRS = 100                   # Maximum preference pairs to create\n",
    "RECOMPUTE_DPO_DATASET = False         # If True, regenerate DPO dataset; if False, use cached if available\n",
    "\n",
    "# LoRA Configuration\n",
    "LORA_R = 32                           # Higher rank for complex reasoning\n",
    "LORA_ALPHA = 64\n",
    "LORA_TARGET_MODULES = [\n",
    "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "    \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "]\n",
    "\n",
    "# Evaluation Configuration\n",
    "EVAL_EPISODES = 10                    # Episodes per evaluation stage\n",
    "\n",
    "# Random Seed\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Paths (relative to notebook location)\n",
    "# Notebook is at: notebooks/agents/aml_investigation_agent_v2.ipynb\n",
    "# Project root is 2 levels up\n",
    "NOTEBOOK_DIR = Path(\".\").resolve()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent.parent  # Go up from notebooks/agents to project root\n",
    "DATA_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"outputs\"\n",
    "\n",
    "# Ensure directories exist\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dataset Selection\n",
    "DATASET_SIZE = \"Small\"   # Options: \"Small\", \"Medium\", \"Large\"\n",
    "DATASET_PREFIX = \"LI\"    # Options: \"LI\" (Low Illicit), \"HI\" (High Illicit)\n",
    "\n",
    "# ============================================================================\n",
    "# RESULTS STORAGE - For comparison across training stages\n",
    "# ============================================================================\n",
    "\n",
    "evaluation_results = {\n",
    "    \"baseline\": None,\n",
    "    \"post_sft\": None,\n",
    "    \"post_grpo\": None,\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üîç AML INVESTIGATION AGENT (Qwen3-30B-A3B-Instruct) - Configuration\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Model:           {MODEL_NAME}\")\n",
    "print(f\"  Judge:           {GEMINI_MODEL}\")\n",
    "print(f\"  Dataset:         {DATASET_PREFIX}-{DATASET_SIZE}\")\n",
    "print(f\"  Max Steps:       {MAX_STEPS}\")\n",
    "print(f\"  Eval Episodes:   {EVAL_EPISODES}\")\n",
    "print(f\"  LoRA Rank:       {LORA_R}\")\n",
    "print(f\"  LoRA Alpha:      {LORA_ALPHA}\")\n",
    "print(f\"  Random Seed:     {RANDOM_SEED}\")\n",
    "print(f\"  Project Root:    {PROJECT_ROOT}\")\n",
    "print(f\"  Data Dir:        {DATA_DIR}\")\n",
    "print(f\"  Models Dir:      {MODELS_DIR}\")\n",
    "print(f\"  GPU Available:   {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU Device:      {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  GPU Memory:      {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "print(f\"  Thinking Model:  {IS_THINKING_MODEL}\")\n",
    "print(f\"  Temperature:     {GENERATION_TEMPERATURE}\")\n",
    "print(f\"  Top-P:           {GENERATION_TOP_P}\")\n",
    "print(f\"  Top-K:           {GENERATION_TOP_K}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading with Polars\n",
    "\n",
    "Download the IBM AML dataset from Kaggle (if not already present) and load using Polars for high-performance data manipulation.\n",
    "\n",
    "**Dataset**: [IBM Transactions for Anti Money Laundering (AML)](https://www.kaggle.com/datasets/ealtman2019/ibm-transactions-for-anti-money-laundering-aml/data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DOWNLOAD DATASET FROM KAGGLE - IBM AML Transactions Dataset\n",
    "# Dataset: https://www.kaggle.com/datasets/ealtman2019/ibm-transactions-for-anti-money-laundering-aml\n",
    "# ============================================================================\n",
    "\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "# Kaggle dataset identifier\n",
    "KAGGLE_DATASET = \"ealtman2019/ibm-transactions-for-anti-money-laundering-aml\"\n",
    "\n",
    "# Check if required files exist\n",
    "trans_file = DATA_DIR / f\"{DATASET_PREFIX}-{DATASET_SIZE}_Trans.csv\"\n",
    "accounts_file = DATA_DIR / f\"{DATASET_PREFIX}-{DATASET_SIZE}_accounts.csv\"\n",
    "patterns_file = DATA_DIR / f\"{DATASET_PREFIX}-{DATASET_SIZE}_Patterns.txt\"\n",
    "\n",
    "files_exist = trans_file.exists() and accounts_file.exists() and patterns_file.exists()\n",
    "\n",
    "if files_exist:\n",
    "    print(f\"‚úì Dataset already exists at {DATA_DIR}\")\n",
    "    print(f\"  - Transactions: {trans_file.name}\")\n",
    "    print(f\"  - Accounts: {accounts_file.name}\")\n",
    "    print(f\"  - Patterns: {patterns_file.name}\")\n",
    "else:\n",
    "    print(f\"üì• Dataset not found. Downloading from Kaggle...\")\n",
    "    print(f\"   Dataset: {KAGGLE_DATASET}\")\n",
    "    \n",
    "    # Ensure data directory exists\n",
    "    DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Import and authenticate Kaggle API\n",
    "        from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "        \n",
    "        api = KaggleApi()\n",
    "        api.authenticate()\n",
    "        \n",
    "        print(f\"   ‚úì Kaggle API authenticated\")\n",
    "        \n",
    "        # Download dataset\n",
    "        print(f\"   Downloading dataset to {DATA_DIR}...\")\n",
    "        api.dataset_download_files(\n",
    "            dataset=KAGGLE_DATASET,\n",
    "            path=str(DATA_DIR),\n",
    "            unzip=True,\n",
    "            quiet=False\n",
    "        )\n",
    "        \n",
    "        print(f\"   ‚úì Download complete!\")\n",
    "        \n",
    "        # List downloaded files\n",
    "        print(f\"\\n   Downloaded files:\")\n",
    "        for f in sorted(DATA_DIR.glob(\"*\")):\n",
    "            size_mb = f.stat().st_size / (1024 * 1024)\n",
    "            print(f\"     - {f.name} ({size_mb:.1f} MB)\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"   ‚ùå Kaggle package not installed.\")\n",
    "        print(\"   Run: pip install kaggle\")\n",
    "        print(\"   Then set up ~/.kaggle/kaggle.json with your API credentials\")\n",
    "        raise ImportError(\"Please install kaggle package: pip install kaggle\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error downloading dataset: {e}\")\n",
    "        print(\"\\n   Manual download instructions:\")\n",
    "        print(f\"   1. Visit: https://www.kaggle.com/datasets/{KAGGLE_DATASET}\")\n",
    "        print(f\"   2. Download and extract to: {DATA_DIR}\")\n",
    "        raise\n",
    "\n",
    "# Verify files exist after download\n",
    "assert trans_file.exists(), f\"Transaction file not found: {trans_file}\"\n",
    "assert accounts_file.exists(), f\"Accounts file not found: {accounts_file}\"\n",
    "assert patterns_file.exists(), f\"Patterns file not found: {patterns_file}\"\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"‚úì DATASET READY: {DATASET_PREFIX}-{DATASET_SIZE}\")\n",
    "print(f\"{'=' * 60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA LOADING - IBM AML Dataset with Polars\n",
    "# ============================================================================\n",
    "# FIX: The transactions file has LEADING ZEROS on bank IDs (e.g., \"001120\")\n",
    "# but the accounts file stores them without (e.g., \"1120\"). We normalize by\n",
    "# casting bank IDs to Int64 first, which strips leading zeros.\n",
    "\n",
    "print(\"üìä Loading IBM AML Dataset with Polars...\")\n",
    "\n",
    "# Load transactions\n",
    "raw_trans_pl = pl.read_csv(\n",
    "    trans_file,\n",
    "    new_columns=[\n",
    "        'timestamp', 'from_bank', 'from_account', 'to_bank', 'to_account',\n",
    "        'amount_received', 'receiving_currency', 'amount_paid',\n",
    "        'payment_currency', 'payment_format', 'is_laundering'\n",
    "    ],\n",
    "    skip_rows=1\n",
    ")\n",
    "print(f\"‚úì Loaded {len(raw_trans_pl):,} transactions\")\n",
    "\n",
    "# Load accounts\n",
    "raw_accounts_pl = pl.read_csv(accounts_file)\n",
    "print(f\"‚úì Loaded {len(raw_accounts_pl):,} accounts\")\n",
    "\n",
    "# Process transactions - Create unique account IDs\n",
    "# CRITICAL FIX: Cast bank IDs to Int64 first to remove leading zeros, then to Utf8\n",
    "transactions_pl = raw_trans_pl.with_columns([\n",
    "    (pl.col('from_bank').cast(pl.Int64).cast(pl.Utf8) + '-' + pl.col('from_account').cast(pl.Utf8)).alias('from_account_id'),\n",
    "    (pl.col('to_bank').cast(pl.Int64).cast(pl.Utf8) + '-' + pl.col('to_account').cast(pl.Utf8)).alias('to_account_id'),\n",
    "    (pl.lit('TXN-') + pl.arange(0, pl.len()).cast(pl.Utf8)).alias('transaction_id'),\n",
    "    pl.col('is_laundering').cast(pl.Int32),\n",
    "]).select([\n",
    "    'transaction_id',\n",
    "    pl.col('from_account_id').alias('from_account'),\n",
    "    pl.col('to_account_id').alias('to_account'),\n",
    "    pl.col('amount_received').alias('amount'),\n",
    "    pl.col('receiving_currency').alias('currency'),\n",
    "    'timestamp', 'is_laundering', 'payment_format',\n",
    "])\n",
    "\n",
    "# Identify laundering destinations for sanctioned marking\n",
    "laundering_dests = set(\n",
    "    transactions_pl.filter(pl.col('is_laundering') == 1)['to_account'].unique().to_list()\n",
    ")\n",
    "print(f\"‚úì Found {len(laundering_dests):,} laundering destination accounts\")\n",
    "\n",
    "# Process accounts - Add risk scores and sanctioned flags\n",
    "# Bank ID in accounts file is already without leading zeros, so just cast to Utf8\n",
    "accounts_pl = raw_accounts_pl.rename({\n",
    "    'Bank Name': 'bank_name', 'Bank ID': 'bank_id',\n",
    "    'Account Number': 'account_number', 'Entity ID': 'entity_id', 'Entity Name': 'entity_name'\n",
    "}).with_columns([\n",
    "    (pl.col('bank_id').cast(pl.Utf8) + '-' + pl.col('account_number').cast(pl.Utf8)).alias('account_id'),\n",
    "    pl.when(pl.col('entity_name').str.contains('Corporation')).then(pl.lit('Corporate'))\n",
    "        .when(pl.col('entity_name').str.contains('Partnership')).then(pl.lit('Partnership'))\n",
    "        .when(pl.col('entity_name').str.contains('Sole Proprietorship')).then(pl.lit('Individual'))\n",
    "        .otherwise(pl.lit('Unknown')).alias('account_type'),\n",
    "])\n",
    "\n",
    "# Add sanctioned flag (30% of laundering destinations) and risk scores\n",
    "account_ids = accounts_pl['account_id'].to_list()\n",
    "accounts_pl = accounts_pl.with_columns([\n",
    "    pl.Series('is_sanctioned', [acc in laundering_dests and random.random() < 0.3 for acc in account_ids]),\n",
    "    pl.Series('risk_score', [round(random.uniform(0.1, 0.9), 2) for _ in range(len(account_ids))]),\n",
    "]).select(['account_id', 'bank_id', 'bank_name', 'entity_id', 'entity_name', 'account_type', 'risk_score', 'is_sanctioned'])\n",
    "\n",
    "# Convert to pandas for NetworkX\n",
    "transactions_df = transactions_pl.to_pandas()\n",
    "accounts_df = accounts_pl.to_pandas()\n",
    "\n",
    "# Summary\n",
    "n_accounts, n_transactions = len(accounts_df), len(transactions_df)\n",
    "n_laundering = int(transactions_df['is_laundering'].sum())\n",
    "n_sanctioned = int(accounts_df['is_sanctioned'].sum())\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"üìä DATASET SUMMARY: {DATASET_PREFIX}-{DATASET_SIZE}\")\n",
    "print(f\"{'=' * 60}\")\n",
    "print(f\"  Accounts:             {n_accounts:>12,}\")\n",
    "print(f\"  Transactions:         {n_transactions:>12,}\")\n",
    "print(f\"  Laundering Txns:      {n_laundering:>12,} ({n_laundering/n_transactions*100:.2f}%)\")\n",
    "print(f\"  Sanctioned Accounts:  {n_sanctioned:>12,}\")\n",
    "print(f\"  Laundering Dests:     {len(laundering_dests):>12,}\")\n",
    "print(f\"{'=' * 60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PARSE LAUNDERING PATTERNS - Extract Pattern Seeds for Training\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class LaunderingPattern:\n",
    "    \"\"\"Represents a single money laundering pattern from the dataset.\"\"\"\n",
    "    pattern_type: str\n",
    "    pattern_info: str\n",
    "    transactions: List[dict]\n",
    "    accounts_involved: set\n",
    "    \n",
    "    @property\n",
    "    def seed_account(self) -> str:\n",
    "        return self.transactions[0].get('from_account', '') if self.transactions else ''\n",
    "    \n",
    "    @property\n",
    "    def terminal_account(self) -> str:\n",
    "        return self.transactions[-1].get('to_account', '') if self.transactions else ''\n",
    "    \n",
    "    @property\n",
    "    def total_amount(self) -> float:\n",
    "        return sum(t.get('amount', 0) for t in self.transactions)\n",
    "    \n",
    "    @property\n",
    "    def hop_count(self) -> int:\n",
    "        return len(self.transactions)\n",
    "\n",
    "\n",
    "def parse_patterns_file(filepath: Path) -> List[LaunderingPattern]:\n",
    "    \"\"\"Parse patterns file to extract laundering patterns.\"\"\"\n",
    "    patterns = []\n",
    "    current_pattern = None\n",
    "    \n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            \n",
    "            if line.startswith('BEGIN LAUNDERING ATTEMPT'):\n",
    "                match = re.match(r'BEGIN LAUNDERING ATTEMPT - (\\w+(?:-\\w+)?):?\\s*(.*)', line)\n",
    "                if match:\n",
    "                    current_pattern = LaunderingPattern(\n",
    "                        pattern_type=match.group(1),\n",
    "                        pattern_info=match.group(2).strip() if match.group(2) else \"\",\n",
    "                        transactions=[], accounts_involved=set()\n",
    "                    )\n",
    "            \n",
    "            elif line.startswith('END LAUNDERING ATTEMPT'):\n",
    "                if current_pattern and current_pattern.transactions:\n",
    "                    patterns.append(current_pattern)\n",
    "                current_pattern = None\n",
    "            \n",
    "            elif current_pattern and line and not line.startswith('BEGIN') and not line.startswith('END'):\n",
    "                parts = line.split(',')\n",
    "                if len(parts) >= 7:\n",
    "                    try:\n",
    "                        # CRITICAL FIX: Normalize bank IDs by converting to int first\n",
    "                        # This removes leading zeros (e.g., \"001120\" -> \"1120\")\n",
    "                        from_bank = str(int(parts[1].strip()))\n",
    "                        to_bank = str(int(parts[3].strip()))\n",
    "                        from_account = f\"{from_bank}-{parts[2].strip()}\"\n",
    "                        to_account = f\"{to_bank}-{parts[4].strip()}\"\n",
    "                        amount = float(parts[5].strip())\n",
    "                        \n",
    "                        current_pattern.transactions.append({\n",
    "                            'timestamp': parts[0].strip(),\n",
    "                            'from_account': from_account, 'to_account': to_account,\n",
    "                            'amount': amount, 'currency': parts[6].strip(),\n",
    "                        })\n",
    "                        current_pattern.accounts_involved.add(from_account)\n",
    "                        current_pattern.accounts_involved.add(to_account)\n",
    "                    except (ValueError, IndexError):\n",
    "                        pass\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "\n",
    "# Parse patterns\n",
    "laundering_patterns = parse_patterns_file(patterns_file)\n",
    "\n",
    "# Statistics\n",
    "pattern_types = {}\n",
    "for p in laundering_patterns:\n",
    "    pattern_types[p.pattern_type] = pattern_types.get(p.pattern_type, 0) + 1\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"üîó LAUNDERING PATTERNS PARSED\")\n",
    "print(f\"{'=' * 60}\")\n",
    "print(f\"  Total Patterns: {len(laundering_patterns):,}\")\n",
    "for ptype, count in sorted(pattern_types.items(), key=lambda x: -x[1]):\n",
    "    print(f\"    - {ptype:<20} {count:>6,}\")\n",
    "print(f\"{'=' * 60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Financial Environment with MLflow-Instrumented Tools\n",
    "\n",
    "Build the transaction graph with NetworkX and create the `FinancialEnvironment` class with MLflow-traced tool functions.\n",
    "\n",
    "**Note**: The tool implementations return JSON-serializable dictionaries that can be used with Qwen3's Hermes-style tool calling format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FINANCIAL ENVIRONMENT - NetworkX Graph with MLflow-Traced Tools\n",
    "# ============================================================================\n",
    "\n",
    "import mlflow\n",
    "\n",
    "mlflow.set_experiment(\"AML_Investigation_Agent_v2\")\n",
    "print(\"‚úì MLflow experiment: AML_Investigation_Agent_v2\")\n",
    "\n",
    "\n",
    "@dataclass \n",
    "class FinancialEnvironment:\n",
    "    \"\"\"Financial investigation environment with path-validated SAR evaluation.\"\"\"\n",
    "    graph: nx.DiGraph = field(default_factory=nx.DiGraph)\n",
    "    accounts: Dict[str, dict] = field(default_factory=dict)\n",
    "    laundering_targets: List[str] = field(default_factory=list)\n",
    "    all_sanctioned: set = field(default_factory=set)\n",
    "    laundering_destinations: set = field(default_factory=set)\n",
    "    transitive_illicit: set = field(default_factory=set)\n",
    "    current_start_account: str = \"\"\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataframes(cls, transactions_df: pd.DataFrame, accounts_df: pd.DataFrame) -> 'FinancialEnvironment':\n",
    "        env = cls()\n",
    "        \n",
    "        for _, row in accounts_df.iterrows():\n",
    "            env.accounts[row['account_id']] = row.to_dict()\n",
    "            env.graph.add_node(row['account_id'], **row.to_dict())\n",
    "        \n",
    "        for _, row in transactions_df.iterrows():\n",
    "            env.graph.add_edge(\n",
    "                row['from_account'], row['to_account'],\n",
    "                transaction_id=row['transaction_id'], amount=row['amount'],\n",
    "                currency=row.get('currency', 'USD'), timestamp=row['timestamp'],\n",
    "                is_laundering=row['is_laundering']\n",
    "            )\n",
    "        \n",
    "        env.all_sanctioned = set(accounts_df[accounts_df['is_sanctioned']]['account_id'])\n",
    "        laundering_txns = transactions_df[transactions_df['is_laundering'] == 1]\n",
    "        env.laundering_destinations = set(laundering_txns['to_account'].unique())\n",
    "        env._compute_transitive_illicit()\n",
    "        env.laundering_targets = list(env.all_sanctioned & env.laundering_destinations)\n",
    "        \n",
    "        return env\n",
    "    \n",
    "    def _compute_transitive_illicit(self):\n",
    "        self.transitive_illicit = set()\n",
    "        laundering_sources = set()\n",
    "        for u, v, data in self.graph.edges(data=True):\n",
    "            if data.get('is_laundering', 0) == 1:\n",
    "                laundering_sources.add(u)\n",
    "                self.transitive_illicit.add(u)\n",
    "                self.transitive_illicit.add(v)\n",
    "        \n",
    "        for source in laundering_sources:\n",
    "            visited = {source}\n",
    "            queue = [source]\n",
    "            while queue:\n",
    "                node = queue.pop(0)\n",
    "                for neighbor in self.graph.successors(node):\n",
    "                    edge_data = self.graph.edges[node, neighbor]\n",
    "                    if edge_data.get('is_laundering', 0) == 1 and neighbor not in visited:\n",
    "                        visited.add(neighbor)\n",
    "                        self.transitive_illicit.add(neighbor)\n",
    "                        queue.append(neighbor)\n",
    "    \n",
    "    def is_on_laundering_path(self, entity_id: str, max_depth: int = 10) -> bool:\n",
    "        if not self.current_start_account:\n",
    "            return False\n",
    "        try:\n",
    "            for path in nx.all_simple_paths(self.graph, self.current_start_account, entity_id, cutoff=max_depth):\n",
    "                if all(self.graph.edges[path[i], path[i+1]].get('is_laundering', 0) == 1 for i in range(len(path)-1)):\n",
    "                    return True\n",
    "            return False\n",
    "        except (nx.NetworkXNoPath, nx.NodeNotFound):\n",
    "            return False\n",
    "    \n",
    "    @mlflow.trace(span_type=\"TOOL\")\n",
    "    def get_account_summary(self, account_id: str) -> dict:\n",
    "        if account_id not in self.accounts:\n",
    "            return {\"error\": f\"Account {account_id} not found\"}\n",
    "        acc = self.accounts[account_id]\n",
    "        return {\n",
    "            \"account_id\": account_id, \"account_type\": acc.get('account_type', 'Unknown'),\n",
    "            \"entity_name\": acc.get('entity_name', 'Unknown'), \"bank_name\": acc.get('bank_name', 'Unknown'),\n",
    "            \"risk_score\": round(acc.get('risk_score', 0), 2), \"is_sanctioned\": acc.get('is_sanctioned', False),\n",
    "            \"transitive_illicit\": account_id in self.transitive_illicit,\n",
    "        }\n",
    "    \n",
    "    @mlflow.trace(span_type=\"TOOL\")\n",
    "    def get_recent_transactions(self, account_id: str, direction: str = \"outgoing\", limit: int = 5) -> List[dict]:\n",
    "        if account_id not in self.graph:\n",
    "            return []\n",
    "        edges = list(self.graph.out_edges(account_id, data=True) if direction == \"outgoing\" \n",
    "                     else self.graph.in_edges(account_id, data=True))\n",
    "        edges = sorted(edges, key=lambda e: e[2].get('amount', 0), reverse=True)[:limit]\n",
    "        \n",
    "        results = []\n",
    "        for edge in edges:\n",
    "            target = edge[1] if direction == \"outgoing\" else edge[0]\n",
    "            results.append({\n",
    "                \"counterparty\": target, \"amount\": round(edge[2].get('amount', 0), 2),\n",
    "                \"currency\": edge[2].get('currency', 'USD'), \"is_laundering\": edge[2].get('is_laundering', 0),\n",
    "                \"high_risk_indicator\": target in self.transitive_illicit,\n",
    "            })\n",
    "        return results\n",
    "    \n",
    "    @mlflow.trace(span_type=\"TOOL\")\n",
    "    def check_sanctions_list(self, entity_id: str) -> dict:\n",
    "        is_sanctioned = entity_id in self.all_sanctioned\n",
    "        return {\"entity_id\": entity_id, \"on_sanctions_list\": is_sanctioned, \"list_type\": \"OFAC SDN\" if is_sanctioned else None}\n",
    "    \n",
    "    @mlflow.trace(span_type=\"TOOL\")\n",
    "    def submit_sar(self, entity_id: str, reason: str) -> dict:\n",
    "        is_sanctioned = entity_id in self.all_sanctioned\n",
    "        is_primary = entity_id in self.laundering_targets\n",
    "        on_path = self.is_on_laundering_path(entity_id)\n",
    "        correct = is_primary or (is_sanctioned and on_path)\n",
    "        \n",
    "        if is_primary:\n",
    "            eval_reason = \"PRIMARY_TARGET: sanctioned + receives laundering directly\"\n",
    "        elif is_sanctioned and on_path:\n",
    "            eval_reason = f\"VALID: sanctioned + on laundering path from {self.current_start_account}\"\n",
    "        elif is_sanctioned:\n",
    "            eval_reason = \"INVALID: sanctioned but NOT on laundering path from start\"\n",
    "        else:\n",
    "            eval_reason = \"INVALID: entity is not sanctioned\"\n",
    "        \n",
    "        return {\n",
    "            \"entity_id\": entity_id, \"reason\": reason, \"report_id\": f\"SAR-{uuid.uuid4().hex[:8].upper()}\",\n",
    "            \"correct_identification\": correct, \"is_sanctioned\": is_sanctioned,\n",
    "            \"is_primary_target\": is_primary, \"on_laundering_path\": on_path, \"evaluation_reason\": eval_reason,\n",
    "        }\n",
    "    \n",
    "    def reset_investigation(self, start_account: str):\n",
    "        self.current_start_account = start_account\n",
    "\n",
    "\n",
    "# Build environment\n",
    "env = FinancialEnvironment.from_dataframes(transactions_df, accounts_df)\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"üè¶ FINANCIAL ENVIRONMENT BUILT\")\n",
    "print(f\"{'=' * 60}\")\n",
    "print(f\"  Graph Nodes:          {env.graph.number_of_nodes():>12,}\")\n",
    "print(f\"  Graph Edges:          {env.graph.number_of_edges():>12,}\")\n",
    "print(f\"  Sanctioned Accounts:  {len(env.all_sanctioned):>12,}\")\n",
    "print(f\"  Primary Targets:      {len(env.laundering_targets):>12,}\")\n",
    "print(f\"  Transitive Illicit:   {len(env.transitive_illicit):>12,}\")\n",
    "print(f\"{'=' * 60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Data Verification: Pattern Reachability\n",
    "\n",
    "Verify that laundering patterns lead to sanctioned entities that the agent can discover. This is critical for evaluating agent success rates.\n",
    "\n",
    "**Key Metrics:**\n",
    "- **Laundering destination coverage**: Do destination accounts from transactions exist in the accounts file?\n",
    "- **Pattern reachability**: What % of patterns have at least one sanctioned entity reachable via `is_laundering=1` edges?\n",
    "- **Primary targets**: The intersection of sanctioned accounts and laundering destinations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA VERIFICATION: Check if patterns lead to sanctioned entities\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üîç VERIFICATION: Can agents reach sanctioned entities from seeds?\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check 1: How many laundering destinations exist in accounts_df?\n",
    "all_account_ids = set(accounts_df['account_id'])\n",
    "laund_dests_in_accounts = laundering_dests & all_account_ids\n",
    "laund_dests_missing = laundering_dests - all_account_ids\n",
    "\n",
    "print(f\"\\nüìä Laundering Destination Coverage:\")\n",
    "print(f\"  Total laundering destinations:    {len(laundering_dests):,}\")\n",
    "print(f\"  Found in accounts_df:             {len(laund_dests_in_accounts):,} ({100*len(laund_dests_in_accounts)/len(laundering_dests):.1f}%)\")\n",
    "print(f\"  Missing from accounts_df:         {len(laund_dests_missing):,}\")\n",
    "\n",
    "# Helper: Get all destination accounts from a pattern (not just the last one)\n",
    "def get_pattern_destinations(pattern):\n",
    "    \"\"\"Get all destination accounts from pattern transactions.\"\"\"\n",
    "    return list(set(t.get('to_account', '') for t in pattern.transactions if t.get('to_account')))\n",
    "\n",
    "# Check 2: How many patterns have at least one reachable sanctioned entity?\n",
    "patterns_with_sanctioned = 0\n",
    "patterns_without_sanctioned = 0\n",
    "patterns_with_missing_terminals = 0\n",
    "valid_patterns = []  # Patterns with at least one reachable sanctioned entity\n",
    "\n",
    "for pattern in laundering_patterns:\n",
    "    # Get all destination accounts in this pattern\n",
    "    terminals = get_pattern_destinations(pattern)\n",
    "    terminals_in_accounts = [t for t in terminals if t in all_account_ids]\n",
    "    terminals_sanctioned = [t for t in terminals if t in env.all_sanctioned]\n",
    "    \n",
    "    if len(terminals) > len(terminals_in_accounts):\n",
    "        patterns_with_missing_terminals += 1\n",
    "    \n",
    "    # Check if any terminal is reachable via laundering path\n",
    "    seed = pattern.seed_account\n",
    "    reachable_sanctioned = []\n",
    "    if seed in env.graph:\n",
    "        for t in terminals_sanctioned:\n",
    "            if t in env.graph:\n",
    "                try:\n",
    "                    for path in nx.all_simple_paths(env.graph, seed, t, cutoff=15):\n",
    "                        if all(env.graph.edges[path[j], path[j+1]].get('is_laundering', 0) == 1 \n",
    "                               for j in range(len(path)-1)):\n",
    "                            reachable_sanctioned.append(t)\n",
    "                            break\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    if reachable_sanctioned:\n",
    "        patterns_with_sanctioned += 1\n",
    "        valid_patterns.append(pattern)\n",
    "    else:\n",
    "        patterns_without_sanctioned += 1\n",
    "\n",
    "print(f\"\\nüìä Pattern Reachability to Sanctioned Entities:\")\n",
    "print(f\"  Total patterns:                   {len(laundering_patterns):,}\")\n",
    "print(f\"  Patterns with ‚â•1 sanctioned:      {patterns_with_sanctioned:,} ({100*patterns_with_sanctioned/len(laundering_patterns):.1f}%)\")\n",
    "print(f\"  Patterns with 0 sanctioned:       {patterns_without_sanctioned:,} ({100*patterns_without_sanctioned/len(laundering_patterns):.1f}%)\")\n",
    "print(f\"  Patterns with missing terminals:  {patterns_with_missing_terminals:,}\")\n",
    "\n",
    "# Check 3: Primary targets (intersection)\n",
    "print(f\"\\nüìä Primary Targets (sanctioned ‚à© laundering_destinations):\")\n",
    "print(f\"  Total sanctioned accounts:        {len(env.all_sanctioned):,}\")\n",
    "print(f\"  Total laundering destinations:    {len(env.laundering_destinations):,}\")\n",
    "print(f\"  Primary targets (intersection):   {len(env.laundering_targets):,}\")\n",
    "\n",
    "# Check 4: Sample verification for a few patterns\n",
    "print(f\"\\nüìã Sample Pattern Analysis (first 5):\")\n",
    "for i, pattern in enumerate(laundering_patterns[:5]):\n",
    "    seed = pattern.seed_account\n",
    "    terminals = get_pattern_destinations(pattern)\n",
    "    \n",
    "    # Check if seed exists\n",
    "    seed_exists = seed in env.accounts\n",
    "    seed_in_graph = seed in env.graph\n",
    "    \n",
    "    # Check terminals\n",
    "    terminals_sanctioned = [t for t in terminals if t in env.all_sanctioned]\n",
    "    terminals_in_accounts = [t for t in terminals if t in env.accounts]\n",
    "    \n",
    "    # Check reachability via is_laundering=1 path\n",
    "    reachable_sanctioned = []\n",
    "    if seed_in_graph:\n",
    "        for t in terminals_sanctioned:\n",
    "            if t in env.graph:\n",
    "                try:\n",
    "                    for path in nx.all_simple_paths(env.graph, seed, t, cutoff=15):\n",
    "                        if all(env.graph.edges[path[j], path[j+1]].get('is_laundering', 0) == 1 \n",
    "                               for j in range(len(path)-1)):\n",
    "                            reachable_sanctioned.append(t)\n",
    "                            break\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    status = \"‚úÖ\" if reachable_sanctioned else \"‚ùå\"\n",
    "    print(f\"\\n  Pattern {i+1} ({pattern.pattern_type}):\")\n",
    "    print(f\"    Seed: {seed} (in accounts: {seed_exists}, in graph: {seed_in_graph})\")\n",
    "    print(f\"    Terminals: {len(terminals)} total, {len(terminals_in_accounts)} in accounts, {len(terminals_sanctioned)} sanctioned\")\n",
    "    print(f\"    Reachable sanctioned: {len(reachable_sanctioned)} {status}\")\n",
    "\n",
    "# Store valid patterns for later use\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(f\"üìå RECOMMENDATION:\")\n",
    "if patterns_with_sanctioned / len(laundering_patterns) < 0.5:\n",
    "    print(f\"   ‚ö†Ô∏è  Only {100*patterns_with_sanctioned/len(laundering_patterns):.1f}% of patterns have reachable sanctioned entities.\")\n",
    "    print(f\"   Consider using 'valid_patterns' ({len(valid_patterns)}) for evaluation instead of all patterns.\")\n",
    "    print(f\"   This ensures the agent CAN succeed if it follows the correct strategy.\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ {100*patterns_with_sanctioned/len(laundering_patterns):.1f}% of patterns have reachable sanctioned entities.\")\n",
    "    print(f\"   Pattern coverage is sufficient for training and evaluation.\")\n",
    "print(f\"{'=' * 70}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Training Needs Assessment\n",
    "\n",
    "Before investing in fine-tuning, we should understand **what problems we're solving**:\n",
    "\n",
    "| Training Stage | Purpose | When Needed |\n",
    "|----------------|---------|-------------|\n",
    "| **No Training** | Use base Qwen3 instruct model | If base model already achieves >70% success with correct tool format |\n",
    "| **SFT Only** | Teach tool format + basic strategy | If model struggles with tool calling syntax or basic reasoning |\n",
    "| **DPO Only** | Optimize decision-making with trajectory preferences | If model uses tools correctly but makes poor strategic choices |\n",
    "| **SFT + DPO** | Full pipeline | If both format AND strategy improvements are needed |\n",
    "\n",
    "**Decision Framework:**\n",
    "1. Run baseline evaluation on `valid_patterns`\n",
    "2. Analyze failure modes:\n",
    "   - **Format errors**: Wrong tool call syntax ‚Üí Needs SFT\n",
    "   - **Wrong tool choice**: Correct syntax but poor decisions ‚Üí Needs DPO\n",
    "   - **Premature SAR**: Files SAR without checking sanctions ‚Üí Needs both\n",
    "   - **Gets stuck**: Doesn't progress investigation ‚Üí Needs DPO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAINING NEEDS ASSESSMENT - Analyze baseline to determine training strategy\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_training_needs(episodes: List['InvestigationEpisode']) -> dict:\n",
    "    \"\"\"\n",
    "    Analyze baseline evaluation results to determine what training is needed.\n",
    "    \n",
    "    Returns dict with:\n",
    "    - needs_sft: bool - whether SFT training would help\n",
    "    - needs_dpo: bool - whether DPO training would help  \n",
    "    - failure_analysis: dict - breakdown of failure modes\n",
    "    - recommendation: str - suggested training approach\n",
    "    \"\"\"\n",
    "    if not episodes:\n",
    "        return {\"error\": \"No episodes to analyze\"}\n",
    "    \n",
    "    # Initialize counters\n",
    "    total = len(episodes)\n",
    "    successes = sum(1 for e in episodes if e.success)\n",
    "    \n",
    "    # Failure mode analysis\n",
    "    format_errors = 0      # Failed to generate valid tool calls\n",
    "    wrong_tool_choice = 0  # Valid format but chose wrong tool\n",
    "    premature_sar = 0      # Filed SAR without checking sanctions\n",
    "    no_progress = 0        # Got stuck, didn't explore graph\n",
    "    missed_target = 0      # Explored correctly but missed sanctioned entity\n",
    "    \n",
    "    for episode in episodes:\n",
    "        if episode.success:\n",
    "            continue\n",
    "            \n",
    "        steps = episode.steps\n",
    "        if not steps:\n",
    "            no_progress += 1\n",
    "            continue\n",
    "        \n",
    "        # Count tool usage\n",
    "        tool_counts = {}\n",
    "        has_format_error = False\n",
    "        filed_sar = False\n",
    "        checked_sanctions = False\n",
    "        explored_transactions = False\n",
    "        \n",
    "        for step in steps:\n",
    "            tool_name = step.get('tool_name', '')\n",
    "            result = step.get('result', {})\n",
    "            \n",
    "            if tool_name == 'fallback':\n",
    "                has_format_error = True\n",
    "            elif tool_name:\n",
    "                tool_counts[tool_name] = tool_counts.get(tool_name, 0) + 1\n",
    "                \n",
    "                if tool_name == 'submit_sar':\n",
    "                    filed_sar = True\n",
    "                elif tool_name == 'check_sanctions_list':\n",
    "                    checked_sanctions = True\n",
    "                elif tool_name == 'get_recent_transactions':\n",
    "                    explored_transactions = True\n",
    "        \n",
    "        # Classify failure mode\n",
    "        if has_format_error and len(steps) < 3:\n",
    "            format_errors += 1\n",
    "        elif filed_sar and not checked_sanctions:\n",
    "            premature_sar += 1\n",
    "        elif not explored_transactions:\n",
    "            no_progress += 1\n",
    "        elif checked_sanctions and explored_transactions:\n",
    "            missed_target += 1  # Did everything right but still failed\n",
    "        else:\n",
    "            wrong_tool_choice += 1\n",
    "    \n",
    "    # Calculate rates\n",
    "    success_rate = successes / total if total > 0 else 0\n",
    "    format_error_rate = format_errors / total if total > 0 else 0\n",
    "    strategy_error_rate = (wrong_tool_choice + premature_sar + no_progress) / total if total > 0 else 0\n",
    "    \n",
    "    # Determine training needs\n",
    "    needs_sft = format_error_rate > 0.1 or premature_sar > 0  # >10% format issues or premature SARs\n",
    "    needs_dpo = strategy_error_rate > 0.2 or missed_target > 0  # >20% strategy issues\n",
    "    \n",
    "    # Generate recommendation\n",
    "    if success_rate >= 0.7:\n",
    "        recommendation = \"NO_TRAINING - Base model performs well (‚â•70% success), DPO optional\"\n",
    "    elif needs_sft and needs_dpo:\n",
    "        recommendation = \"SFT_THEN_DPO - Both format and strategy improvements needed\"\n",
    "    elif needs_sft:\n",
    "        recommendation = \"SFT_ONLY - Tool calling format improvements needed\"\n",
    "    elif needs_dpo:\n",
    "        recommendation = \"DPO_ONLY - Strategic decision-making improvements needed\"\n",
    "    else:\n",
    "        recommendation = \"SFT_THEN_DPO - Default full training pipeline\"\n",
    "    \n",
    "    return {\n",
    "        \"total_episodes\": total,\n",
    "        \"success_rate\": success_rate,\n",
    "        \"needs_sft\": needs_sft,\n",
    "        \"needs_dpo\": needs_dpo,\n",
    "        \"failure_analysis\": {\n",
    "            \"format_errors\": format_errors,\n",
    "            \"wrong_tool_choice\": wrong_tool_choice,\n",
    "            \"premature_sar\": premature_sar,\n",
    "            \"no_progress\": no_progress,\n",
    "            \"missed_target\": missed_target,\n",
    "        },\n",
    "        \"recommendation\": recommendation,\n",
    "    }\n",
    "\n",
    "\n",
    "def print_training_assessment(analysis: dict):\n",
    "    \"\"\"Pretty print the training needs assessment.\"\"\"\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"üìã TRAINING NEEDS ASSESSMENT\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "    \n",
    "    print(f\"\\nüìä Baseline Performance:\")\n",
    "    print(f\"  Success Rate: {analysis['success_rate']*100:.1f}%\")\n",
    "    print(f\"  Total Episodes: {analysis['total_episodes']}\")\n",
    "    \n",
    "    fa = analysis['failure_analysis']\n",
    "    print(f\"\\n‚ùå Failure Mode Breakdown:\")\n",
    "    print(f\"  Format Errors:      {fa['format_errors']:3d}  (tool call syntax issues)\")\n",
    "    print(f\"  Wrong Tool Choice:  {fa['wrong_tool_choice']:3d}  (correct syntax, poor decisions)\")\n",
    "    print(f\"  Premature SAR:      {fa['premature_sar']:3d}  (filed SAR without sanctions check)\")\n",
    "    print(f\"  No Progress:        {fa['no_progress']:3d}  (got stuck, didn't explore)\")\n",
    "    print(f\"  Missed Target:      {fa['missed_target']:3d}  (explored well but missed entity)\")\n",
    "    \n",
    "    print(f\"\\nüéØ Training Recommendation:\")\n",
    "    print(f\"  Needs SFT: {'YES' if analysis['needs_sft'] else 'NO'}\")\n",
    "    print(f\"  Needs DPO: {'YES' if analysis['needs_dpo'] else 'NO'}\")\n",
    "    print(f\"\\n  ‚Üí {analysis['recommendation']}\")\n",
    "    \n",
    "    # Provide actionable guidance\n",
    "    print(f\"\\nüìå Action Items:\")\n",
    "    if \"NO_TRAINING\" in analysis['recommendation']:\n",
    "        print(f\"  1. Base model is sufficient - proceed to deployment\")\n",
    "        print(f\"  2. Consider DPO for further optimization (optional)\")\n",
    "    elif \"SFT\" in analysis['recommendation']:\n",
    "        print(f\"  1. Run SFT training to teach Qwen3 Hermes-style tool calling\")\n",
    "        print(f\"  2. SFT will fix format errors and teach basic investigation flow\")\n",
    "    if \"DPO\" in analysis['recommendation']:\n",
    "        print(f\"  3. Run DPO training to optimize strategic decisions\")\n",
    "        print(f\"  4. DPO will learn from successful/failed trajectories when to explore vs. submit SAR\")\n",
    "    \n",
    "    print(f\"{'=' * 70}\")\n",
    "\n",
    "\n",
    "# Store function for use after baseline evaluation\n",
    "print(\"‚úì Training needs assessment functions defined\")\n",
    "print(\"  - analyze_training_needs(episodes): Analyze failure modes\")\n",
    "print(\"  - print_training_assessment(analysis): Display recommendations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Qwen3 Tool Calling Format (Hermes Style)\n",
    "\n",
    "Implement Qwen3's native Hermes-style tool calling format using `<tool_call>` XML tags.\n",
    "\n",
    "**Key Features:**\n",
    "- Tool definitions use OpenAI-compatible JSON Schema format\n",
    "- Tool calls are wrapped in `<tool_call></tool_call>` tags  \n",
    "- Tool responses are wrapped in `<tool_response></tool_response>` tags\n",
    "- Uses `apply_chat_template` with `tools` parameter for proper formatting\n",
    "- Supports parallel and multi-turn tool calling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# QWEN3 TOOL CALLING FORMAT (Hermes Style)\n",
    "# Ref: https://qwen.readthedocs.io/en/latest/getting_started/concepts.html#tool-calling\n",
    "# Ref: https://github.com/NousResearch/Hermes-Function-Calling\n",
    "# ============================================================================\n",
    "\n",
    "# Define tools in OpenAI-compatible JSON Schema format (used by apply_chat_template)\n",
    "QWEN3_TOOLS = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_account_summary\",\n",
    "            \"description\": \"Get account metadata and risk assessment for an account ID.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"account_id\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The unique account identifier (format: 'bankid-accountnumber')\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"account_id\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_recent_transactions\",\n",
    "            \"description\": \"Get the top-5 recent transactions by amount for an account.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"account_id\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The account to get transactions for\"\n",
    "                    },\n",
    "                    \"direction\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"outgoing\", \"incoming\"],\n",
    "                        \"description\": \"Transaction direction. Defaults to 'outgoing'\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"account_id\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"check_sanctions_list\",\n",
    "            \"description\": \"Check if an entity is on the OFAC sanctions list. ALWAYS call this before submitting SAR!\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"entity_id\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The account/entity ID to check against sanctions\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"entity_id\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"submit_sar\",\n",
    "            \"description\": \"Submit a Suspicious Activity Report. THIS IS A TERMINAL ACTION. Only call after confirming entity is on sanctions list!\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"entity_id\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The sanctioned entity to report\"\n",
    "                    },\n",
    "                    \"reason\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Detailed justification for the SAR submission\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"entity_id\", \"reason\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Investigation system prompt for Qwen3\n",
    "INVESTIGATION_SYSTEM_PROMPT = \"\"\"You are an expert AML (Anti-Money Laundering) investigator. Your task is to investigate financial transaction networks to identify money laundering patterns.\n",
    "\n",
    "## INVESTIGATION STRATEGY\n",
    "1. **START**: Get account summary of the seed account to understand its risk profile\n",
    "2. **EXPLORE**: Get recent transactions to find money flows and counterparties  \n",
    "3. **FOLLOW**: Investigate high-amount and high-risk counterparties\n",
    "4. **VERIFY**: Check sanctions list for suspicious entities BEFORE reporting\n",
    "5. **REPORT**: Submit SAR ONLY after confirming sanctioned status\n",
    "\n",
    "## IMPORTANT RULES\n",
    "- ALWAYS check sanctions list before submitting a SAR\n",
    "- Focus on accounts with high_risk_indicator or transitive_illicit flags\n",
    "- Follow the money trail by examining outgoing transactions\n",
    "- Submit SAR only when you have confirmed evidence of sanctions violations\n",
    "\n",
    "Think step-by-step about your investigation strategy before each action.\"\"\"\n",
    "\n",
    "# Tool mapping to environment functions\n",
    "TOOL_MAPPING = {\n",
    "    \"get_account_summary\": lambda args: env.get_account_summary(args.get(\"account_id\", \"\")),\n",
    "    \"get_recent_transactions\": lambda args: env.get_recent_transactions(args.get(\"account_id\", \"\"), args.get(\"direction\", \"outgoing\")),\n",
    "    \"check_sanctions_list\": lambda args: env.check_sanctions_list(args.get(\"entity_id\", \"\")),\n",
    "    \"submit_sar\": lambda args: env.submit_sar(args.get(\"entity_id\", \"\"), args.get(\"reason\", \"Suspicious activity\")),\n",
    "}\n",
    "\n",
    "VALID_TOOLS = set(TOOL_MAPPING.keys())\n",
    "\n",
    "# Tool name corrections for hallucination handling\n",
    "TOOL_NAME_CORRECTIONS = {\n",
    "    \"get_account\": \"get_account_summary\",\n",
    "    \"get_transactions\": \"get_recent_transactions\",\n",
    "    \"check_sanctions\": \"check_sanctions_list\",\n",
    "    \"submit_report\": \"submit_sar\",\n",
    "}\n",
    "\n",
    "\n",
    "def harden_tool_call(call: dict) -> dict:\n",
    "    \"\"\"Correct common tool name hallucinations.\"\"\"\n",
    "    if not call:\n",
    "        return call\n",
    "    name = call.get(\"name\", \"\").lower().strip()\n",
    "    return {\"name\": TOOL_NAME_CORRECTIONS.get(name, name), \"arguments\": call.get(\"arguments\", {})}\n",
    "\n",
    "\n",
    "def extract_tool_calls(text: str) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Extract Qwen3/Hermes-style tool calls from model output.\n",
    "    \n",
    "    Qwen3 outputs tool calls in <tool_call> tags with JSON content:\n",
    "    <tool_call>\n",
    "    {\"name\": \"function_name\", \"arguments\": {\"param\": \"value\"}}\n",
    "    </tool_call>\n",
    "    \"\"\"\n",
    "    tool_calls = []\n",
    "    \n",
    "    # Clean up response\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Pattern 1: Qwen3 native <tool_call> tags\n",
    "    tool_call_pattern = r'<tool_call>\\s*(\\{.*?\\})\\s*</tool_call>'\n",
    "    matches = re.findall(tool_call_pattern, text, re.DOTALL)\n",
    "    for match in matches:\n",
    "        try:\n",
    "            call_data = json.loads(match)\n",
    "            name = call_data.get(\"name\", \"\")\n",
    "            args = call_data.get(\"arguments\", {})\n",
    "            if isinstance(args, str):\n",
    "                args = json.loads(args)\n",
    "            if name.lower() in [t.lower() for t in VALID_TOOLS]:\n",
    "                tool_calls.append(harden_tool_call({\"name\": name, \"arguments\": args}))\n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "            pass\n",
    "    \n",
    "    if tool_calls:\n",
    "        return tool_calls\n",
    "    \n",
    "    # Pattern 2: Raw JSON object with name and arguments\n",
    "    json_pattern = r'\\{[^{}]*\"name\"\\s*:\\s*\"(\\w+)\"[^{}]*\"arguments\"\\s*:\\s*(\\{[^{}]*\\})[^{}]*\\}'\n",
    "    matches = re.findall(json_pattern, text, re.DOTALL)\n",
    "    for name, args_str in matches:\n",
    "        try:\n",
    "            args = json.loads(args_str)\n",
    "            if name.lower() in [t.lower() for t in VALID_TOOLS]:\n",
    "                tool_calls.append(harden_tool_call({\"name\": name, \"arguments\": args}))\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    \n",
    "    if tool_calls:\n",
    "        return tool_calls\n",
    "    \n",
    "    # Pattern 3: Try to find any JSON-like structure\n",
    "    try:\n",
    "        json_start = text.find('{')\n",
    "        if json_start >= 0:\n",
    "            json_end = text.rfind('}') + 1\n",
    "            if json_end > json_start:\n",
    "                json_str = text[json_start:json_end]\n",
    "                data = json.loads(json_str)\n",
    "                if isinstance(data, dict) and 'name' in data:\n",
    "                    name = data['name']\n",
    "                    args = data.get('arguments', data.get('args', {}))\n",
    "                    if isinstance(args, str):\n",
    "                        args = json.loads(args)\n",
    "                    if name.lower() in [t.lower() for t in VALID_TOOLS]:\n",
    "                        tool_calls.append(harden_tool_call({\"name\": name, \"arguments\": args}))\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        pass\n",
    "    \n",
    "    return tool_calls\n",
    "\n",
    "\n",
    "def extract_function_call(text: str) -> Optional[dict]:\n",
    "    \"\"\"Extract single function call (for backwards compatibility).\"\"\"\n",
    "    calls = extract_tool_calls(text)\n",
    "    return calls[0] if calls else None\n",
    "\n",
    "\n",
    "def format_tool_response(tool_name: str, result: Any) -> str:\n",
    "    \"\"\"Format tool result in Qwen3/Hermes response format.\"\"\"\n",
    "    return f\"<tool_response>\\n{json.dumps(result, default=str)}\\n</tool_response>\"\n",
    "\n",
    "\n",
    "def execute_tool_call(call: dict) -> Tuple[str, Any]:\n",
    "    \"\"\"Execute a tool call and return (tool_name, result).\"\"\"\n",
    "    if not call:\n",
    "        return (\"error\", {\"error\": \"No valid tool call\"})\n",
    "    tool_name = call.get(\"name\", \"\")\n",
    "    if tool_name not in TOOL_MAPPING:\n",
    "        return (\"error\", {\"error\": f\"Unknown tool: {tool_name}\"})\n",
    "    try:\n",
    "        return (tool_name, TOOL_MAPPING[tool_name](call.get(\"arguments\", {})))\n",
    "    except Exception as e:\n",
    "        return (\"error\", {\"error\": str(e)})\n",
    "\n",
    "\n",
    "print(\"‚úì Qwen3 tool calling format configured (Hermes style)\")\n",
    "print(f\"  Available tools: {list(VALID_TOOLS)}\")\n",
    "print(f\"  Tools defined in OpenAI JSON Schema format for apply_chat_template\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Base Model with Unsloth\n",
    "\n",
    "Load **Qwen3-30B-A3B-Instruct-2507** with Unsloth FastModel for optimized 4-bit inference.\n",
    "\n",
    "**Important**: MOE (Mixture of Experts) models like Qwen3-30B-A3B require `FastModel` instead of `FastLanguageModel`. The model is imported at the top of the notebook to ensure proper optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LOAD MODEL WITH UNSLOTH - Qwen3 MOE with FastModel\n",
    "# ============================================================================\n",
    "# NOTE: For MOE models like Qwen3-30B-A3B, use FastModel (not FastLanguageModel)\n",
    "#       FastModel is imported at the top of the notebook (before transformers)\n",
    "#       to ensure all Unsloth optimizations are applied correctly.\n",
    "#\n",
    "# See: https://unsloth.ai/docs/models/qwen3-how-to-run-and-fine-tune/qwen3-2507\n",
    "\n",
    "print(\"üì• Loading Qwen3 MOE model with Unsloth FastModel...\")\n",
    "print(f\"   Model: {MODEL_NAME}\")\n",
    "print(f\"   Mode:  {'Thinking' if IS_THINKING_MODEL else 'Instruct'}\")\n",
    "\n",
    "# Load model using FastModel (required for MOE architectures)\n",
    "# Settings from Unsloth docs for Qwen3-2507\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=8192,          # Supports up to 256K context, use 8K for memory efficiency\n",
    "    load_in_4bit=True,            # 4-bit quantization to reduce memory\n",
    "    load_in_8bit=False,           # Alternative: slightly more accurate but 2x memory\n",
    "    full_finetuning=False,        # Use LoRA for efficient fine-tuning\n",
    "    device_map=\"cuda:0\",          # Explicit GPU mapping to prevent CPU offload\n",
    "    # token = \"hf_...\",           # Use if accessing gated models\n",
    ")\n",
    "\n",
    "# Enable faster inference mode\n",
    "FastModel.for_inference(model)\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"ü§ñ MODEL LOADED (Qwen3-2507 MOE)\")\n",
    "print(f\"{'=' * 60}\")\n",
    "print(f\"  Model:            {MODEL_NAME}\")\n",
    "print(f\"  Architecture:     Mixture of Experts (MOE)\")\n",
    "print(f\"  Mode:             {'Thinking' if IS_THINKING_MODEL else 'Instruct'}\")\n",
    "print(f\"  Max Seq Length:   8,192\")\n",
    "print(f\"  Quantization:     4-bit\")\n",
    "print(f\"  Device:           {next(model.parameters()).device}\")\n",
    "print(f\"  Temperature:      {GENERATION_TEMPERATURE}\")\n",
    "print(f\"  Top-P:            {GENERATION_TOP_P}\")\n",
    "print(f\"  Top-K:            {GENERATION_TOP_K}\")\n",
    "print(f\"{'=' * 60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Agent State and Execution Logic\n",
    "\n",
    "Define the `InvestigationState`, `InvestigationEpisode`, and agent execution functions with reward calculation.\n",
    "\n",
    "**Key Changes for Qwen3:**\n",
    "- Uses `apply_chat_template` with `tools` parameter for proper Hermes-style formatting\n",
    "- Extracts tool calls from `<tool_call>` XML tags\n",
    "- Uses Qwen3-recommended generation parameters (temp=0.7, top_p=0.8, top_k=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# AGENT STATE AND EXECUTION LOGIC (Qwen3 Hermes-style Tool Calling)\n",
    "# ============================================================================\n",
    "\n",
    "class InvestigationState(TypedDict):\n",
    "    \"\"\"Complete state for the investigation agent.\"\"\"\n",
    "    start_account: str\n",
    "    accounts_analyzed: Dict[str, dict]\n",
    "    entities_checked: Dict[str, bool]\n",
    "    risk_indicators: List[str]\n",
    "    investigation_path: List[str]\n",
    "    accounts_on_laundering_trail: List[str]  # Accounts reachable via is_laundering=1\n",
    "    high_risk_counterparties: List[str]      # Accounts with high_risk_indicator=True\n",
    "    total_amount_traced: float\n",
    "    current_strategy: str\n",
    "    messages: List[dict]\n",
    "    steps: List[dict]\n",
    "    step_count: int\n",
    "    terminated: bool\n",
    "    success: bool\n",
    "    final_result: dict\n",
    "\n",
    "\n",
    "def create_initial_state(start_account: str) -> InvestigationState:\n",
    "    return InvestigationState(\n",
    "        start_account=start_account, accounts_analyzed={}, entities_checked={},\n",
    "        risk_indicators=[], investigation_path=[start_account], \n",
    "        accounts_on_laundering_trail=[start_account],  # Seed is always on the trail\n",
    "        high_risk_counterparties=[],\n",
    "        total_amount_traced=0.0,\n",
    "        current_strategy=\"explore\", messages=[], steps=[], step_count=0,\n",
    "        terminated=False, success=False, final_result={},\n",
    "    )\n",
    "\n",
    "\n",
    "def build_messages_for_qwen3(state: InvestigationState) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Build messages list for Qwen3's apply_chat_template with tools parameter.\n",
    "    \n",
    "    Uses the Hermes-style format where:\n",
    "    - System message contains investigation instructions\n",
    "    - User message starts the investigation\n",
    "    - Tool responses are formatted as user messages with <tool_response> tags\n",
    "    \"\"\"\n",
    "    messages = []\n",
    "    start_account = state[\"start_account\"]\n",
    "    \n",
    "    # Build status context\n",
    "    status_lines = [\n",
    "        f\"INVESTIGATION TARGET: {start_account}\",\n",
    "        f\"Accounts analyzed: {len(state['accounts_analyzed'])}\",\n",
    "        f\"Path: {' ‚Üí '.join(state['investigation_path'][-5:]) if state['investigation_path'] else '(none)'}\",\n",
    "        f\"Amount traced: ${state['total_amount_traced']:,.2f}\",\n",
    "    ]\n",
    "    \n",
    "    # Add laundering trail info\n",
    "    trail = state.get(\"accounts_on_laundering_trail\", [])\n",
    "    if len(trail) > 1:\n",
    "        status_lines.append(f\"Laundering trail: {' ‚Üí '.join(trail[-5:])}\")\n",
    "    \n",
    "    # Add sanctions check results\n",
    "    sanctioned = [e for e, s in state['entities_checked'].items() if s]\n",
    "    if sanctioned:\n",
    "        sanctioned_on_trail = [e for e in sanctioned if e in trail]\n",
    "        if sanctioned_on_trail:\n",
    "            status_lines.append(f\"‚ö†Ô∏è SANCTIONED ON TRAIL: {', '.join(sanctioned_on_trail)}\")\n",
    "    \n",
    "    if state['risk_indicators']:\n",
    "        status_lines.append(f\"Risk indicators: {'; '.join(state['risk_indicators'][-3:])}\")\n",
    "    \n",
    "    status_text = \"\\n\".join(status_lines)\n",
    "    \n",
    "    # System message with investigation instructions\n",
    "    system_content = f\"{INVESTIGATION_SYSTEM_PROMPT}\\n\\n## CURRENT STATUS\\n{status_text}\"\n",
    "    messages.append({\"role\": \"system\", \"content\": system_content})\n",
    "    \n",
    "    # Initial user message\n",
    "    if not state['messages']:\n",
    "        messages.append({\n",
    "            \"role\": \"user\", \n",
    "            \"content\": f\"Begin investigation of account {start_account}. Find sanctioned entities on the laundering trail and submit a SAR.\"\n",
    "        })\n",
    "    else:\n",
    "        # Add conversation history\n",
    "        for msg in state['messages'][-MAX_HISTORY_TURNS*2:]:\n",
    "            if msg['role'] == 'assistant':\n",
    "                messages.append({\"role\": \"assistant\", \"content\": msg['content']})\n",
    "            elif msg['role'] == 'tool':\n",
    "                # Tool responses as user messages (Hermes style)\n",
    "                messages.append({\"role\": \"user\", \"content\": msg['content']})\n",
    "    \n",
    "    return messages\n",
    "\n",
    "\n",
    "def update_state_from_result(state: InvestigationState, tool_name: str, args: dict, result: Any) -> InvestigationState:\n",
    "    \"\"\"Update state based on tool execution result, tracking laundering trail.\"\"\"\n",
    "    new_state = dict(state)\n",
    "    new_state['accounts_analyzed'] = dict(state['accounts_analyzed'])\n",
    "    new_state['entities_checked'] = dict(state['entities_checked'])\n",
    "    new_state['risk_indicators'] = list(state['risk_indicators'])\n",
    "    new_state['investigation_path'] = list(state['investigation_path'])\n",
    "    new_state['accounts_on_laundering_trail'] = list(state.get('accounts_on_laundering_trail', [state['start_account']]))\n",
    "    new_state['high_risk_counterparties'] = list(state.get('high_risk_counterparties', []))\n",
    "    \n",
    "    if tool_name == \"get_account_summary\":\n",
    "        acc_id = args.get(\"account_id\", \"\")\n",
    "        if isinstance(result, dict) and \"error\" not in result:\n",
    "            new_state['accounts_analyzed'][acc_id] = result\n",
    "            if acc_id not in new_state['investigation_path']:\n",
    "                new_state['investigation_path'].append(acc_id)\n",
    "            if result.get(\"risk_score\", 0) > 0.7:\n",
    "                new_state['risk_indicators'].append(f\"HIGH_RISK:{acc_id}\")\n",
    "            if result.get(\"is_sanctioned\"):\n",
    "                new_state['risk_indicators'].append(f\"SANCTIONED:{acc_id}\")\n",
    "            if result.get(\"transitive_illicit\"):\n",
    "                new_state['risk_indicators'].append(f\"ILLICIT_PATH:{acc_id}\")\n",
    "    \n",
    "    elif tool_name == \"get_recent_transactions\":\n",
    "        source_account = args.get(\"account_id\", \"\")\n",
    "        if isinstance(result, list):\n",
    "            for txn in result:\n",
    "                new_state['total_amount_traced'] += txn.get(\"amount\", 0)\n",
    "                counterparty = txn.get(\"counterparty\")\n",
    "                \n",
    "                if counterparty:\n",
    "                    if counterparty not in new_state['investigation_path']:\n",
    "                        new_state['investigation_path'].append(counterparty)\n",
    "                    \n",
    "                    # Track laundering trail: if source is on trail AND txn is laundering\n",
    "                    if txn.get(\"is_laundering\") == 1:\n",
    "                        if source_account in new_state['accounts_on_laundering_trail'] or source_account == state['start_account']:\n",
    "                            if counterparty not in new_state['accounts_on_laundering_trail']:\n",
    "                                new_state['accounts_on_laundering_trail'].append(counterparty)\n",
    "                    \n",
    "                    # Track high-risk counterparties\n",
    "                    if txn.get(\"high_risk_indicator\"):\n",
    "                        new_state['risk_indicators'].append(f\"ILLICIT_TXN:{counterparty}\")\n",
    "                        if counterparty not in new_state['high_risk_counterparties']:\n",
    "                            new_state['high_risk_counterparties'].append(counterparty)\n",
    "    \n",
    "    elif tool_name == \"check_sanctions_list\":\n",
    "        entity_id = args.get(\"entity_id\", \"\")\n",
    "        is_sanctioned = result.get(\"on_sanctions_list\", False) if isinstance(result, dict) else False\n",
    "        new_state['entities_checked'][entity_id] = is_sanctioned\n",
    "        if is_sanctioned:\n",
    "            new_state['risk_indicators'].append(f\"SANCTIONED:{entity_id}\")\n",
    "    \n",
    "    return new_state\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class InvestigationEpisode:\n",
    "    \"\"\"Records an investigation episode for evaluation and training.\"\"\"\n",
    "    start_account: str\n",
    "    steps: List[dict] = field(default_factory=list)\n",
    "    terminated: bool = False\n",
    "    success: bool = False\n",
    "    final_result: dict = field(default_factory=dict)\n",
    "    total_reward: float = 0.0\n",
    "    \n",
    "    def add_step(self, tool_name: str, args: dict, result: Any, reward: float = 0.0):\n",
    "        self.steps.append({\"step\": len(self.steps) + 1, \"tool_name\": tool_name, \"arguments\": args, \"result\": result, \"reward\": reward})\n",
    "        self.total_reward += reward\n",
    "\n",
    "\n",
    "def calculate_reward(tool_name: str, args: dict, result: Any, state: InvestigationState) -> float:\n",
    "    \"\"\"\n",
    "    Calculate step-wise reward for agent actions.\n",
    "    \n",
    "    Rewards are designed to shape learning and track investigation quality:\n",
    "    - Following laundering path: HIGH reward (core skill to learn)\n",
    "    - Strategic sanctions checks: MEDIUM reward\n",
    "    - Correct SAR: MAJOR reward (goal)\n",
    "    - Wrong SAR: MAJOR penalty (avoid false positives)\n",
    "    \"\"\"\n",
    "    reward = -0.05  # Small step penalty (allow exploration)\n",
    "    \n",
    "    # Get state context\n",
    "    laundering_trail = state.get(\"accounts_on_laundering_trail\", [])\n",
    "    \n",
    "    if tool_name == \"get_account_summary\":\n",
    "        if isinstance(result, dict):\n",
    "            if result.get(\"transitive_illicit\"):\n",
    "                reward += 0.3  # Found account on laundering network\n",
    "            if result.get(\"is_sanctioned\"):\n",
    "                reward += 0.4  # Found potentially sanctioned\n",
    "    \n",
    "    elif tool_name == \"get_recent_transactions\":\n",
    "        if isinstance(result, list):\n",
    "            laundering_count = sum(1 for txn in result if txn.get(\"is_laundering\") == 1)\n",
    "            high_risk_count = sum(1 for txn in result if txn.get(\"high_risk_indicator\"))\n",
    "            \n",
    "            # KEY REWARD: Finding laundering transactions extends the trail\n",
    "            reward += 0.4 * laundering_count\n",
    "            reward += 0.1 * high_risk_count\n",
    "    \n",
    "    elif tool_name == \"check_sanctions_list\":\n",
    "        entity_id = args.get(\"entity_id\", \"\")\n",
    "        if isinstance(result, dict) and result.get(\"on_sanctions_list\"):\n",
    "            # Check if entity is on the laundering trail\n",
    "            if entity_id in laundering_trail:\n",
    "                reward += 1.5  # MAJOR: Sanctioned AND on trail = SAR candidate!\n",
    "            else:\n",
    "                reward += 0.3  # Found sanctioned but not yet on trail\n",
    "        elif entity_id in laundering_trail:\n",
    "            reward += 0.1  # Good strategic check even if not sanctioned\n",
    "    \n",
    "    elif tool_name == \"submit_sar\":\n",
    "        if isinstance(result, dict):\n",
    "            if result.get(\"correct_identification\"):\n",
    "                reward += 5.0  # MAJOR SUCCESS - correct SAR\n",
    "            else:\n",
    "                reward -= 3.0  # MAJOR PENALTY - wrong SAR\n",
    "    \n",
    "    return reward\n",
    "\n",
    "\n",
    "print(\"‚úì Agent state and execution logic defined (Qwen3 Hermes-style)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# AGENT EXECUTION - Run Investigation Episode with Qwen3 Tool Calling\n",
    "# ============================================================================\n",
    "\n",
    "def run_investigation(\n",
    "    start_account: str, \n",
    "    model, \n",
    "    tokenizer, \n",
    "    max_steps: int = MAX_STEPS, \n",
    "    verbose: bool = False,\n",
    "    show_memory: bool = True,\n",
    "    show_raw_response: bool = False\n",
    ") -> InvestigationEpisode:\n",
    "    \"\"\"\n",
    "    Run a complete investigation episode using the Qwen3 agent with Hermes-style tool calling.\n",
    "    \n",
    "    Args:\n",
    "        start_account: The seed account to investigate\n",
    "        model: The Qwen3 model\n",
    "        tokenizer: The tokenizer\n",
    "        max_steps: Maximum investigation steps\n",
    "        verbose: Show step-by-step execution details\n",
    "        show_memory: Display memory context (accounts analyzed, risk indicators)\n",
    "        show_raw_response: Display raw model output (for debugging)\n",
    "    \n",
    "    Returns:\n",
    "        InvestigationEpisode with full investigation record\n",
    "    \"\"\"\n",
    "    env.reset_investigation(start_account)\n",
    "    state = create_initial_state(start_account)\n",
    "    episode = InvestigationEpisode(start_account=start_account)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'‚ïê' * 70}\")\n",
    "        print(f\"üîç INVESTIGATION START\")\n",
    "        print(f\"{'‚ïê' * 70}\")\n",
    "        print(f\"  Target Account: {start_account}\")\n",
    "        print(f\"  Max Steps:      {max_steps}\")\n",
    "        print(f\"  Model:          {MODEL_NAME}\")\n",
    "        print(f\"  Temperature:    {GENERATION_TEMPERATURE}\")\n",
    "        print(f\"{'‚îÄ' * 70}\")\n",
    "    \n",
    "    for step_num in range(max_steps):\n",
    "        # Build messages for Qwen3 apply_chat_template\n",
    "        messages = build_messages_for_qwen3(state)\n",
    "        \n",
    "        # Apply chat template WITH TOOLS for native Hermes-style function calling\n",
    "        try:\n",
    "            prompt = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tools=QWEN3_TOOLS,  # Pass tools for native function calling!\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            # Fallback: some tokenizers don't support tools parameter\n",
    "            if verbose:\n",
    "                print(f\"‚îÇ ‚ö†Ô∏è apply_chat_template with tools failed: {e}\")\n",
    "            prompt = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "            )\n",
    "        \n",
    "        prompt_tokens = len(tokenizer.encode(prompt))\n",
    "        \n",
    "        if verbose and show_memory:\n",
    "            trail = state.get('accounts_on_laundering_trail', [])\n",
    "            print(f\"\\n‚îå‚îÄ STEP {step_num + 1} {'‚îÄ' * 55}‚îê\")\n",
    "            print(f\"‚îÇ üìä MEMORY CONTEXT:\")\n",
    "            print(f\"‚îÇ   Accounts Analyzed: {len(state['accounts_analyzed'])}\")\n",
    "            print(f\"‚îÇ   Path: {' ‚Üí '.join(state['investigation_path'][-4:]) if state['investigation_path'] else '(empty)'}\")\n",
    "            print(f\"‚îÇ   Laundering Trail: {' ‚Üí '.join(trail[-4:]) if trail else '(empty)'}\")\n",
    "            print(f\"‚îÇ   Amount Traced: ${state['total_amount_traced']:,.2f}\")\n",
    "            print(f\"‚îÇ   Entities Checked: {len(state['entities_checked'])} (sanctioned: {sum(state['entities_checked'].values())})\")\n",
    "            print(f\"‚îÇ   Prompt Tokens: {prompt_tokens:,}\")\n",
    "        \n",
    "        # Tokenize and generate\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=6000).to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=256,\n",
    "                temperature=GENERATION_TEMPERATURE,\n",
    "                do_sample=True,\n",
    "                top_p=GENERATION_TOP_P,\n",
    "                top_k=GENERATION_TOP_K,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                repetition_penalty=1.05,  # Reduce repetition\n",
    "            )\n",
    "        \n",
    "        # Decode response\n",
    "        response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "        \n",
    "        if verbose and show_raw_response:\n",
    "            print(f\"‚îÇ üî§ RAW RESPONSE:\")\n",
    "            for line in response[:500].split('\\n'):\n",
    "                print(f\"‚îÇ   {line[:65]}\")\n",
    "            if len(response) > 500:\n",
    "                print(f\"‚îÇ   ... (truncated, {len(response)} chars total)\")\n",
    "        \n",
    "        # Extract tool call from response\n",
    "        tool_call = extract_function_call(response)\n",
    "        used_fallback = False\n",
    "        \n",
    "        if not tool_call:\n",
    "            used_fallback = True\n",
    "            # Smart fallback strategy based on state\n",
    "            \n",
    "            trail = state.get('accounts_on_laundering_trail', [])\n",
    "            \n",
    "            # Check if we found a sanctioned entity on the laundering trail\n",
    "            sanctioned_on_trail = [\n",
    "                entity for entity, is_sanct in state['entities_checked'].items() \n",
    "                if is_sanct and entity in trail\n",
    "            ]\n",
    "            if sanctioned_on_trail:\n",
    "                tool_call = {\"name\": \"submit_sar\", \"arguments\": {\n",
    "                    \"entity_id\": sanctioned_on_trail[0], \n",
    "                    \"reason\": f\"Sanctioned entity on laundering trail from {start_account}\"\n",
    "                }}\n",
    "            # Step 0: Start with account summary\n",
    "            elif state['step_count'] == 0 or not state['accounts_analyzed']:\n",
    "                tool_call = {\"name\": \"get_account_summary\", \"arguments\": {\"account_id\": start_account}}\n",
    "            # Step 1: Get transactions to find connected accounts\n",
    "            elif len(state['accounts_analyzed']) == 1:\n",
    "                tool_call = {\"name\": \"get_recent_transactions\", \"arguments\": {\"account_id\": start_account}}\n",
    "            else:\n",
    "                # Priority: Check sanctions on accounts on laundering trail\n",
    "                unchecked_on_trail = [a for a in trail if a not in state['entities_checked'] and a != start_account]\n",
    "                \n",
    "                if unchecked_on_trail:\n",
    "                    tool_call = {\"name\": \"check_sanctions_list\", \"arguments\": {\"entity_id\": unchecked_on_trail[0]}}\n",
    "                else:\n",
    "                    # Check high-risk counterparties\n",
    "                    high_risk = state.get('high_risk_counterparties', [])\n",
    "                    unchecked_high_risk = [a for a in high_risk if a not in state['entities_checked']]\n",
    "                    \n",
    "                    if unchecked_high_risk:\n",
    "                        tool_call = {\"name\": \"check_sanctions_list\", \"arguments\": {\"entity_id\": unchecked_high_risk[0]}}\n",
    "                    else:\n",
    "                        # Explore next account on trail\n",
    "                        unexplored_trail = [a for a in trail if a not in state['accounts_analyzed']]\n",
    "                        if unexplored_trail:\n",
    "                            tool_call = {\"name\": \"get_recent_transactions\", \"arguments\": {\"account_id\": unexplored_trail[0]}}\n",
    "                        else:\n",
    "                            # Check any unchecked account\n",
    "                            all_unchecked = [a for a in state['investigation_path'] if a not in state['entities_checked']]\n",
    "                            if all_unchecked:\n",
    "                                tool_call = {\"name\": \"check_sanctions_list\", \"arguments\": {\"entity_id\": all_unchecked[0]}}\n",
    "                            else:\n",
    "                                if verbose:\n",
    "                                    print(f\"‚îÇ ‚ö†Ô∏è No valid action - terminating\")\n",
    "                                break\n",
    "        \n",
    "        # Execute tool call\n",
    "        tool_name, result = execute_tool_call(tool_call)\n",
    "        args = tool_call.get(\"arguments\", {})\n",
    "        \n",
    "        if verbose:\n",
    "            fallback_indicator = \" (FALLBACK)\" if used_fallback else \"\"\n",
    "            print(f\"‚îÇ üîß TOOL CALL{fallback_indicator}:\")\n",
    "            print(f\"‚îÇ   Function: {tool_name}\")\n",
    "            print(f\"‚îÇ   Arguments: {json.dumps(args, default=str)[:60]}\")\n",
    "            \n",
    "            # Show result summary based on tool type\n",
    "            if tool_name == \"get_account_summary\" and isinstance(result, dict):\n",
    "                illicit = \"üî¥ ILLICIT\" if result.get(\"transitive_illicit\") else \"\"\n",
    "                sanct = \"‚ö†Ô∏è SANCTIONED\" if result.get(\"is_sanctioned\") else \"\"\n",
    "                print(f\"‚îÇ   Result: {result.get('account_type', 'Unknown')} | Risk: {result.get('risk_score', 0):.2f} {illicit} {sanct}\")\n",
    "            elif tool_name == \"get_recent_transactions\" and isinstance(result, list):\n",
    "                laundering = sum(1 for t in result if t.get('is_laundering') == 1)\n",
    "                print(f\"‚îÇ   Result: {len(result)} transactions ({laundering} laundering)\")\n",
    "                for txn in result[:3]:\n",
    "                    risk = \"üî¥\" if txn.get(\"high_risk_indicator\") else \"\"\n",
    "                    laund = \"üí∞\" if txn.get(\"is_laundering\") == 1 else \"\"\n",
    "                    print(f\"‚îÇ     ‚Üí {txn.get('counterparty', '?')[:20]}: ${txn.get('amount', 0):,.2f} {risk}{laund}\")\n",
    "            elif tool_name == \"check_sanctions_list\" and isinstance(result, dict):\n",
    "                status = \"‚ö†Ô∏è ON SANCTIONS LIST\" if result.get(\"on_sanctions_list\") else \"‚úì Clear\"\n",
    "                print(f\"‚îÇ   Result: {status}\")\n",
    "            elif tool_name == \"submit_sar\" and isinstance(result, dict):\n",
    "                status = \"‚úÖ CORRECT\" if result.get(\"correct_identification\") else \"‚ùå INCORRECT\"\n",
    "                print(f\"‚îÇ   Result: {status}\")\n",
    "                print(f\"‚îÇ   Reason: {result.get('evaluation_reason', '')[:50]}\")\n",
    "            elif isinstance(result, dict) and \"error\" in result:\n",
    "                print(f\"‚îÇ   Result: ‚ö†Ô∏è ERROR - {result.get('error', '')[:50]}\")\n",
    "        \n",
    "        # Calculate reward\n",
    "        reward = calculate_reward(tool_name, args, result, state)\n",
    "        \n",
    "        if verbose:\n",
    "            reward_color = \"üü¢\" if reward > 0 else (\"üî¥\" if reward < 0 else \"‚ö™\")\n",
    "            print(f\"‚îÇ üí∞ REWARD: {reward_color} {reward:+.2f}\")\n",
    "            print(f\"‚îî{'‚îÄ' * 68}‚îò\")\n",
    "        \n",
    "        # Record step\n",
    "        episode.add_step(tool_name, args, result, reward)\n",
    "        \n",
    "        # Update state\n",
    "        state = update_state_from_result(state, tool_name, args, result)\n",
    "        state['step_count'] = step_num + 1\n",
    "        \n",
    "        # Format tool call and response for message history\n",
    "        tool_call_str = f\"<tool_call>\\n{json.dumps({'name': tool_name, 'arguments': args})}\\n</tool_call>\"\n",
    "        state['messages'] = list(state['messages']) + [\n",
    "            {\"role\": \"assistant\", \"content\": tool_call_str},\n",
    "            {\"role\": \"tool\", \"content\": format_tool_response(tool_name, result)},\n",
    "        ]\n",
    "        \n",
    "        # Check for terminal action (SAR submission)\n",
    "        if tool_name == \"submit_sar\":\n",
    "            episode.terminated = True\n",
    "            episode.success = result.get(\"correct_identification\", False)\n",
    "            episode.final_result = result\n",
    "            break\n",
    "    \n",
    "    if not episode.terminated:\n",
    "        episode.terminated = True\n",
    "        episode.success = False\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'‚ïê' * 70}\")\n",
    "        print(f\"üìã INVESTIGATION COMPLETE\")\n",
    "        print(f\"{'‚ïê' * 70}\")\n",
    "        print(f\"  Result: {'‚úÖ SUCCESS' if episode.success else '‚ùå FAILED'}\")\n",
    "        print(f\"  Steps: {len(episode.steps)}\")\n",
    "        print(f\"  Total Reward: {episode.total_reward:+.2f}\")\n",
    "        if episode.final_result:\n",
    "            print(f\"  SAR Reason: {episode.final_result.get('evaluation_reason', 'N/A')}\")\n",
    "        print(f\"{'‚ïê' * 70}\")\n",
    "    \n",
    "    return episode\n",
    "\n",
    "\n",
    "print(\"‚úì Agent execution function defined (Qwen3 Hermes-style tool calling)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. LLM-as-Judge Evaluation with Gemini\n",
    "\n",
    "Evaluate agent performance using Gemini as an LLM judge with integrated MLflow tracing.\n",
    "\n",
    "**Rubric:**\n",
    "1. **Strategy Quality** (0-10): Did the agent prioritize high-value/high-risk transfers?\n",
    "2. **Decision Persistence** (0-10): Did it pivot correctly after hitting dead ends?\n",
    "3. **Outcome** (0-10): Did the agent correctly identify a sanctioned entity?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LLM-AS-JUDGE EVALUATION - Gemini-based Scoring\n",
    "# Using google.genai library (google.generativeai is deprecated)\n",
    "# ============================================================================\n",
    "\n",
    "from google import genai\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\", \"\")\n",
    "if GEMINI_API_KEY:\n",
    "    gemini_client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "    print(\"‚úì Gemini API configured (google.genai)\")\n",
    "else:\n",
    "    gemini_client = None\n",
    "    print(\"‚ö†Ô∏è GEMINI_API_KEY not set - LLM-as-Judge will be disabled\")\n",
    "\n",
    "\n",
    "JUDGE_PROMPT = \"\"\"You are an expert evaluator of AML investigations.\n",
    "\n",
    "## Investigation Trace\n",
    "{trace}\n",
    "\n",
    "## Evaluation Rubric\n",
    "1. **Strategy Quality** (0-10): Did the agent prioritize high-value and high-risk transfers?\n",
    "2. **Decision Persistence** (0-10): Did the agent correctly pivot after hitting dead ends?\n",
    "3. **Outcome Quality** (0-10): Did the agent correctly identify a sanctioned entity?\n",
    "\n",
    "## Response Format\n",
    "Provide your evaluation as JSON:\n",
    "{{\"strategy_score\": <0-10>, \"persistence_score\": <0-10>, \"outcome_score\": <0-10>, \"overall_score\": <0-10>, \"reasoning\": \"<brief explanation>\"}}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@mlflow.trace(span_type=\"LLM_JUDGE\")\n",
    "def evaluate_episode_with_llm(episode: InvestigationEpisode) -> dict:\n",
    "    \"\"\"Evaluate an investigation episode using Gemini as LLM judge.\"\"\"\n",
    "    default_scores = {\n",
    "        \"strategy_score\": 0, \n",
    "        \"persistence_score\": 0, \n",
    "        \"outcome_score\": 10 if episode.success else 0,\n",
    "        \"overall_score\": 5 if episode.success else 0, \n",
    "        \"reasoning\": \"Default score\"\n",
    "    }\n",
    "    \n",
    "    if not gemini_client:\n",
    "        default_scores[\"reasoning\"] = \"LLM evaluation disabled (no API key)\"\n",
    "        return default_scores\n",
    "    \n",
    "    # Build investigation trace\n",
    "    trace_lines = [f\"Start Account: {episode.start_account}\", \"\"]\n",
    "    for step in episode.steps:\n",
    "        tool_name = step.get('tool_name', 'unknown')\n",
    "        trace_lines.append(f\"Step {step['step']}: {tool_name}\")\n",
    "        trace_lines.append(f\"  Args: {json.dumps(step.get('arguments', {}), default=str)[:80]}\")\n",
    "        \n",
    "        # More detailed result summary based on tool type\n",
    "        result = step.get('result', {})\n",
    "        if tool_name == \"get_account_summary\" and isinstance(result, dict):\n",
    "            trace_lines.append(f\"  Result: Type={result.get('account_type', 'Unknown')}, Risk={result.get('risk_score', 0):.2f}, Illicit={result.get('transitive_illicit', False)}\")\n",
    "        elif tool_name == \"get_recent_transactions\" and isinstance(result, list):\n",
    "            trace_lines.append(f\"  Result: {len(result)} transactions (high_risk: {sum(1 for t in result if t.get('high_risk_indicator'))})\")\n",
    "        elif tool_name == \"check_sanctions_list\" and isinstance(result, dict):\n",
    "            trace_lines.append(f\"  Result: Sanctioned={result.get('on_sanctions_list', False)}\")\n",
    "        elif tool_name == \"submit_sar\" and isinstance(result, dict):\n",
    "            trace_lines.append(f\"  Result: Correct={result.get('correct_identification', False)}, Reason={result.get('evaluation_reason', 'N/A')[:50]}\")\n",
    "        else:\n",
    "            trace_lines.append(f\"  Result: {json.dumps(result, default=str)[:100]}\")\n",
    "        trace_lines.append(f\"  Reward: {step.get('reward', 0):+.2f}\")\n",
    "    \n",
    "    trace_lines.append(f\"\\nFinal: {'SUCCESS' if episode.success else 'FAILED'} | Steps: {len(episode.steps)} | Total Reward: {episode.total_reward:+.2f}\")\n",
    "    \n",
    "    trace_text = \"\\n\".join(trace_lines)\n",
    "    \n",
    "    try:\n",
    "        response = gemini_client.models.generate_content(\n",
    "            model=GEMINI_MODEL, \n",
    "            contents=JUDGE_PROMPT.format(trace=trace_text)\n",
    "        )\n",
    "        \n",
    "        # Extract JSON from response\n",
    "        json_match = re.search(r'\\{[^{}]*\\}', response.text, re.DOTALL)\n",
    "        if json_match:\n",
    "            scores = json.loads(json_match.group())\n",
    "            # Ensure all required fields are present\n",
    "            for key in [\"strategy_score\", \"persistence_score\", \"outcome_score\", \"overall_score\"]:\n",
    "                if key not in scores:\n",
    "                    scores[key] = 0\n",
    "            if \"reasoning\" not in scores:\n",
    "                scores[\"reasoning\"] = \"No reasoning provided\"\n",
    "            return scores\n",
    "        else:\n",
    "            default_scores[\"reasoning\"] = f\"Failed to parse JSON from: {response.text[:100]}\"\n",
    "            return default_scores\n",
    "            \n",
    "    except Exception as e:\n",
    "        default_scores[\"reasoning\"] = f\"API error: {str(e)[:50]}\"\n",
    "        return default_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EVALUATION FUNCTION - Run Multiple Episodes with Detailed Logging\n",
    "# ============================================================================\n",
    "\n",
    "def run_evaluation(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    stage_name: str, \n",
    "    n_episodes: int = EVAL_EPISODES, \n",
    "    use_llm_judge: bool = True, \n",
    "    verbose: bool = True,\n",
    "    show_episode_details: bool = True,  # Show step-by-step for each episode\n",
    "    show_memory: bool = True,\n",
    "    show_raw_response: bool = False     # Show raw model output for debugging\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Run evaluation episodes and collect metrics.\n",
    "    \n",
    "    Args:\n",
    "        model: The Qwen3 model to evaluate\n",
    "        tokenizer: The tokenizer\n",
    "        stage_name: Name for this evaluation stage (e.g., \"Baseline\", \"Post-SFT\")\n",
    "        n_episodes: Number of episodes to run\n",
    "        use_llm_judge: Whether to use Gemini LLM-as-Judge\n",
    "        verbose: Show progress and summaries\n",
    "        show_episode_details: Show step-by-step details for each episode\n",
    "        show_memory: Show memory context at each step\n",
    "        show_raw_response: Show raw model output (for debugging)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (DataFrame with evaluation results, List of InvestigationEpisode objects)\n",
    "    \"\"\"\n",
    "    # Use valid_patterns (with reachable sanctioned entities) if available, else fall back to all patterns\n",
    "    # This ensures we only evaluate on patterns where success is actually achievable\n",
    "    patterns_to_use = globals().get('valid_patterns') or laundering_patterns\n",
    "    eval_seeds = [p.seed_account for p in random.sample(patterns_to_use, min(n_episodes, len(patterns_to_use))) if p.seed_account]\n",
    "    \n",
    "    results = []\n",
    "    episodes_list = []  # Store episodes for training needs analysis\n",
    "    \n",
    "    print(f\"\\n{'‚ïê' * 70}\")\n",
    "    print(f\"üß™ EVALUATION: {stage_name.upper()}\")\n",
    "    print(f\"{'‚ïê' * 70}\")\n",
    "    print(f\"  Episodes: {n_episodes}\")\n",
    "    print(f\"  Max Steps: {MAX_STEPS}\")\n",
    "    print(f\"  Model: {MODEL_NAME}\")\n",
    "    print(f\"  LLM Judge: {'Enabled' if use_llm_judge and gemini_client else 'Disabled'}\")\n",
    "    using_valid = globals().get('valid_patterns') is not None and patterns_to_use is globals().get('valid_patterns')\n",
    "    print(f\"  Pattern Source: {'valid_patterns (verified reachable)' if using_valid else 'all patterns'} ({len(patterns_to_use)} available)\")\n",
    "    print(f\"{'‚ïê' * 70}\")\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"eval_{stage_name}\"):\n",
    "        for i, seed in enumerate(eval_seeds[:n_episodes]):\n",
    "            print(f\"\\n{'‚ñì' * 70}\")\n",
    "            print(f\"  EPISODE {i+1}/{n_episodes}\")\n",
    "            print(f\"  Seed: {seed}\")\n",
    "            print(f\"{'‚ñì' * 70}\")\n",
    "            \n",
    "            # Run investigation with verbose output if requested\n",
    "            episode = run_investigation(\n",
    "                seed, model, tokenizer, \n",
    "                verbose=show_episode_details,\n",
    "                show_memory=show_memory,\n",
    "                show_raw_response=show_raw_response\n",
    "            )\n",
    "            episodes_list.append(episode)  # Store for training needs analysis\n",
    "            \n",
    "            # LLM Judge evaluation\n",
    "            if use_llm_judge and gemini_client:\n",
    "                print(f\"\\n  ü§ñ LLM-AS-JUDGE EVALUATION...\")\n",
    "                llm_scores = evaluate_episode_with_llm(episode)\n",
    "                print(f\"     Strategy:    {llm_scores.get('strategy_score', 0)}/10\")\n",
    "                print(f\"     Persistence: {llm_scores.get('persistence_score', 0)}/10\")\n",
    "                print(f\"     Outcome:     {llm_scores.get('outcome_score', 0)}/10\")\n",
    "                print(f\"     Overall:     {llm_scores.get('overall_score', 0)}/10\")\n",
    "                if llm_scores.get('reasoning'):\n",
    "                    print(f\"     Reasoning:   {llm_scores.get('reasoning', '')[:60]}...\")\n",
    "            else:\n",
    "                llm_scores = {\n",
    "                    \"strategy_score\": 0, \n",
    "                    \"persistence_score\": 0, \n",
    "                    \"outcome_score\": 10 if episode.success else 0, \n",
    "                    \"overall_score\": 5 if episode.success else 0,\n",
    "                    \"reasoning\": \"LLM judge disabled\"\n",
    "                }\n",
    "            \n",
    "            result = {\n",
    "                \"seed_account\": seed, \n",
    "                \"success\": episode.success, \n",
    "                \"steps\": len(episode.steps),\n",
    "                \"total_reward\": episode.total_reward, \n",
    "                **llm_scores\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            # Episode summary\n",
    "            print(f\"\\n  ‚îå‚îÄ EPISODE {i+1} RESULT {'‚îÄ' * 44}‚îê\")\n",
    "            print(f\"  ‚îÇ Outcome:      {'‚úÖ SUCCESS' if episode.success else '‚ùå FAILED':<20} ‚îÇ\")\n",
    "            print(f\"  ‚îÇ Steps:        {len(episode.steps):<20} ‚îÇ\")\n",
    "            print(f\"  ‚îÇ Total Reward: {episode.total_reward:+.2f}{' ' * 17} ‚îÇ\")\n",
    "            print(f\"  ‚îÇ LLM Score:    {llm_scores.get('overall_score', 0)}/10{' ' * 16} ‚îÇ\")\n",
    "            print(f\"  ‚îî{'‚îÄ' * 53}‚îò\")\n",
    "        \n",
    "        df = pd.DataFrame(results)\n",
    "        \n",
    "        # Log to MLflow\n",
    "        mlflow.log_metrics({\n",
    "            f\"{stage_name}_success_rate\": df['success'].mean(),\n",
    "            f\"{stage_name}_avg_steps\": df['steps'].mean(),\n",
    "            f\"{stage_name}_avg_reward\": df['total_reward'].mean(),\n",
    "            f\"{stage_name}_avg_score\": df['overall_score'].mean(),\n",
    "        })\n",
    "    \n",
    "    # Final Summary\n",
    "    print(f\"\\n{'‚ïê' * 70}\")\n",
    "    print(f\"üìä {stage_name.upper()} - FINAL SUMMARY\")\n",
    "    print(f\"{'‚ïê' * 70}\")\n",
    "    print(f\"  Episodes Run:     {len(results)}\")\n",
    "    print(f\"  Success Rate:     {df['success'].mean()*100:.1f}% ({df['success'].sum()}/{len(results)})\")\n",
    "    print(f\"  Avg Steps:        {df['steps'].mean():.1f}\")\n",
    "    print(f\"  Avg Reward:       {df['total_reward'].mean():+.2f}\")\n",
    "    print(f\"  Avg LLM Score:    {df['overall_score'].mean():.1f}/10\")\n",
    "    print(f\"{'‚îÄ' * 70}\")\n",
    "    print(f\"  Score Breakdown:\")\n",
    "    print(f\"    Strategy:       {df['strategy_score'].mean():.1f}/10\")\n",
    "    print(f\"    Persistence:    {df['persistence_score'].mean():.1f}/10\")\n",
    "    print(f\"    Outcome:        {df['outcome_score'].mean():.1f}/10\")\n",
    "    print(f\"{'‚ïê' * 70}\")\n",
    "    \n",
    "    return df, episodes_list\n",
    "\n",
    "\n",
    "print(\"‚úì Evaluation framework configured (Qwen3 tool calling)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. STAGE 1: Baseline Evaluation (Pre-Training)\n",
    "\n",
    "Run evaluation on the **base Qwen3-30B-A3B-Instruct model** before any fine-tuning to establish baseline performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CLEANUP - Remove Previous Training Outputs (for notebook re-runs)\n",
    "# ============================================================================\n",
    "\n",
    "import shutil\n",
    "\n",
    "def cleanup_training_outputs():\n",
    "    \"\"\"Remove previous training outputs to ensure clean re-runs.\"\"\"\n",
    "    cleanup_dirs = [\n",
    "        MODELS_DIR / \"sft_output\",\n",
    "        MODELS_DIR / \"sft_adapter\", \n",
    "        MODELS_DIR / \"grpo_output\",\n",
    "        MODELS_DIR / \"grpo_adapter\",\n",
    "        MODELS_DIR / \"aml_agent_final\",\n",
    "    ]\n",
    "    \n",
    "    print(\"üßπ Cleaning up previous training outputs...\")\n",
    "    \n",
    "    for dir_path in cleanup_dirs:\n",
    "        if dir_path.exists():\n",
    "            try:\n",
    "                shutil.rmtree(dir_path)\n",
    "                print(f\"   ‚úì Removed: {dir_path.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö† Could not remove {dir_path.name}: {e}\")\n",
    "        else:\n",
    "            print(f\"   - Not found: {dir_path.name} (skipping)\")\n",
    "    \n",
    "    print(\"‚úì Cleanup complete\\n\")\n",
    "\n",
    "# Run cleanup\n",
    "cleanup_training_outputs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STAGE 1: BASELINE EVALUATION - Pre-Training Performance\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üîç STAGE 1: BASELINE EVALUATION\")\n",
    "print(f\"   Testing base {MODEL_NAME} (no fine-tuning)\")\n",
    "\n",
    "# Initialize results dictionary for all stages\n",
    "all_results = {}\n",
    "\n",
    "# Ensure model is in inference mode\n",
    "FastModel.for_inference(model)\n",
    "\n",
    "# Configuration for evaluation verbosity\n",
    "# Set show_raw_response=True to see what the model actually generates\n",
    "SHOW_EPISODE_DETAILS = True  # Show step-by-step execution\n",
    "SHOW_MEMORY = True           # Show memory context\n",
    "SHOW_RAW_RESPONSE = False    # Show raw model output (useful for debugging)\n",
    "\n",
    "# Run baseline evaluation with detailed logging\n",
    "baseline_results, baseline_episodes = run_evaluation(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    stage_name=\"Baseline\",\n",
    "    n_episodes=EVAL_EPISODES,\n",
    "    use_llm_judge=bool(gemini_client),\n",
    "    verbose=True,\n",
    "    show_episode_details=SHOW_EPISODE_DETAILS,\n",
    "    show_memory=SHOW_MEMORY\n",
    ")\n",
    "\n",
    "# Store results for comparison\n",
    "all_results[\"Baseline\"] = baseline_results\n",
    "\n",
    "print(\"\\nüìä Baseline Results Summary:\")\n",
    "print(baseline_results[['success', 'steps', 'total_reward', 'overall_score']].describe())\n",
    "\n",
    "# Analyze training needs based on baseline performance\n",
    "training_assessment = analyze_training_needs(baseline_episodes)\n",
    "print_training_assessment(training_assessment)\n",
    "\n",
    "# Store assessment for conditional training decisions\n",
    "NEEDS_SFT = training_assessment.get('needs_sft', True)\n",
    "NEEDS_DPO = training_assessment.get('needs_dpo', True)\n",
    "TRAINING_RECOMMENDATION = training_assessment.get('recommendation', 'SFT_THEN_DPO')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. SFT Training Data Generation\n",
    "\n",
    "Generate Supervised Fine-Tuning samples from laundering patterns using Qwen3's Hermes-style `<tool_call>` format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SFT TRAINING DATA GENERATION - Qwen3 Hermes-style Tool Calling\n",
    "# ============================================================================\n",
    "\n",
    "def generate_sft_sample(pattern: LaunderingPattern) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Generate a multi-turn SFT training sample with Qwen3 <tool_call> format.\n",
    "    \n",
    "    TRAINABLE DESIGN:\n",
    "    - Shows explicit reasoning about laundering path tracking\n",
    "    - Demonstrates recognition of SAR conditions (sanctioned + on trail)\n",
    "    - Uses proper <tool_call> XML tags\n",
    "    \"\"\"\n",
    "    messages = []\n",
    "    seed_account = pattern.seed_account\n",
    "    terminal_account = pattern.terminal_account\n",
    "    \n",
    "    if not seed_account or not terminal_account:\n",
    "        return []\n",
    "    \n",
    "    # Get intermediate accounts from pattern\n",
    "    intermediate_accounts = list(pattern.accounts_involved - {seed_account, terminal_account})[:2]\n",
    "    \n",
    "    # System message with tools will be added by apply_chat_template\n",
    "    messages.append({\n",
    "        \"role\": \"system\", \n",
    "        \"content\": f\"{INVESTIGATION_SYSTEM_PROMPT}\\n\\nINVESTIGATION TARGET: {seed_account}\"\n",
    "    })\n",
    "    \n",
    "    # User starts investigation\n",
    "    messages.append({\n",
    "        \"role\": \"user\", \n",
    "        \"content\": f\"Investigate account {seed_account} for money laundering. Find sanctioned entities on the laundering trail.\"\n",
    "    })\n",
    "    \n",
    "    # Step 1: Get account summary\n",
    "    messages.append({\n",
    "        \"role\": \"assistant\", \n",
    "        \"content\": f\"\"\"I need to check the seed account's risk profile first.\n",
    "\n",
    "<tool_call>\n",
    "{{\"name\": \"get_account_summary\", \"arguments\": {{\"account_id\": \"{seed_account}\"}}}}\n",
    "</tool_call>\"\"\"\n",
    "    })\n",
    "    \n",
    "    messages.append({\n",
    "        \"role\": \"user\", \n",
    "        \"content\": f'<tool_response>\\n{json.dumps({\"account_id\": seed_account, \"account_type\": \"Corporate\", \"risk_score\": 0.75, \"transitive_illicit\": True, \"is_sanctioned\": False})}\\n</tool_response>'\n",
    "    })\n",
    "    \n",
    "    # Step 2: Get transactions - FOCUS ON is_laundering=1\n",
    "    messages.append({\n",
    "        \"role\": \"assistant\", \n",
    "        \"content\": f\"\"\"The account has transitive_illicit=True, meaning it's connected to laundering activity.\n",
    "Now I need to find the specific laundering transactions (is_laundering=1) to build the laundering trail.\n",
    "\n",
    "<tool_call>\n",
    "{{\"name\": \"get_recent_transactions\", \"arguments\": {{\"account_id\": \"{seed_account}\", \"direction\": \"outgoing\"}}}}\n",
    "</tool_call>\"\"\"\n",
    "    })\n",
    "    \n",
    "    # Show transactions with is_laundering flag\n",
    "    txn_results = [\n",
    "        {\"counterparty\": terminal_account, \"amount\": pattern.transactions[0]['amount'] if pattern.transactions else 5000, \"is_laundering\": 1, \"high_risk_indicator\": True},\n",
    "        {\"counterparty\": intermediate_accounts[0] if intermediate_accounts else \"999-ACC001\", \"amount\": 3000, \"is_laundering\": 1, \"high_risk_indicator\": True},\n",
    "        {\"counterparty\": \"999-LEGIT01\", \"amount\": 2000, \"is_laundering\": 0, \"high_risk_indicator\": False}\n",
    "    ]\n",
    "    messages.append({\n",
    "        \"role\": \"user\", \n",
    "        \"content\": f'<tool_response>\\n{json.dumps(txn_results)}\\n</tool_response>'\n",
    "    })\n",
    "    \n",
    "    # Step 3: REASONING about laundering trail before sanctions check\n",
    "    messages.append({\n",
    "        \"role\": \"assistant\", \n",
    "        \"content\": f\"\"\"Analyzing the transactions:\n",
    "- {terminal_account}: is_laundering=1, high_risk_indicator=True ‚Üí ON LAUNDERING TRAIL\n",
    "- {intermediate_accounts[0] if intermediate_accounts else \"999-ACC001\"}: is_laundering=1 ‚Üí ON LAUNDERING TRAIL  \n",
    "- 999-LEGIT01: is_laundering=0 ‚Üí NOT on laundering trail\n",
    "\n",
    "For a valid SAR, I need to find an entity that is BOTH:\n",
    "1. On the sanctions list\n",
    "2. On the laundering trail (reached via is_laundering=1)\n",
    "\n",
    "Let me check sanctions for {terminal_account} since it's on the laundering trail.\n",
    "\n",
    "<tool_call>\n",
    "{{\"name\": \"check_sanctions_list\", \"arguments\": {{\"entity_id\": \"{terminal_account}\"}}}}\n",
    "</tool_call>\"\"\"\n",
    "    })\n",
    "    \n",
    "    messages.append({\n",
    "        \"role\": \"user\", \n",
    "        \"content\": f'<tool_response>\\n{json.dumps({\"entity_id\": terminal_account, \"on_sanctions_list\": True, \"list_type\": \"OFAC SDN\"})}\\n</tool_response>'\n",
    "    })\n",
    "    \n",
    "    # Step 4: Submit SAR - EXPLICIT REASONING about both conditions\n",
    "    reason = f\"Entity {terminal_account} is sanctioned (OFAC) and on laundering trail from {seed_account}\"\n",
    "    messages.append({\n",
    "        \"role\": \"assistant\", \n",
    "        \"content\": f\"\"\"Checking SAR conditions for {terminal_account}:\n",
    "1. ‚úì On sanctions list: YES (OFAC SDN confirmed)\n",
    "2. ‚úì On laundering trail: YES (reached via is_laundering=1 from {seed_account})\n",
    "\n",
    "BOTH conditions are met. Submitting SAR now.\n",
    "\n",
    "<tool_call>\n",
    "{{\"name\": \"submit_sar\", \"arguments\": {{\"entity_id\": \"{terminal_account}\", \"reason\": \"{reason}\"}}}}\n",
    "</tool_call>\"\"\"\n",
    "    })\n",
    "    \n",
    "    return messages\n",
    "\n",
    "\n",
    "def generate_sft_dataset(patterns: List[LaunderingPattern], max_samples: int = 100) -> List[dict]:\n",
    "    \"\"\"Generate SFT dataset from laundering patterns.\"\"\"\n",
    "    dataset = []\n",
    "    for i, pattern in enumerate(random.sample(patterns, min(max_samples, len(patterns)))):\n",
    "        messages = generate_sft_sample(pattern)\n",
    "        if messages:\n",
    "            dataset.append({\"id\": f\"sft_{i}\", \"pattern_type\": pattern.pattern_type, \"messages\": messages})\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def format_for_unsloth_sft(dataset: List[dict]) -> List[dict]:\n",
    "    \"\"\"Format dataset for Unsloth SFT trainer using Qwen3 chat template.\"\"\"\n",
    "    formatted = []\n",
    "    for sample in dataset:\n",
    "        # Apply chat template to messages (with tools for proper formatting)\n",
    "        try:\n",
    "            text = tokenizer.apply_chat_template(\n",
    "                sample['messages'],\n",
    "                tools=QWEN3_TOOLS,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=False,\n",
    "            )\n",
    "        except Exception:\n",
    "            # Fallback without tools parameter\n",
    "            text = tokenizer.apply_chat_template(\n",
    "                sample['messages'],\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=False,\n",
    "            )\n",
    "        formatted.append({\"text\": text})\n",
    "    return formatted\n",
    "\n",
    "\n",
    "# Generate SFT dataset - prefer valid_patterns (with reachable sanctioned entities)\n",
    "print(\"üìä Generating SFT training data (Qwen3 Hermes-style)...\")\n",
    "patterns_for_sft = globals().get('valid_patterns') or laundering_patterns\n",
    "using_valid_sft = globals().get('valid_patterns') is not None and patterns_for_sft is globals().get('valid_patterns')\n",
    "print(f\"   Using {len(patterns_for_sft)} patterns for SFT (verified reachable: {'yes' if using_valid_sft else 'no'})\")\n",
    "sft_dataset = generate_sft_dataset(patterns_for_sft, max_samples=100)\n",
    "sft_formatted = format_for_unsloth_sft(sft_dataset)\n",
    "\n",
    "print(f\"‚úì Generated {len(sft_formatted)} SFT training samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. SFT Training with Unsloth\n",
    "\n",
    "Fine-tune with LoRA adapters (r=32, alpha=64) targeting attention and MLP layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SFT TRAINING WITH UNSLOTH - LoRA Fine-tuning (Conditional)\n",
    "# ============================================================================\n",
    "\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Check if SFT training is needed based on baseline assessment\n",
    "SKIP_SFT = not globals().get('NEEDS_SFT', True)\n",
    "\n",
    "if SKIP_SFT:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"‚è≠Ô∏è  SKIPPING SFT TRAINING\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"   Reason: {globals().get('TRAINING_RECOMMENDATION', 'N/A')}\")\n",
    "    print(f\"   Base model performs well on tool calling format.\")\n",
    "    print(f\"   Setting model_for_training = base model (no LoRA)\")\n",
    "    model_for_training = model  # Use base model without LoRA\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"üì• Configuring LoRA adapters for SFT...\")\n",
    "    \n",
    "    # Get PEFT model with LoRA\n",
    "    model_for_training = FastModel.get_peft_model(\n",
    "        model, \n",
    "        r=LORA_R, \n",
    "        target_modules=LORA_TARGET_MODULES, \n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        lora_dropout=0.05, \n",
    "        bias=\"none\", \n",
    "        use_gradient_checkpointing=\"unsloth\", \n",
    "        random_state=RANDOM_SEED,\n",
    "    )\n",
    "\n",
    "    # Create dataset\n",
    "    sft_hf_dataset = Dataset.from_list([{\"text\": s[\"text\"]} for s in sft_formatted])\n",
    "\n",
    "    # Training arguments\n",
    "    sft_output_dir = MODELS_DIR / \"sft_output\"\n",
    "    sft_args = TrainingArguments(\n",
    "        output_dir=str(sft_output_dir), per_device_train_batch_size=2, gradient_accumulation_steps=4,\n",
    "        num_train_epochs=SFT_EPOCHS, learning_rate=SFT_LEARNING_RATE, warmup_ratio=0.1,\n",
    "        logging_steps=10, save_strategy=\"epoch\", fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(), optim=\"adamw_8bit\", seed=RANDOM_SEED, report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    # Create trainer\n",
    "    sft_trainer = SFTTrainer(\n",
    "        model=model_for_training, tokenizer=tokenizer, train_dataset=sft_hf_dataset,\n",
    "        args=sft_args, max_seq_length=4096,\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"üéì STARTING SFT TRAINING\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    print(f\"  Epochs: {SFT_EPOCHS} | LR: {SFT_LEARNING_RATE} | LoRA r={LORA_R}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    # Train\n",
    "    sft_trainer.train()\n",
    "\n",
    "    # Save adapter\n",
    "    sft_adapter_path = MODELS_DIR / \"sft_adapter\"\n",
    "    model_for_training.save_pretrained(str(sft_adapter_path))\n",
    "    tokenizer.save_pretrained(str(sft_adapter_path))\n",
    "\n",
    "    print(f\"\\n‚úì SFT training complete! Adapter saved to: {sft_adapter_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. STAGE 2: Post-SFT Evaluation\n",
    "\n",
    "Evaluate the **SFT-tuned model** to measure improvement from supervised fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STAGE 2: POST-SFT EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üîç STAGE 2: POST-SFT EVALUATION\")\n",
    "print(\"   Testing model after Supervised Fine-Tuning\")\n",
    "\n",
    "# Check if SFT was actually performed\n",
    "if globals().get('SKIP_SFT', False):\n",
    "    print(\"=\" * 60)\n",
    "    print(\"‚è≠Ô∏è  SKIPPING POST-SFT EVALUATION (SFT was skipped)\")\n",
    "    print(\"=\" * 60)\n",
    "    post_sft_results = all_results[\"Baseline\"].copy()  # Use baseline as placeholder\n",
    "    post_sft_episodes = baseline_episodes\n",
    "    all_results[\"Post-SFT\"] = post_sft_results\n",
    "else:\n",
    "    # Switch to inference mode\n",
    "    FastModel.for_inference(model_for_training)\n",
    "\n",
    "    # Run post-SFT evaluation with detailed logging\n",
    "    post_sft_results, post_sft_episodes = run_evaluation(\n",
    "        model=model_for_training,\n",
    "        tokenizer=tokenizer,\n",
    "        stage_name=\"Post-SFT\",\n",
    "        n_episodes=EVAL_EPISODES,\n",
    "        use_llm_judge=bool(gemini_client),\n",
    "        verbose=True,\n",
    "        show_episode_details=SHOW_EPISODE_DETAILS,\n",
    "        show_memory=SHOW_MEMORY\n",
    "    )\n",
    "\n",
    "    # Store results for comparison\n",
    "    all_results[\"Post-SFT\"] = post_sft_results\n",
    "\n",
    "    # Show improvement over baseline\n",
    "    baseline_sr = all_results[\"Baseline\"]['success'].mean()\n",
    "    post_sft_sr = post_sft_results['success'].mean()\n",
    "    print(f\"\\nüìà SFT Improvement: {baseline_sr*100:.1f}% ‚Üí {post_sft_sr*100:.1f}% ({(post_sft_sr-baseline_sr)*100:+.1f}%)\")\n",
    "\n",
    "    print(\"\\nüìä Post-SFT Results Summary:\")\n",
    "    print(post_sft_results[['success', 'steps', 'total_reward', 'overall_score']].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. DPO Training - Direct Preference Optimization\n",
    "\n",
    "Train the agent using **Direct Preference Optimization (DPO)** with full episode trajectories.\n",
    "\n",
    "### What is DPO?\n",
    "\n",
    "DPO is a preference-based learning algorithm that teaches the model to prefer \"chosen\" (successful) trajectories over \"rejected\" (failed) trajectories. Unlike reward-based RL methods, DPO directly optimizes the policy using binary preference pairs.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "```\n",
    "L_DPO = -E[log œÉ(Œ≤ ¬∑ (log œÄ(y_w|x) - log œÄ(y_l|x) - log œÄ_ref(y_w|x) + log œÄ_ref(y_l|x)))]\n",
    "```\n",
    "Where:\n",
    "- `y_w` = chosen (successful) trajectory\n",
    "- `y_l` = rejected (failed) trajectory\n",
    "- `Œ≤` = KL penalty coefficient (controls deviation from reference model)\n",
    "- `œÄ_ref` = reference model (frozen base model)\n",
    "\n",
    "### Why DPO for AML Investigation?\n",
    "\n",
    "Based on research from [Zhang et al. (arXiv:2506.00845)](https://arxiv.org/abs/2506.00845) on graph reasoning with LLMs:\n",
    "\n",
    "| Approach | Pros | Cons |\n",
    "|----------|------|------|\n",
    "| **GRPO (single-step)** | On-policy learning | Only optimizes individual actions, not full trajectories |\n",
    "| **DPO (trajectory-level)** | Learns from full successful/failed episodes | Off-policy (can't explore new strategies) |\n",
    "\n",
    "**Key research findings:**\n",
    "- Process-based rewards outperform solution-based by ~24%\n",
    "- DPO is only ~5% behind GRPO but much simpler to implement\n",
    "- Full trajectory comparison teaches \"when to stop exploring\"\n",
    "\n",
    "### DPO Training Process\n",
    "\n",
    "| Step | Description | Output |\n",
    "|------|-------------|--------|\n",
    "| **12.1** | Collect episode traces | Successful & failed investigations |\n",
    "| **12.2** | Create preference pairs | DPO dataset (saved to disk) |\n",
    "| **12.3** | Train with DPO | Fine-tuned model adapters |\n",
    "\n",
    "### Dataset Caching\n",
    "\n",
    "The DPO dataset is saved to `DATA_DIR/dpo_dataset/` for:\n",
    "- **Review**: Inspect preference pairs before training\n",
    "- **Reproducibility**: Reuse same dataset across runs\n",
    "- **Efficiency**: Skip trace collection on subsequent runs\n",
    "\n",
    "Set `RECOMPUTE_DPO_DATASET = True` in config to regenerate.\n",
    "\n",
    "### Expected Outcomes\n",
    "\n",
    "- ‚úÖ Learn when to stop exploring and file SAR (avoid excessive steps)\n",
    "- ‚úÖ Prefer sanctioned entities on laundering paths\n",
    "- ‚úÖ Avoid filing incorrect SARs (learned from failed trajectories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DPO DATASET CREATION - Collect Traces and Create Preference Pairs\n",
    "# ============================================================================\n",
    "\n",
    "from copy import deepcopy\n",
    "import shutil\n",
    "import pickle\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def format_trajectory_for_dpo(episode: InvestigationEpisode) -> str:\n",
    "    \"\"\"\n",
    "    Format an episode trajectory as a sequence of tool calls for DPO.\n",
    "    This represents what the model should output for a successful/failed investigation.\n",
    "    \"\"\"\n",
    "    output_parts = []\n",
    "    \n",
    "    for step in episode.steps:\n",
    "        # Format each tool call in Hermes style\n",
    "        tool_call = {\n",
    "            \"name\": step[\"tool_name\"],\n",
    "            \"arguments\": step[\"arguments\"]\n",
    "        }\n",
    "        tool_call_str = f\"<tool_call>\\n{json.dumps(tool_call, indent=2)}\\n</tool_call>\"\n",
    "        \n",
    "        # Format the tool response\n",
    "        tool_response_str = f\"<tool_response>\\n{json.dumps(step['result'], default=str)}\\n</tool_response>\"\n",
    "        \n",
    "        output_parts.append(tool_call_str)\n",
    "        output_parts.append(tool_response_str)\n",
    "    \n",
    "    return \"\\n\".join(output_parts)\n",
    "\n",
    "\n",
    "def create_dpo_prompt(seed_account: str) -> str:\n",
    "    \"\"\"Create the initial prompt for a DPO training example.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"{INVESTIGATION_SYSTEM_PROMPT}\\n\\nINVESTIGATION TARGET: {seed_account}\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Begin investigation of account: {seed_account}\"}\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tools=QWEN3_TOOLS,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "    except Exception:\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "\n",
    "def collect_dpo_traces(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    n_episodes: int = 50,\n",
    "    verbose: bool = True\n",
    ") -> Tuple[List[InvestigationEpisode], List[InvestigationEpisode]]:\n",
    "    \"\"\"\n",
    "    Collect episode traces for DPO training.\n",
    "    Returns (successful_episodes, failed_episodes).\n",
    "    \"\"\"\n",
    "    successful = []\n",
    "    failed = []\n",
    "    \n",
    "    # Use valid_patterns if available\n",
    "    patterns_to_use = globals().get('valid_patterns') or laundering_patterns\n",
    "    seeds = [p.seed_account for p in patterns_to_use if p.seed_account]\n",
    "    random.shuffle(seeds)\n",
    "    \n",
    "    print(f\"{'=' * 60}\")\n",
    "    print(f\"üìä COLLECTING DPO TRAINING TRACES\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    print(f\"  Target Episodes: {n_episodes}\")\n",
    "    print(f\"  Available Seeds: {len(seeds)}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    \n",
    "    # Set model to inference mode\n",
    "    FastModel.for_inference(model)\n",
    "    \n",
    "    for i, seed in enumerate(seeds[:n_episodes]):\n",
    "        if verbose and (i + 1) % 10 == 0:\n",
    "            print(f\"  Progress: {i+1}/{min(n_episodes, len(seeds))} episodes (‚úì {len(successful)}, ‚úó {len(failed)})\")\n",
    "        \n",
    "        episode = run_investigation(\n",
    "            seed, model, tokenizer,\n",
    "            max_steps=MAX_STEPS,\n",
    "            verbose=False,\n",
    "            show_memory=False\n",
    "        )\n",
    "        \n",
    "        if episode.success:\n",
    "            successful.append(episode)\n",
    "        else:\n",
    "            failed.append(episode)\n",
    "    \n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"‚úì TRACE COLLECTION COMPLETE\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    print(f\"  Successful Episodes: {len(successful)} ({100*len(successful)/n_episodes:.1f}%)\")\n",
    "    print(f\"  Failed Episodes:     {len(failed)} ({100*len(failed)/n_episodes:.1f}%)\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    \n",
    "    return successful, failed\n",
    "\n",
    "\n",
    "def create_dpo_preference_pairs(\n",
    "    successful_episodes: List[InvestigationEpisode],\n",
    "    failed_episodes: List[InvestigationEpisode],\n",
    "    max_pairs: int = 100\n",
    ") -> List[dict]:\n",
    "    \"\"\"\n",
    "    Create preference pairs for DPO training.\n",
    "    Each pair: (prompt, chosen=success_trajectory, rejected=failed_trajectory)\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    \n",
    "    # Create pairs by matching successful with failed episodes\n",
    "    for succ_ep in successful_episodes:\n",
    "        for fail_ep in failed_episodes:\n",
    "            if len(pairs) >= max_pairs:\n",
    "                break\n",
    "            \n",
    "            # Create prompt from successful episode's seed\n",
    "            prompt = create_dpo_prompt(succ_ep.start_account)\n",
    "            \n",
    "            # Format trajectories\n",
    "            chosen = format_trajectory_for_dpo(succ_ep)\n",
    "            rejected = format_trajectory_for_dpo(fail_ep)\n",
    "            \n",
    "            pairs.append({\n",
    "                \"prompt\": prompt,\n",
    "                \"chosen\": chosen,\n",
    "                \"rejected\": rejected,\n",
    "            })\n",
    "        \n",
    "        if len(pairs) >= max_pairs:\n",
    "            break\n",
    "    \n",
    "    # Also create pairs with same seed if we have both success and failure\n",
    "    seed_to_success = {e.start_account: e for e in successful_episodes}\n",
    "    seed_to_failure = {e.start_account: e for e in failed_episodes}\n",
    "    \n",
    "    common_seeds = set(seed_to_success.keys()) & set(seed_to_failure.keys())\n",
    "    for seed in common_seeds:\n",
    "        if len(pairs) >= max_pairs:\n",
    "            break\n",
    "        \n",
    "        prompt = create_dpo_prompt(seed)\n",
    "        chosen = format_trajectory_for_dpo(seed_to_success[seed])\n",
    "        rejected = format_trajectory_for_dpo(seed_to_failure[seed])\n",
    "        \n",
    "        # These are high-quality pairs (same seed, different outcomes)\n",
    "        pairs.insert(0, {\n",
    "            \"prompt\": prompt,\n",
    "            \"chosen\": chosen,\n",
    "            \"rejected\": rejected,\n",
    "        })\n",
    "    \n",
    "    return pairs[:max_pairs]\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DPO DATASET CREATION WITH CACHING\n",
    "# ============================================================================\n",
    "\n",
    "# Define paths for caching\n",
    "DPO_DATASET_DIR = DATA_DIR / \"dpo_dataset\"\n",
    "DPO_PAIRS_FILE = DPO_DATASET_DIR / \"preference_pairs.json\"\n",
    "DPO_EPISODES_FILE = DPO_DATASET_DIR / \"episodes.pkl\"\n",
    "DPO_METADATA_FILE = DPO_DATASET_DIR / \"metadata.json\"\n",
    "\n",
    "def save_dpo_dataset(pairs: List[dict], successful_eps: List, failed_eps: List, metadata: dict):\n",
    "    \"\"\"Save DPO dataset to disk for review and reuse.\"\"\"\n",
    "    DPO_DATASET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save preference pairs as JSON (human-readable)\n",
    "    with open(DPO_PAIRS_FILE, 'w') as f:\n",
    "        json.dump(pairs, f, indent=2, default=str)\n",
    "    \n",
    "    # Save episodes as pickle (for potential re-processing)\n",
    "    with open(DPO_EPISODES_FILE, 'wb') as f:\n",
    "        pickle.dump({'successful': successful_eps, 'failed': failed_eps}, f)\n",
    "    \n",
    "    # Save metadata\n",
    "    with open(DPO_METADATA_FILE, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüíæ DPO Dataset saved to: {DPO_DATASET_DIR}\")\n",
    "    print(f\"   - Preference pairs: {DPO_PAIRS_FILE.name} ({len(pairs)} pairs)\")\n",
    "    print(f\"   - Episodes: {DPO_EPISODES_FILE.name}\")\n",
    "    print(f\"   - Metadata: {DPO_METADATA_FILE.name}\")\n",
    "\n",
    "\n",
    "def load_dpo_dataset() -> Tuple[List[dict], dict]:\n",
    "    \"\"\"Load cached DPO dataset from disk.\"\"\"\n",
    "    if not DPO_PAIRS_FILE.exists():\n",
    "        return None, None\n",
    "    \n",
    "    with open(DPO_PAIRS_FILE, 'r') as f:\n",
    "        pairs = json.load(f)\n",
    "    \n",
    "    metadata = {}\n",
    "    if DPO_METADATA_FILE.exists():\n",
    "        with open(DPO_METADATA_FILE, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "    \n",
    "    return pairs, metadata\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN DATASET CREATION LOGIC\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üìä DPO DATASET CREATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  Recompute: {RECOMPUTE_DPO_DATASET}\")\n",
    "print(f\"  Cache Dir: {DPO_DATASET_DIR}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check if we should use cached dataset\n",
    "cached_pairs, cached_metadata = load_dpo_dataset()\n",
    "use_cache = not RECOMPUTE_DPO_DATASET and cached_pairs is not None\n",
    "\n",
    "if use_cache:\n",
    "    print(f\"\\n‚úì Using cached DPO dataset\")\n",
    "    print(f\"  Loaded {len(cached_pairs)} preference pairs\")\n",
    "    if cached_metadata:\n",
    "        print(f\"  Created: {cached_metadata.get('created_at', 'unknown')}\")\n",
    "        print(f\"  Success rate: {cached_metadata.get('success_rate', 'unknown')}\")\n",
    "    \n",
    "    dpo_pairs = cached_pairs\n",
    "    \n",
    "else:\n",
    "    print(f\"\\nüì• Creating new DPO dataset...\")\n",
    "    \n",
    "    # Step 1: Collect traces for DPO\n",
    "    print(\"\\nüì• Step 1: Collecting episode traces...\")\n",
    "    \n",
    "    # Use baseline_episodes if available, otherwise collect new traces\n",
    "    if 'baseline_episodes' in dir() and len(baseline_episodes) >= 20:\n",
    "        print(f\"   Using existing baseline_episodes ({len(baseline_episodes)} episodes)\")\n",
    "        all_trace_episodes = baseline_episodes\n",
    "    else:\n",
    "        print(f\"   Collecting {DPO_TRACE_EPISODES} new episodes...\")\n",
    "        successful_eps, failed_eps = collect_dpo_traces(\n",
    "            model, tokenizer, \n",
    "            n_episodes=DPO_TRACE_EPISODES,\n",
    "            verbose=True\n",
    "        )\n",
    "        all_trace_episodes = successful_eps + failed_eps\n",
    "    \n",
    "    # Split into successful and failed\n",
    "    successful_episodes = [e for e in all_trace_episodes if e.success]\n",
    "    failed_episodes = [e for e in all_trace_episodes if not e.success]\n",
    "    \n",
    "    print(f\"\\nüìä Episode Distribution:\")\n",
    "    print(f\"   Successful: {len(successful_episodes)}\")\n",
    "    print(f\"   Failed:     {len(failed_episodes)}\")\n",
    "    \n",
    "    # Check if we have enough data\n",
    "    if len(successful_episodes) < 3 or len(failed_episodes) < 3:\n",
    "        print(\"\\n‚ö†Ô∏è  Insufficient data for DPO training!\")\n",
    "        print(\"   Need at least 3 successful and 3 failed episodes.\")\n",
    "        print(\"   Collecting additional traces...\")\n",
    "        \n",
    "        additional_successful, additional_failed = collect_dpo_traces(\n",
    "            model, tokenizer,\n",
    "            n_episodes=DPO_TRACE_EPISODES,\n",
    "            verbose=True\n",
    "        )\n",
    "        successful_episodes.extend(additional_successful)\n",
    "        failed_episodes.extend(additional_failed)\n",
    "        \n",
    "        print(f\"\\nüìä Updated Episode Distribution:\")\n",
    "        print(f\"   Successful: {len(successful_episodes)}\")\n",
    "        print(f\"   Failed:     {len(failed_episodes)}\")\n",
    "    \n",
    "    # Step 2: Create preference pairs\n",
    "    print(f\"\\nüì• Step 2: Creating DPO preference pairs...\")\n",
    "    dpo_pairs = create_dpo_preference_pairs(\n",
    "        successful_episodes,\n",
    "        failed_episodes,\n",
    "        max_pairs=min(DPO_MAX_PAIRS, len(successful_episodes) * len(failed_episodes))\n",
    "    )\n",
    "    print(f\"   Created {len(dpo_pairs)} preference pairs\")\n",
    "    \n",
    "    # Step 3: Save dataset\n",
    "    metadata = {\n",
    "        \"created_at\": str(pd.Timestamp.now()),\n",
    "        \"n_successful\": len(successful_episodes),\n",
    "        \"n_failed\": len(failed_episodes),\n",
    "        \"n_pairs\": len(dpo_pairs),\n",
    "        \"success_rate\": len(successful_episodes) / (len(successful_episodes) + len(failed_episodes)),\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"max_pairs\": DPO_MAX_PAIRS,\n",
    "    }\n",
    "    save_dpo_dataset(dpo_pairs, successful_episodes, failed_episodes, metadata)\n",
    "\n",
    "# Create HuggingFace Dataset for DPO training\n",
    "dpo_dataset = Dataset.from_list(dpo_pairs)\n",
    "\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(f\"‚úì DPO DATASET READY\")\n",
    "print(f\"{'=' * 70}\")\n",
    "print(f\"  Total pairs: {len(dpo_dataset)}\")\n",
    "if dpo_pairs:\n",
    "    print(f\"  Avg prompt length: {sum(len(p['prompt']) for p in dpo_pairs) / len(dpo_pairs):.0f} chars\")\n",
    "    print(f\"  Avg chosen length: {sum(len(p['chosen']) for p in dpo_pairs) / len(dpo_pairs):.0f} chars\")\n",
    "    print(f\"  Avg rejected length: {sum(len(p['rejected']) for p in dpo_pairs) / len(dpo_pairs):.0f} chars\")\n",
    "print(f\"{'=' * 70}\")\n",
    "\n",
    "# Show example pair for review\n",
    "if dpo_pairs:\n",
    "    print(f\"\\nüìã Example Preference Pair (first 500 chars each):\")\n",
    "    print(f\"\\n--- PROMPT ---\")\n",
    "    print(dpo_pairs[0]['prompt'][:500])\n",
    "    print(f\"\\n--- CHOSEN (successful trajectory) ---\")\n",
    "    print(dpo_pairs[0]['chosen'][:500])\n",
    "    print(f\"\\n--- REJECTED (failed trajectory) ---\")\n",
    "    print(dpo_pairs[0]['rejected'][:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3 DPO Model Training\n",
    "\n",
    "Train the model using the preference pairs created above. The DPO trainer will:\n",
    "1. **Load the quantized model** with LoRA adapters\n",
    "2. **Optimize preferences**: Increase probability of chosen (successful) trajectories\n",
    "3. **Apply KL penalty**: Prevent excessive deviation from the reference model\n",
    "4. **Save trained adapters**: For evaluation and deployment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DPO MODEL TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "import shutil\n",
    "import gc\n",
    "\n",
    "# ============================================================================\n",
    "# GPU HEALTH CHECK AND MEMORY CLEANUP\n",
    "# ============================================================================\n",
    "# NOTE: If you encounter CUDA errors like \"operation not permitted\" or\n",
    "# \"cudaErrorNotPermitted\", it usually means the GPU is in a bad state.\n",
    "# You'll need to: Kernel ‚Üí Restart Kernel, then run all cells from the beginning.\n",
    "\n",
    "print(\"üîß Pre-Training GPU Health Check...\")\n",
    "\n",
    "# Force garbage collection and clear CUDA cache\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Quick GPU health test\n",
    "try:\n",
    "    test_tensor = torch.randn(100, 100, device=\"cuda\")\n",
    "    result = torch.matmul(test_tensor, test_tensor)\n",
    "    del test_tensor, result\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"   ‚úì GPU is responsive and healthy\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  GPU ERROR DETECTED: {e}\")\n",
    "    print(\"   ‚ö†Ô∏è  Please restart the kernel (Kernel ‚Üí Restart) and run all cells again.\")\n",
    "    raise RuntimeError(f\"GPU is in a bad state. Restart the kernel and try again. Error: {e}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CLEANUP: Remove previous training artifacts for clean re-runs\n",
    "# ============================================================================\n",
    "\n",
    "def cleanup_training_artifacts():\n",
    "    \"\"\"Clean up previous training outputs for a fresh run.\"\"\"\n",
    "    dirs_to_clean = [\n",
    "        MODELS_DIR / \"dpo_output\",\n",
    "        MODELS_DIR / \"dpo_adapter\",\n",
    "        MODELS_DIR / \"aml_agent_final\",\n",
    "    ]\n",
    "    \n",
    "    cleaned = 0\n",
    "    for dir_path in dirs_to_clean:\n",
    "        if dir_path.exists():\n",
    "            shutil.rmtree(dir_path)\n",
    "            cleaned += 1\n",
    "    \n",
    "    if cleaned > 0:\n",
    "        print(f\"üßπ Cleaned up {cleaned} previous training directories\")\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "# Clean up previous artifacts if re-running\n",
    "cleanup_training_artifacts()\n",
    "\n",
    "# ============================================================================\n",
    "# PREPARE MODEL WITH LORA ADAPTERS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üéØ DPO MODEL TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if we have a model with adapters from SFT\n",
    "# Use globals() for reliable variable checking in Jupyter\n",
    "existing_model = globals().get('model_for_training', None)\n",
    "has_existing_adapters = (existing_model is not None and \n",
    "                         hasattr(existing_model, 'peft_config') and \n",
    "                         existing_model.peft_config is not None)\n",
    "\n",
    "print(f\"  Existing model_for_training: {'Yes' if existing_model is not None else 'No'}\")\n",
    "print(f\"  Has LoRA adapters: {'Yes' if has_existing_adapters else 'No'}\")\n",
    "\n",
    "if has_existing_adapters:\n",
    "    print(\"\\n‚úì Using existing model with LoRA adapters (from SFT)\")\n",
    "    model_for_dpo = existing_model\n",
    "else:\n",
    "    # Need to add LoRA adapters for DPO training\n",
    "    # ALWAYS add fresh adapters to the base model\n",
    "    print(\"\\nüì• Adding LoRA adapters for DPO training...\")\n",
    "    print(f\"   Base model: {MODEL_NAME}\")\n",
    "    \n",
    "    model_for_dpo = FastModel.get_peft_model(\n",
    "        model,  # Always use base model to add fresh adapters\n",
    "        r=LORA_R,\n",
    "        target_modules=LORA_TARGET_MODULES,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        use_gradient_checkpointing=\"unsloth\",\n",
    "        random_state=RANDOM_SEED,\n",
    "    )\n",
    "    print(f\"   ‚úì LoRA adapters added (r={LORA_R}, alpha={LORA_ALPHA})\")\n",
    "\n",
    "# Verify adapters are attached\n",
    "if not hasattr(model_for_dpo, 'peft_config') or model_for_dpo.peft_config is None:\n",
    "    raise RuntimeError(\"Failed to attach LoRA adapters to model. Please restart the notebook and run all cells.\")\n",
    "\n",
    "print(f\"\\n‚úì Model ready for DPO training\")\n",
    "print(f\"  Model type: {type(model_for_dpo).__name__}\")\n",
    "print(f\"  Has peft_config: {hasattr(model_for_dpo, 'peft_config')}\")\n",
    "\n",
    "# IMPORTANT: Explicitly set model to training mode\n",
    "# This is required after inference mode was used for trace collection\n",
    "print(\"\\nüì• Setting model to training mode...\")\n",
    "if hasattr(model_for_dpo, 'for_training'):\n",
    "    model_for_dpo.for_training()\n",
    "    print(\"   ‚úì Called model.for_training()\")\n",
    "model_for_dpo.train()\n",
    "print(\"   ‚úì Model set to train mode\")\n",
    "\n",
    "# Clear CUDA cache to avoid memory issues\n",
    "torch.cuda.empty_cache()\n",
    "print(\"   ‚úì CUDA cache cleared\")\n",
    "\n",
    "# Create reference model (frozen copy for KL divergence)\n",
    "# For DPO, we need the base model as reference\n",
    "print(\"\\nüì• Creating reference model for DPO...\")\n",
    "ref_model = None  # TRL DPOTrainer can use implicit reference model\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURE DPO TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "dpo_output_dir = MODELS_DIR / \"dpo_output\"\n",
    "dpo_config = DPOConfig(\n",
    "    output_dir=str(dpo_output_dir),\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=DPO_EPOCHS,\n",
    "    learning_rate=DPO_LEARNING_RATE,\n",
    "    beta=DPO_BETA,\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\",\n",
    "    max_length=4096,\n",
    "    max_prompt_length=1024,\n",
    "    remove_unused_columns=False,\n",
    "    # Optimization\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    optim=\"adamw_8bit\",\n",
    ")\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"üéØ STARTING DPO TRAINING\")\n",
    "print(f\"{'=' * 60}\")\n",
    "print(f\"  Epochs:         {DPO_EPOCHS}\")\n",
    "print(f\"  Learning Rate:  {DPO_LEARNING_RATE}\")\n",
    "print(f\"  Beta (KL pen.): {DPO_BETA}\")\n",
    "print(f\"  Pairs:          {len(dpo_dataset)}\")\n",
    "print(f\"  Max Length:     4096\")\n",
    "print(f\"{'=' * 60}\")\n",
    "\n",
    "# ============================================================================\n",
    "# RUN DPO TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "# Ensure model is in training mode and synchronized\n",
    "model_for_dpo.train()\n",
    "torch.cuda.synchronize()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "try:\n",
    "    dpo_trainer = DPOTrainer(\n",
    "        model=model_for_dpo,\n",
    "        ref_model=ref_model,\n",
    "        train_dataset=dpo_dataset,\n",
    "        processing_class=tokenizer,\n",
    "        args=dpo_config,\n",
    "    )\n",
    "\n",
    "    # Train with DPO\n",
    "    dpo_trainer.train()\n",
    "except RuntimeError as e:\n",
    "    if \"CUDA\" in str(e) or \"not permitted\" in str(e) or \"out of memory\" in str(e).lower():\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"‚ö†Ô∏è  CUDA ERROR DURING DPO TRAINING\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"\\nThis usually means the GPU is in a bad state.\")\n",
    "        print(\"Please restart the kernel (Kernel ‚Üí Restart)\")\n",
    "        print(\"and run all cells from the beginning.\")\n",
    "        print(\"=\"*60)\n",
    "    raise\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE TRAINED ADAPTERS\n",
    "# ============================================================================\n",
    "\n",
    "dpo_adapter_path = MODELS_DIR / \"dpo_adapter\"\n",
    "model_for_dpo.save_pretrained(str(dpo_adapter_path))\n",
    "tokenizer.save_pretrained(str(dpo_adapter_path))\n",
    "\n",
    "# Update model_for_training reference for subsequent evaluation\n",
    "model_for_training = model_for_dpo\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"‚úì DPO TRAINING COMPLETE\")\n",
    "print(f\"{'=' * 60}\")\n",
    "print(f\"  Adapter saved to: {dpo_adapter_path}\")\n",
    "print(f\"{'=' * 60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. STAGE 3: Post-DPO Evaluation\n",
    "\n",
    "Evaluate the **DPO-trained model** to measure improvement from trajectory-level preference learning.\n",
    "\n",
    "**Expected Improvements:**\n",
    "- ‚úÖ Better \"when to stop\" behavior (fewer excessive exploration steps)\n",
    "- ‚úÖ Higher success rate (learned from full successful trajectories)  \n",
    "- ‚úÖ More efficient investigations (fewer average steps to reach SAR)\n",
    "- ‚úÖ Improved decision quality (learned preference for sanctioned entities on laundering paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STAGE 3: POST-DPO EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üîç STAGE 3: POST-DPO EVALUATION\")\n",
    "print(\"   Testing model after DPO Trajectory Preference Learning\")\n",
    "\n",
    "# DPO training always runs (no conditional skip)\n",
    "if True:\n",
    "    # Switch to inference mode\n",
    "    FastModel.for_inference(model_for_training)\n",
    "\n",
    "    # Run post-DPO evaluation with detailed logging\n",
    "    post_dpo_results, post_dpo_episodes = run_evaluation(\n",
    "        model=model_for_training,\n",
    "        tokenizer=tokenizer,\n",
    "        stage_name=\"Post-DPO\",\n",
    "        n_episodes=EVAL_EPISODES,\n",
    "        use_llm_judge=bool(gemini_client),\n",
    "        verbose=True,\n",
    "        show_episode_details=SHOW_EPISODE_DETAILS,\n",
    "        show_memory=SHOW_MEMORY\n",
    "    )\n",
    "\n",
    "    # Store results for comparison\n",
    "    all_results[\"Post-DPO\"] = post_dpo_results\n",
    "\n",
    "    print(\"\\nüìä Post-DPO Results Summary:\")\n",
    "    print(post_dpo_results[['success', 'steps', 'total_reward', 'overall_score']].describe())\n",
    "    \n",
    "    # Compare with baseline\n",
    "    if \"Baseline\" in all_results:\n",
    "        baseline_success = all_results[\"Baseline\"]['success'].mean() * 100\n",
    "        dpo_success = post_dpo_results['success'].mean() * 100\n",
    "        baseline_steps = all_results[\"Baseline\"]['steps'].mean()\n",
    "        dpo_steps = post_dpo_results['steps'].mean()\n",
    "        \n",
    "        print(f\"\\nüìà DPO vs Baseline:\")\n",
    "        print(f\"   Success Rate: {baseline_success:.1f}% ‚Üí {dpo_success:.1f}% ({dpo_success - baseline_success:+.1f}%)\")\n",
    "        print(f\"   Avg Steps:    {baseline_steps:.1f} ‚Üí {dpo_steps:.1f} ({dpo_steps - baseline_steps:+.1f})\")\n",
    "    \n",
    "    # Compare with SFT if available\n",
    "    if \"Post-SFT\" in all_results:\n",
    "        sft_success = all_results[\"Post-SFT\"]['success'].mean() * 100\n",
    "        sft_steps = all_results[\"Post-SFT\"]['steps'].mean()\n",
    "        \n",
    "        print(f\"\\nüìà DPO vs SFT:\")\n",
    "        print(f\"   Success Rate: {sft_success:.1f}% ‚Üí {dpo_success:.1f}% ({dpo_success - sft_success:+.1f}%)\")\n",
    "        print(f\"   Avg Steps:    {sft_steps:.1f} ‚Üí {dpo_steps:.1f} ({dpo_steps - sft_steps:+.1f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 14. Final Comparison: All Training Stages\n",
    "\n",
    "Compare metrics across all training stages (Baseline ‚Üí SFT ‚Üí DPO):\n",
    "\n",
    "| Metric | Description |\n",
    "|--------|-------------|\n",
    "| **Success Rate** | Percentage of investigations that correctly identified a sanctioned entity on the laundering path |\n",
    "| **Average Steps** | Efficiency of investigation (lower is better) |\n",
    "| **Average Reward** | Cumulative reward from the investigation (higher is better) |\n",
    "| **LLM-as-Judge Scores** | Strategy, Persistence, Outcome, and Overall scores from Gemini evaluation |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FINAL COMPARISON - Baseline vs SFT vs DPO\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä FINAL COMPARISON: ALL TRAINING STAGES\")\n",
    "print(\"=\" * 80)\n",
    "print(\"   Comparing: Baseline ‚Üí SFT ‚Üí DPO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Build comparison DataFrame\n",
    "comparison_data = []\n",
    "for stage_name, results_df in all_results.items():\n",
    "    comparison_data.append({\n",
    "        \"Stage\": stage_name,\n",
    "        \"Success Rate (%)\": results_df['success'].mean() * 100,\n",
    "        \"Avg Steps\": results_df['steps'].mean(),\n",
    "        \"Avg Reward\": results_df['total_reward'].mean(),\n",
    "        \"Avg Strategy Score\": results_df['strategy_score'].mean() if 'strategy_score' in results_df else 0,\n",
    "        \"Avg Persistence Score\": results_df['persistence_score'].mean() if 'persistence_score' in results_df else 0,\n",
    "        \"Avg Outcome Score\": results_df['outcome_score'].mean() if 'outcome_score' in results_df else 0,\n",
    "        \"Avg Overall Score\": results_df['overall_score'].mean() if 'overall_score' in results_df else 0,\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.set_index(\"Stage\")\n",
    "\n",
    "# Display comparison table\n",
    "print(\"\\nüìà METRICS COMPARISON:\")\n",
    "print(\"-\" * 80)\n",
    "print(comparison_df.round(2).to_string())\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Calculate improvements\n",
    "if len(comparison_data) >= 2:\n",
    "    baseline = comparison_data[0]\n",
    "    \n",
    "    print(\"\\nüìä IMPROVEMENTS OVER BASELINE:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, stage in enumerate(comparison_data[1:], 1):\n",
    "        stage_name = stage[\"Stage\"]\n",
    "        success_improvement = stage[\"Success Rate (%)\"] - baseline[\"Success Rate (%)\"]\n",
    "        reward_improvement = stage[\"Avg Reward\"] - baseline[\"Avg Reward\"]\n",
    "        score_improvement = stage[\"Avg Overall Score\"] - baseline[\"Avg Overall Score\"]\n",
    "        \n",
    "        print(f\"\\n  {stage_name}:\")\n",
    "        print(f\"    Success Rate: {success_improvement:+.1f}%\")\n",
    "        print(f\"    Avg Reward:   {reward_improvement:+.2f}\")\n",
    "        print(f\"    Overall Score: {score_improvement:+.1f}/10\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Log to MLflow\n",
    "with mlflow.start_run(run_name=\"final_comparison\"):\n",
    "    for stage_name, results_df in all_results.items():\n",
    "        mlflow.log_metrics({\n",
    "            f\"{stage_name}_success_rate\": results_df['success'].mean(),\n",
    "            f\"{stage_name}_avg_steps\": results_df['steps'].mean(),\n",
    "            f\"{stage_name}_avg_reward\": results_df['total_reward'].mean(),\n",
    "            f\"{stage_name}_avg_overall_score\": results_df.get('overall_score', pd.Series([0])).mean(),\n",
    "        })\n",
    "\n",
    "print(\"‚úì Comparison logged to MLflow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATION - Performance Comparison Charts\n",
    "# ============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create comparison visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('AML Investigation Agent (Qwen3-30B-A3B-Instruct): Training Stage Comparison', fontsize=14, fontweight='bold')\n",
    "\n",
    "stages = list(all_results.keys())\n",
    "# Colors: Baseline (red), SFT (blue), DPO (green)\n",
    "colors = ['#e74c3c', '#3498db', '#2ecc71'][:len(stages)]\n",
    "\n",
    "# 1. Success Rate\n",
    "ax1 = axes[0, 0]\n",
    "success_rates = [all_results[s]['success'].mean() * 100 for s in stages]\n",
    "bars1 = ax1.bar(stages, success_rates, color=colors, edgecolor='black', linewidth=1.2)\n",
    "ax1.set_ylabel('Success Rate (%)', fontweight='bold')\n",
    "ax1.set_title('SAR Submission Accuracy', fontweight='bold')\n",
    "ax1.set_ylim(0, 100)\n",
    "for bar, val in zip(bars1, success_rates):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2, f'{val:.1f}%', \n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 2. Average Reward\n",
    "ax2 = axes[0, 1]\n",
    "avg_rewards = [all_results[s]['total_reward'].mean() for s in stages]\n",
    "bars2 = ax2.bar(stages, avg_rewards, color=colors, edgecolor='black', linewidth=1.2)\n",
    "ax2.set_ylabel('Average Reward', fontweight='bold')\n",
    "ax2.set_title('Cumulative Reward Score', fontweight='bold')\n",
    "ax2.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "for bar, val in zip(bars2, avg_rewards):\n",
    "    ypos = bar.get_height() + 0.1 if val >= 0 else bar.get_height() - 0.3\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, ypos, f'{val:.2f}', \n",
    "             ha='center', va='bottom' if val >= 0 else 'top', fontweight='bold')\n",
    "\n",
    "# 3. Average Steps\n",
    "ax3 = axes[1, 0]\n",
    "avg_steps = [all_results[s]['steps'].mean() for s in stages]\n",
    "bars3 = ax3.bar(stages, avg_steps, color=colors, edgecolor='black', linewidth=1.2)\n",
    "ax3.set_ylabel('Average Steps', fontweight='bold')\n",
    "ax3.set_title('Investigation Efficiency', fontweight='bold')\n",
    "for bar, val in zip(bars3, avg_steps):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3, f'{val:.1f}', \n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 4. LLM-as-Judge Overall Score\n",
    "ax4 = axes[1, 1]\n",
    "if 'overall_score' in all_results[stages[0]].columns:\n",
    "    overall_scores = [all_results[s]['overall_score'].mean() for s in stages]\n",
    "    bars4 = ax4.bar(stages, overall_scores, color=colors, edgecolor='black', linewidth=1.2)\n",
    "    ax4.set_ylabel('Overall Score (0-10)', fontweight='bold')\n",
    "    ax4.set_title('LLM-as-Judge Score', fontweight='bold')\n",
    "    ax4.set_ylim(0, 10)\n",
    "    for bar, val in zip(bars4, overall_scores):\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.2, f'{val:.1f}', \n",
    "                 ha='center', va='bottom', fontweight='bold')\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'LLM-as-Judge\\nNot Available', ha='center', va='center', fontsize=12)\n",
    "    ax4.set_title('LLM-as-Judge Score', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "comparison_chart_path = OUTPUT_DIR / \"training_comparison.png\"\n",
    "plt.savefig(str(comparison_chart_path), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Comparison chart saved to {comparison_chart_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 15. Save Final Model\n",
    "\n",
    "Save the trained model adapters for deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SAVE FINAL MODEL\n",
    "# ============================================================================\n",
    "\n",
    "# Determine what training was actually performed\n",
    "skip_sft = globals().get('SKIP_SFT', False)\n",
    "dpo_trained = 'dpo_adapter_path' in dir()\n",
    "\n",
    "# Check if we have a trainable model with adapters\n",
    "has_adapters = hasattr(model_for_training, 'peft_config') or hasattr(model_for_training, 'active_adapters')\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"üíæ SAVING MODEL\")\n",
    "print(f\"{'=' * 60}\")\n",
    "print(f\"  SFT performed:    {'No' if skip_sft else 'Yes'}\")\n",
    "print(f\"  DPO performed:    {'Yes' if dpo_trained else 'No'}\")\n",
    "print(f\"  Has LoRA adapters: {'Yes' if has_adapters else 'No'}\")\n",
    "\n",
    "# Save final adapter to models directory (only if we have adapters)\n",
    "final_model_path = MODELS_DIR / \"aml_agent_final\"\n",
    "if has_adapters:\n",
    "    model_for_training.save_pretrained(str(final_model_path))\n",
    "    tokenizer.save_pretrained(str(final_model_path))\n",
    "    print(f\"  ‚úì Model saved to: {final_model_path}\")\n",
    "else:\n",
    "    # No training was done, just save tokenizer for reference\n",
    "    tokenizer.save_pretrained(str(final_model_path))\n",
    "    print(f\"  ‚ö†Ô∏è  No adapters to save (no training performed)\")\n",
    "    print(f\"  ‚úì Tokenizer saved to: {final_model_path}\")\n",
    "\n",
    "# Save comparison results\n",
    "comparison_csv_path = OUTPUT_DIR / \"training_comparison.csv\"\n",
    "comparison_df.to_csv(str(comparison_csv_path))\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"‚úÖ TRAINING COMPLETE\")\n",
    "print(f\"{'=' * 60}\")\n",
    "print(f\"  Final Model:      {final_model_path}\")\n",
    "\n",
    "# Only print adapter paths if they exist\n",
    "if 'sft_adapter_path' in dir() and not skip_sft:\n",
    "    print(f\"  SFT Adapter:      {sft_adapter_path}\")\n",
    "else:\n",
    "    print(f\"  SFT Adapter:      (skipped)\")\n",
    "\n",
    "# DPO adapter\n",
    "if 'dpo_adapter_path' in dir():\n",
    "    print(f\"  DPO Adapter:      {dpo_adapter_path}\")\n",
    "else:\n",
    "    print(f\"  DPO Adapter:      (not trained)\")\n",
    "\n",
    "print(f\"  Comparison Chart: {OUTPUT_DIR / 'training_comparison.png'}\")\n",
    "print(f\"  Comparison CSV:   {comparison_csv_path}\")\n",
    "print(f\"{'=' * 60}\")\n",
    "\n",
    "print(f\"\\nüéâ AML Investigation Agent (Qwen3-30B-A3B-Instruct) Training Complete!\")\n",
    "print(f\"\\nüìä Final Performance Summary:\")\n",
    "print(comparison_df.round(2).to_string())\n",
    "\n",
    "print(f\"\\nüîß Model Configuration:\")\n",
    "print(f\"  Base Model:       {MODEL_NAME}\")\n",
    "print(f\"  Architecture:     Mixture of Experts (MOE)\")\n",
    "print(f\"  Tool Format:      Hermes-style <tool_call> tags\")\n",
    "print(f\"  Quantization:     4-bit\")\n",
    "\n",
    "print(f\"\\nNext Steps:\")\n",
    "if has_adapters:\n",
    "    print(f\"  1. Load adapter with Unsloth FastModel for inference\")\n",
    "    print(f\"  2. Deploy agent with custom orchestration\")\n",
    "    print(f\"  3. Monitor with MLflow tracing\")\n",
    "else:\n",
    "    print(f\"  1. Use base model directly (no fine-tuning was needed)\")\n",
    "    print(f\"  2. Deploy agent with custom orchestration\")\n",
    "    print(f\"  3. Monitor with MLflow tracing\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
