{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Benchmarking Agentic Qwen3 on HotpotQA\n",
        "\n",
        "This notebook implements a **Reflexive Metacognitive Agent** powered by `qwen3:30b` (via Ollama) to solve multi-hop reasoning tasks from the HotpotQA dataset.\n",
        "\n",
        "## Architecture Overview\n",
        "\n",
        "The agent employs a self-correcting loop structure:\n",
        "\n",
        "1. **Metacognitive Router** - Classifies query complexity (SIMPLE/COMPLEX)\n",
        "2. **Planner** - Breaks down complex queries into search steps\n",
        "3. **Executor** - Retrieves information from the HotpotQA context\n",
        "4. **Reflector** - Validates retrieved information and decides next action\n",
        "5. **Final Answer Synthesis** - Generates the final response\n",
        "\n",
        "## Components\n",
        "\n",
        "- **LangGraph** - Agent orchestration\n",
        "- **Ollama** - Local LLM inference (qwen3:30b)\n",
        "- **MLflow** - Tracing and experiment tracking\n",
        "- **Google Gemini** - LLM-as-a-Judge evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Dependencies\n",
        "\n",
        "1. **Ollama**: Install from [ollama.ai](https://ollama.ai) and pull the model:\n",
        "   ```bash\n",
        "   ollama pull qwen3:30b\n",
        "   ```\n",
        "\n",
        "2. **MLflow Server** (for tracing): Start before running the benchmark:\n",
        "   ```bash\n",
        "   mlflow ui --port 5000\n",
        "   ```\n",
        "\n",
        "3. **Gemini API Key**: Add to `.env`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Loaded .env from: /home/vincent/Code/zgx-onboard/.env\n",
            "\n",
            "Configuration:\n",
            "  Ollama URL: http://localhost:11434\n",
            "  Ollama Model: qwen3:30b\n",
            "  Gemini API Key: ‚úì Set\n",
            "  Max Reflections: 3\n",
            "  Sample Size: 10\n",
            "  Random Seed: 42\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from typing import TypedDict, Literal, Annotated, List, Optional\n",
        "from dotenv import load_dotenv\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load environment variables from .env file\n",
        "# First try the project root, then current directory\n",
        "project_root = Path().absolute().parent.parent\n",
        "env_path = project_root / \".env\"\n",
        "if env_path.exists():\n",
        "    load_dotenv(env_path)\n",
        "    print(f\"‚úì Loaded .env from: {env_path}\")\n",
        "else:\n",
        "    load_dotenv()\n",
        "    print(\"‚úì Loaded .env from current directory\")\n",
        "\n",
        "# Configuration\n",
        "OLLAMA_BASE_URL = os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n",
        "OLLAMA_MODEL = os.getenv(\"OLLAMA_MODEL\", \"qwen3:30b\")\n",
        "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
        "GEMINI_MODEL = \"gemini-2.5-pro\"\n",
        "\n",
        "# Agent configuration\n",
        "MAX_REFLECTIONS = 3  # Prevent infinite loops\n",
        "SAMPLE_SIZE = 10  # Number of questions to evaluate\n",
        "RANDOM_SEED = 42  # Seed for reproducible random sampling\n",
        "\n",
        "print(f\"\\nConfiguration:\")\n",
        "print(f\"  Ollama URL: {OLLAMA_BASE_URL}\")\n",
        "print(f\"  Ollama Model: {OLLAMA_MODEL}\")\n",
        "print(f\"  Gemini API Key: {'‚úì Set' if GEMINI_API_KEY else '‚úó Not set'}\")\n",
        "print(f\"  Max Reflections: {MAX_REFLECTIONS}\")\n",
        "print(f\"  Sample Size: {SAMPLE_SIZE}\")\n",
        "print(f\"  Random Seed: {RANDOM_SEED}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Connectivity Tests\n",
        "\n",
        "Before proceeding, we verify connectivity to both the Ollama and Gemini models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Ollama Connectivity Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing Ollama connection via LangChain ChatOllama...\n",
            "  Base URL: http://localhost:11434\n",
            "  Model: qwen3:30b\n",
            "\n",
            "  Running inference test...\n",
            "  Prompt: 'What is the capital of France? Answer in one word.'\n",
            "  Response: 'Paris'\n",
            "‚úì Ollama (ChatOllama) connectivity test passed\n"
          ]
        }
      ],
      "source": [
        "from langchain_ollama import ChatOllama\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "def test_ollama_connectivity(base_url: str, model: str) -> bool:\n",
        "    \"\"\"\n",
        "    Test connectivity to Ollama server using LangChain's ChatOllama.\n",
        "    This uses the same connection method as the agent.\n",
        "    \n",
        "    Args:\n",
        "        base_url: Ollama server URL\n",
        "        model: Model name to test\n",
        "    \n",
        "    Returns:\n",
        "        bool: True if connection successful and model responds\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"Testing Ollama connection via LangChain ChatOllama...\")\n",
        "        print(f\"  Base URL: {base_url}\")\n",
        "        print(f\"  Model: {model}\")\n",
        "        \n",
        "        # Initialize ChatOllama - same as the agent uses\n",
        "        test_llm = ChatOllama(\n",
        "            model=model,\n",
        "            base_url=base_url,\n",
        "            temperature=0.1,\n",
        "        )\n",
        "        \n",
        "        # Run a simple inference test\n",
        "        print(f\"\\n  Running inference test...\")\n",
        "        test_prompt = \"What is the capital of France? Answer in one word.\"\n",
        "        \n",
        "        response = test_llm.invoke([HumanMessage(content=test_prompt)])\n",
        "        answer = response.content.strip()[:100]\n",
        "        \n",
        "        print(f\"  Prompt: '{test_prompt}'\")\n",
        "        print(f\"  Response: '{answer}'\")\n",
        "        print(f\"‚úì Ollama (ChatOllama) connectivity test passed\")\n",
        "        \n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚úó Ollama connection failed: {e}\")\n",
        "        print(f\"  Make sure Ollama is running: ollama serve\")\n",
        "        print(f\"  And the model is pulled: ollama pull {model}\")\n",
        "        return False\n",
        "\n",
        "# Run Ollama connectivity test\n",
        "ollama_connected = test_ollama_connectivity(OLLAMA_BASE_URL, OLLAMA_MODEL)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Gemini Connectivity Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Gemini API connected\n",
            "  Available models (sample): models/gemini-2.5-pro-preview-03-25, models/gemini-2.5-flash, models/gemini-2.5-pro-preview-05-06, models/gemini-2.5-pro-preview-06-05, models/gemini-2.5-pro...\n",
            "\n",
            "  Running inference test...\n",
            "  Prompt: 'What is the capital of France? Answer in one word.'\n",
            "  Response: 'Paris'\n",
            "‚úì Gemini inference test passed\n"
          ]
        }
      ],
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "def test_gemini_connectivity(api_key: str, model: str = \"gemini-2.5-pro\") -> bool:\n",
        "    \"\"\"\n",
        "    Test connectivity to Google Gemini API.\n",
        "    \n",
        "    Args:\n",
        "        api_key: Google API key for Gemini\n",
        "        model: Gemini model to use\n",
        "    \n",
        "    Returns:\n",
        "        bool: True if connection successful\n",
        "    \"\"\"\n",
        "    if not api_key:\n",
        "        print(\"‚úó Gemini API key not set\")\n",
        "        print(\"  Set GEMINI_API_KEY in your .env file\")\n",
        "        print(\"  Get your key at: https://aistudio.google.com/apikey\")\n",
        "        return False\n",
        "    \n",
        "    try:\n",
        "        # Configure the API\n",
        "        genai.configure(api_key=api_key)\n",
        "        \n",
        "        # List available models\n",
        "        available_models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]\n",
        "        print(f\"‚úì Gemini API connected\")\n",
        "        print(f\"  Available models (sample): {', '.join(available_models[:5])}...\")\n",
        "        \n",
        "        # Try to initialize the model\n",
        "        # Handle model name variations\n",
        "        model_name = model\n",
        "        if not model_name.startswith(\"models/\"):\n",
        "            model_name = f\"models/{model_name}\"\n",
        "        \n",
        "        # Check if exact model exists, otherwise find a compatible one\n",
        "        if model_name not in available_models:\n",
        "            # Try to find a matching model\n",
        "            matching = [m for m in available_models if model.split('-')[0] in m]\n",
        "            if matching:\n",
        "                model_name = matching[0]\n",
        "                print(f\"  Using model: {model_name}\")\n",
        "            else:\n",
        "                # Fallback to gemini-pro\n",
        "                model_name = \"models/gemini-2.0-flash\"\n",
        "                print(f\"  Falling back to: {model_name}\")\n",
        "        \n",
        "        gemini_model = genai.GenerativeModel(model_name)\n",
        "        \n",
        "        # Run a simple test\n",
        "        print(f\"\\n  Running inference test...\")\n",
        "        test_prompt = \"What is the capital of France? Answer in one word.\"\n",
        "        response = gemini_model.generate_content(test_prompt)\n",
        "        answer = response.text.strip()[:100]\n",
        "        print(f\"  Prompt: '{test_prompt}'\")\n",
        "        print(f\"  Response: '{answer}'\")\n",
        "        print(f\"‚úì Gemini inference test passed\")\n",
        "        \n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚úó Gemini connection failed: {e}\")\n",
        "        return False\n",
        "\n",
        "# Run Gemini connectivity test\n",
        "gemini_connected = test_gemini_connectivity(GEMINI_API_KEY, GEMINI_MODEL)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "CONNECTIVITY TEST SUMMARY\n",
            "============================================================\n",
            "  Ollama (qwen3:30b): ‚úì Connected\n",
            "  Gemini (gemini-2.5-pro): ‚úì Connected\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Summary of connectivity tests\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CONNECTIVITY TEST SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"  Ollama ({OLLAMA_MODEL}): {'‚úì Connected' if ollama_connected else '‚úó Failed'}\")\n",
        "print(f\"  Gemini ({GEMINI_MODEL}): {'‚úì Connected' if gemini_connected else '‚úó Failed'}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if not ollama_connected:\n",
        "    print(\"\\n‚ö† Ollama is required for the agent. Please fix the connection before proceeding.\")\n",
        "if not gemini_connected:\n",
        "    print(\"\\n‚ö† Gemini is required for evaluation. The agent will work but evaluation will fail.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load HotpotQA Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading HotpotQA dataset...\n",
            "‚úì Dataset loaded\n",
            "  Total samples: 7405\n",
            "  Features: ['id', 'question', 'answer', 'type', 'level', 'supporting_facts', 'context']\n",
            "  Sample size for evaluation: 10\n",
            "  Random seed: 42\n",
            "  Sampled indices: [5238, 912, 204, 6074, 2253]...\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import random\n",
        "\n",
        "# Load HotpotQA distractor dataset\n",
        "print(\"Loading HotpotQA dataset...\")\n",
        "dataset = load_dataset(\"hotpot_qa\", \"distractor\", split=\"validation\")\n",
        "\n",
        "print(f\"‚úì Dataset loaded\")\n",
        "print(f\"  Total samples: {len(dataset)}\")\n",
        "print(f\"  Features: {list(dataset.features.keys())}\")\n",
        "\n",
        "# Randomly sample a subset for testing (reproducible with seed)\n",
        "random.seed(RANDOM_SEED)\n",
        "sample_indices = random.sample(range(len(dataset)), min(SAMPLE_SIZE, len(dataset)))\n",
        "sample_dataset = dataset.select(sample_indices)\n",
        "print(f\"  Sample size for evaluation: {len(sample_dataset)}\")\n",
        "print(f\"  Random seed: {RANDOM_SEED}\")\n",
        "print(f\"  Sampled indices: {sample_indices[:5]}{'...' if len(sample_indices) > 5 else ''}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample Question:\n",
            "  ID: 5a8b57f25542995d1e6f1371\n",
            "  Question: Were Scott Derrickson and Ed Wood of the same nationality?\n",
            "  Answer: yes\n",
            "  Type: comparison\n",
            "  Level: hard\n",
            "\n",
            "Context (10 paragraphs - 2 gold, 8 distractors):\n",
            "  [1] Ed Wood (film): Ed Wood is a 1994 American biographical period comedy-drama film directed and produced by Tim Burton...\n",
            "  [2] Scott Derrickson: Scott Derrickson (born July 16, 1966) is an American director, screenwriter and producer.  He lives ...\n",
            "  [3] Woodson, Arkansas: Woodson is a census-designated place (CDP) in Pulaski County, Arkansas, in the United States.  Its p...\n",
            "  [4] Tyler Bates: Tyler Bates (born June 5, 1965) is an American musician, music producer, and composer for films, tel...\n",
            "  [5] Ed Wood: Edward Davis Wood Jr. (October 10, 1924 ‚Äì December 10, 1978) was an American filmmaker, actor, write...\n",
            "  [6] Deliver Us from Evil (2014 film): Deliver Us from Evil is a 2014 American supernatural horror film directed by Scott Derrickson and pr...\n",
            "  [7] Adam Collis: Adam Collis is an American filmmaker and actor.  He attended the Duke University from 1986 to 1990 a...\n",
            "  [8] Sinister (film): Sinister is a 2012 supernatural horror film directed by Scott Derrickson and written by Derrickson a...\n",
            "  [9] Conrad Brooks: Conrad Brooks (born Conrad Biedrzycki on January 3, 1931 in Baltimore, Maryland) is an American acto...\n",
            "  [10] Doctor Strange (2016 film): Doctor Strange is a 2016 American superhero film based on the Marvel Comics character of the same na...\n"
          ]
        }
      ],
      "source": [
        "# Examine a sample from the dataset\n",
        "sample = dataset[0]\n",
        "print(\"Sample Question:\")\n",
        "print(f\"  ID: {sample['id']}\")\n",
        "print(f\"  Question: {sample['question']}\")\n",
        "print(f\"  Answer: {sample['answer']}\")\n",
        "print(f\"  Type: {sample['type']}\")\n",
        "print(f\"  Level: {sample['level']}\")\n",
        "print(f\"\\nContext (10 paragraphs - 2 gold, 8 distractors):\")\n",
        "for i, (title, sentences) in enumerate(zip(sample['context']['title'], sample['context']['sentences'])):\n",
        "    preview = ' '.join(sentences)[:100] + \"...\"\n",
        "    print(f\"  [{i+1}] {title}: {preview}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Document Search Environment\n",
        "\n",
        "The agent is NOT given the context directly. Instead, it uses a `search_documents` tool that performs keyword search against the paragraphs hidden in the environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Environment:\n",
            "  Question: Were Scott Derrickson and Ed Wood of the same nationality?\n",
            "  Ground Truth: yes\n",
            "  Paragraphs: 10\n",
            "\n",
            "Test Search Query: 'Were Scott Derrickson and Ed'\n",
            "  [0.518] Scott Derrickson: Scott Derrickson (born July 16, 1966) is an American director, screenwriter and ...\n",
            "  [0.377] Adam Collis: Adam Collis is an American filmmaker and actor.  He attended the Duke University...\n",
            "  [0.370] Deliver Us from Evil (2014 film): Deliver Us from Evil is a 2014 American supernatural horror film directed by Sco...\n"
          ]
        }
      ],
      "source": [
        "from dataclasses import dataclass, field\n",
        "from difflib import SequenceMatcher\n",
        "import re\n",
        "\n",
        "@dataclass\n",
        "class HotpotQAEnvironment:\n",
        "    \"\"\"\n",
        "    Simulates a document retrieval environment for a HotpotQA question.\n",
        "    The agent must use the search_documents tool to retrieve relevant context.\n",
        "    \"\"\"\n",
        "    question: str\n",
        "    answer: str\n",
        "    paragraphs: List[dict] = field(default_factory=list)\n",
        "    \n",
        "    @classmethod\n",
        "    def from_sample(cls, sample: dict) -> 'HotpotQAEnvironment':\n",
        "        \"\"\"Create environment from a HotpotQA sample.\"\"\"\n",
        "        paragraphs = []\n",
        "        for title, sentences in zip(sample['context']['title'], sample['context']['sentences']):\n",
        "            paragraphs.append({\n",
        "                'title': title,\n",
        "                'content': ' '.join(sentences)\n",
        "            })\n",
        "        return cls(\n",
        "            question=sample['question'],\n",
        "            answer=sample['answer'],\n",
        "            paragraphs=paragraphs\n",
        "        )\n",
        "    \n",
        "    def search_documents(self, query: str, top_k: int = 3) -> List[dict]:\n",
        "        \"\"\"\n",
        "        Search for documents matching the query using keyword/fuzzy matching.\n",
        "        \n",
        "        Args:\n",
        "            query: Search query\n",
        "            top_k: Number of results to return\n",
        "        \n",
        "        Returns:\n",
        "            List of matching paragraphs with relevance scores\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        query_lower = query.lower()\n",
        "        query_words = set(re.findall(r'\\w+', query_lower))\n",
        "        \n",
        "        for para in self.paragraphs:\n",
        "            title_lower = para['title'].lower()\n",
        "            content_lower = para['content'].lower()\n",
        "            full_text = f\"{title_lower} {content_lower}\"\n",
        "            \n",
        "            # Calculate relevance score\n",
        "            # 1. Keyword overlap\n",
        "            text_words = set(re.findall(r'\\w+', full_text))\n",
        "            keyword_overlap = len(query_words & text_words) / max(len(query_words), 1)\n",
        "            \n",
        "            # 2. Sequence matching for title\n",
        "            title_similarity = SequenceMatcher(None, query_lower, title_lower).ratio()\n",
        "            \n",
        "            # 3. Check for exact phrase matches\n",
        "            phrase_bonus = 0.3 if query_lower in full_text else 0\n",
        "            \n",
        "            # Combined score\n",
        "            score = (keyword_overlap * 0.5) + (title_similarity * 0.3) + phrase_bonus\n",
        "            \n",
        "            if score > 0.1:  # Threshold\n",
        "                results.append({\n",
        "                    'title': para['title'],\n",
        "                    'content': para['content'],\n",
        "                    'relevance_score': round(score, 3)\n",
        "                })\n",
        "        \n",
        "        # Sort by relevance and return top_k\n",
        "        results.sort(key=lambda x: x['relevance_score'], reverse=True)\n",
        "        return results[:top_k]\n",
        "\n",
        "# Test the environment\n",
        "test_env = HotpotQAEnvironment.from_sample(dataset[0])\n",
        "print(f\"Test Environment:\")\n",
        "print(f\"  Question: {test_env.question}\")\n",
        "print(f\"  Ground Truth: {test_env.answer}\")\n",
        "print(f\"  Paragraphs: {len(test_env.paragraphs)}\")\n",
        "\n",
        "# Test search\n",
        "test_query = test_env.question.split()[:5]  # Use first 5 words\n",
        "test_query = ' '.join(test_query)\n",
        "print(f\"\\nTest Search Query: '{test_query}'\")\n",
        "results = test_env.search_documents(test_query)\n",
        "for r in results:\n",
        "    print(f\"  [{r['relevance_score']:.3f}] {r['title']}: {r['content'][:80]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Agent State & Graph Definition\n",
        "\n",
        "We define the agent's state schema and the LangGraph workflow implementing the Reflexive Metacognitive pattern.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì AgentState defined\n"
          ]
        }
      ],
      "source": [
        "from langgraph.graph import StateGraph, END, START\n",
        "from langchain_ollama import ChatOllama\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
        "import operator\n",
        "import json\n",
        "\n",
        "# Define the agent state\n",
        "class AgentState(TypedDict):\n",
        "    \"\"\"State schema for the Reflexive Metacognitive Agent.\"\"\"\n",
        "    messages: Annotated[List, operator.add]  # Chat history\n",
        "    question: str  # Original question\n",
        "    plan: List[str]  # Current list of search steps\n",
        "    context: List[dict]  # Retrieved snippets\n",
        "    reflection_count: int  # Counter to prevent infinite loops\n",
        "    complexity: str  # SIMPLE or COMPLEX\n",
        "    final_answer: Optional[str]  # The synthesized answer\n",
        "    environment: Optional[HotpotQAEnvironment]  # The search environment\n",
        "\n",
        "print(\"‚úì AgentState defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì LLM initialized: qwen3:30b\n"
          ]
        }
      ],
      "source": [
        "# Initialize the Ollama LLM\n",
        "llm = ChatOllama(\n",
        "    model=OLLAMA_MODEL,\n",
        "    base_url=OLLAMA_BASE_URL,\n",
        "    temperature=0.1,\n",
        ")\n",
        "\n",
        "print(f\"‚úì LLM initialized: {OLLAMA_MODEL}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Router node defined\n"
          ]
        }
      ],
      "source": [
        "import time as _time\n",
        "\n",
        "# Node 1: Metacognitive Router\n",
        "def router_node(state: AgentState) -> AgentState:\n",
        "    \"\"\"\n",
        "    Analyzes the question complexity and routes accordingly.\n",
        "    Output: SIMPLE or COMPLEX\n",
        "    \"\"\"\n",
        "    print(\"  ‚îú‚îÄ üîÄ [Router] Analyzing question complexity...\")\n",
        "    start = _time.time()\n",
        "    \n",
        "    question = state[\"question\"]\n",
        "    \n",
        "    router_prompt = f\"\"\"Analyze this question and determine its complexity.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Is this a SIMPLE lookup question (can be answered with a single fact) or a COMPLEX multi-step reasoning question (requires combining information from multiple sources)?\n",
        "\n",
        "Output ONLY one word: SIMPLE or COMPLEX\"\"\"\n",
        "    \n",
        "    llm_start = _time.time()\n",
        "    response = llm.invoke([HumanMessage(content=router_prompt)])\n",
        "    llm_time = _time.time() - llm_start\n",
        "    \n",
        "    complexity = \"COMPLEX\" if \"COMPLEX\" in response.content.upper() else \"SIMPLE\"\n",
        "    total_time = _time.time() - start\n",
        "    \n",
        "    print(f\"  ‚îÇ    ‚îî‚îÄ Classified as: {complexity} (LLM: {llm_time:.2f}s, total: {total_time:.2f}s)\")\n",
        "    \n",
        "    return {\n",
        "        **state,\n",
        "        \"complexity\": complexity,\n",
        "        \"messages\": [AIMessage(content=f\"[Router] Classified as: {complexity}\")]\n",
        "    }\n",
        "\n",
        "print(\"‚úì Router node defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Planner node defined\n"
          ]
        }
      ],
      "source": [
        "# Node 2: Planner\n",
        "def planner_node(state: AgentState) -> AgentState:\n",
        "    \"\"\"\n",
        "    Breaks down complex queries into search steps.\n",
        "    Generates specific search queries for the retrieval tool.\n",
        "    \"\"\"\n",
        "    reflection_count = state.get(\"reflection_count\", 0)\n",
        "    print(f\"  ‚îú‚îÄ üìã [Planner] Generating search queries (iteration {reflection_count + 1})...\")\n",
        "    start = _time.time()\n",
        "    \n",
        "    question = state[\"question\"]\n",
        "    current_context = state.get(\"context\", [])\n",
        "    \n",
        "    # Build context summary if we have prior retrievals\n",
        "    context_summary = \"\"\n",
        "    if current_context:\n",
        "        context_summary = \"\\n\\nInformation already retrieved:\\n\"\n",
        "        for ctx in current_context:\n",
        "            context_summary += f\"- {ctx['title']}: {ctx['content'][:100]}...\\n\"\n",
        "        print(f\"  ‚îÇ    ‚îú‚îÄ Prior context: {len(current_context)} documents\")\n",
        "    \n",
        "    planner_prompt = f\"\"\"You are a research planner. Your task is to create search queries to find information needed to answer a question.\n",
        "\n",
        "Question: {question}\n",
        "{context_summary}\n",
        "\n",
        "Generate 1-3 specific search queries that would help find the missing information. Each query should focus on a specific entity or fact mentioned in the question.\n",
        "\n",
        "Output as a JSON array of strings, e.g.: [\"query 1\", \"query 2\"]\n",
        "\n",
        "Search queries:\"\"\"\n",
        "    \n",
        "    llm_start = _time.time()\n",
        "    response = llm.invoke([HumanMessage(content=planner_prompt)])\n",
        "    llm_time = _time.time() - llm_start\n",
        "    \n",
        "    # Parse the response to extract queries\n",
        "    try:\n",
        "        # Try to extract JSON array\n",
        "        content = response.content\n",
        "        # Find JSON array in response\n",
        "        start_idx = content.find('[')\n",
        "        end_idx = content.rfind(']') + 1\n",
        "        if start_idx != -1 and end_idx > start_idx:\n",
        "            queries = json.loads(content[start_idx:end_idx])\n",
        "        else:\n",
        "            # Fallback: split by newlines\n",
        "            queries = [q.strip('- ') for q in content.split('\\n') if q.strip()]\n",
        "    except:\n",
        "        # Ultimate fallback: use the question itself\n",
        "        queries = [question]\n",
        "    \n",
        "    total_time = _time.time() - start\n",
        "    print(f\"  ‚îÇ    ‚îî‚îÄ Generated {len(queries[:3])} queries (LLM: {llm_time:.2f}s, total: {total_time:.2f}s)\")\n",
        "    for i, q in enumerate(queries[:3]):\n",
        "        print(f\"  ‚îÇ       [{i+1}] {q[:60]}{'...' if len(q) > 60 else ''}\")\n",
        "    \n",
        "    return {\n",
        "        **state,\n",
        "        \"plan\": queries[:3],  # Limit to 3 queries\n",
        "        \"messages\": [AIMessage(content=f\"[Planner] Generated {len(queries)} search queries: {queries}\")]\n",
        "    }\n",
        "\n",
        "print(\"‚úì Planner node defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Executor node defined\n"
          ]
        }
      ],
      "source": [
        "# Node 3: Executor (Tool Node)\n",
        "def executor_node(state: AgentState) -> AgentState:\n",
        "    \"\"\"\n",
        "    Executes the search plan by querying the document environment.\n",
        "    Simulates retrieval from the HotpotQA context.\n",
        "    \"\"\"\n",
        "    print(\"  ‚îú‚îÄ üîç [Executor] Executing search queries...\")\n",
        "    start = _time.time()\n",
        "    \n",
        "    plan = state.get(\"plan\", [])\n",
        "    environment = state.get(\"environment\")\n",
        "    current_context = state.get(\"context\", [])\n",
        "    \n",
        "    if not environment:\n",
        "        print(\"  ‚îÇ    ‚îî‚îÄ ‚ùå Error: No environment set\")\n",
        "        return {\n",
        "            **state,\n",
        "            \"messages\": [AIMessage(content=\"[Executor] Error: No environment set\")]\n",
        "        }\n",
        "    \n",
        "    # Execute each query in the plan\n",
        "    new_context = list(current_context)\n",
        "    retrieved_titles = {ctx['title'] for ctx in new_context}\n",
        "    new_docs_count = 0\n",
        "    \n",
        "    for i, query in enumerate(plan):\n",
        "        query_start = _time.time()\n",
        "        results = environment.search_documents(query, top_k=2)\n",
        "        query_time = _time.time() - query_start\n",
        "        added = 0\n",
        "        for result in results:\n",
        "            # Avoid duplicates\n",
        "            if result['title'] not in retrieved_titles:\n",
        "                new_context.append(result)\n",
        "                retrieved_titles.add(result['title'])\n",
        "                added += 1\n",
        "                new_docs_count += 1\n",
        "        print(f\"  ‚îÇ    ‚îú‚îÄ Query {i+1}: found {len(results)} docs, added {added} new ({query_time*1000:.1f}ms)\")\n",
        "    \n",
        "    total_time = _time.time() - start\n",
        "    print(f\"  ‚îÇ    ‚îî‚îÄ Total: {len(new_context)} docs in context (+{new_docs_count} new) ({total_time:.2f}s)\")\n",
        "    \n",
        "    return {\n",
        "        **state,\n",
        "        \"context\": new_context,\n",
        "        \"messages\": [AIMessage(content=f\"[Executor] Retrieved {len(new_context)} documents\")]\n",
        "    }\n",
        "\n",
        "print(\"‚úì Executor node defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Reflector node defined\n"
          ]
        }
      ],
      "source": [
        "# Node 4: Reflector (Critic)\n",
        "def reflector_node(state: AgentState) -> AgentState:\n",
        "    \"\"\"\n",
        "    Validates retrieved information against the user query.\n",
        "    Decides whether to continue searching or generate final answer.\n",
        "    \"\"\"\n",
        "    reflection_count = state.get(\"reflection_count\", 0)\n",
        "    print(f\"  ‚îú‚îÄ ü§î [Reflector] Evaluating information sufficiency (reflection #{reflection_count + 1}/{MAX_REFLECTIONS})...\")\n",
        "    start = _time.time()\n",
        "    \n",
        "    question = state[\"question\"]\n",
        "    context = state.get(\"context\", [])\n",
        "    \n",
        "    # Build context text\n",
        "    context_text = \"\\n\\n\".join([\n",
        "        f\"**{ctx['title']}**: {ctx['content']}\"\n",
        "        for ctx in context\n",
        "    ])\n",
        "    \n",
        "    reflector_prompt = f\"\"\"You are a critical reviewer. Analyze whether the retrieved information is sufficient to answer the question.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Retrieved Information:\n",
        "{context_text}\n",
        "\n",
        "Based ONLY on the retrieved information:\n",
        "1. Can you confidently answer the question?\n",
        "2. Is there any critical information missing?\n",
        "\n",
        "Output your assessment in this format:\n",
        "SUFFICIENT: YES or NO\n",
        "MISSING: [what information is still needed, if any]\n",
        "REASONING: [brief explanation]\"\"\"\n",
        "    \n",
        "    llm_start = _time.time()\n",
        "    response = llm.invoke([HumanMessage(content=reflector_prompt)])\n",
        "    llm_time = _time.time() - llm_start\n",
        "    content = response.content.upper()\n",
        "    \n",
        "    # Determine if we have sufficient information\n",
        "    is_sufficient = \"SUFFICIENT: YES\" in content or \"SUFFICIENT:YES\" in content\n",
        "    \n",
        "    # Increment reflection count\n",
        "    new_reflection_count = reflection_count + 1\n",
        "    \n",
        "    # Force completion if max reflections reached\n",
        "    forced = False\n",
        "    if new_reflection_count >= MAX_REFLECTIONS and not is_sufficient:\n",
        "        is_sufficient = True\n",
        "        forced = True\n",
        "    \n",
        "    total_time = _time.time() - start\n",
        "    status = \"‚úÖ Sufficient\" if is_sufficient else \"üîÑ Need more info\"\n",
        "    if forced:\n",
        "        status = \"‚ö†Ô∏è Max reflections reached, forcing completion\"\n",
        "    print(f\"  ‚îÇ    ‚îî‚îÄ {status} (LLM: {llm_time:.2f}s, total: {total_time:.2f}s)\")\n",
        "    \n",
        "    return {\n",
        "        **state,\n",
        "        \"reflection_count\": new_reflection_count,\n",
        "        \"complexity\": \"DONE\" if is_sufficient else \"CONTINUE\",\n",
        "        \"messages\": [AIMessage(content=f\"[Reflector] Reflection #{new_reflection_count}: {'Sufficient' if is_sufficient else 'Need more info'}\")]\n",
        "    }\n",
        "\n",
        "print(\"‚úì Reflector node defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Direct Answer node defined\n"
          ]
        }
      ],
      "source": [
        "# Node 5: Direct Answer (for SIMPLE queries)\n",
        "def direct_answer_node(state: AgentState) -> AgentState:\n",
        "    \"\"\"\n",
        "    Generates a quick answer for simple queries with minimal retrieval.\n",
        "    \"\"\"\n",
        "    print(\"  ‚îú‚îÄ ‚ö° [DirectAnswer] Generating quick answer for simple query...\")\n",
        "    start = _time.time()\n",
        "    \n",
        "    question = state[\"question\"]\n",
        "    environment = state.get(\"environment\")\n",
        "    \n",
        "    # Quick retrieval\n",
        "    if environment:\n",
        "        search_start = _time.time()\n",
        "        results = environment.search_documents(question, top_k=3)\n",
        "        search_time = _time.time() - search_start\n",
        "        context_text = \"\\n\".join([f\"- {r['title']}: {r['content'][:200]}\" for r in results])\n",
        "        print(f\"  ‚îÇ    ‚îú‚îÄ Quick search: {len(results)} docs ({search_time*1000:.1f}ms)\")\n",
        "    else:\n",
        "        context_text = \"No context available.\"\n",
        "        results = []\n",
        "        print(\"  ‚îÇ    ‚îú‚îÄ No environment, proceeding without context\")\n",
        "    \n",
        "    answer_prompt = f\"\"\"Answer the following question based on the provided context. Be concise and direct.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Context:\n",
        "{context_text}\n",
        "\n",
        "Answer:\"\"\"\n",
        "    \n",
        "    llm_start = _time.time()\n",
        "    response = llm.invoke([HumanMessage(content=answer_prompt)])\n",
        "    llm_time = _time.time() - llm_start\n",
        "    \n",
        "    total_time = _time.time() - start\n",
        "    print(f\"  ‚îÇ    ‚îî‚îÄ Answer generated (LLM: {llm_time:.2f}s, total: {total_time:.2f}s)\")\n",
        "    \n",
        "    return {\n",
        "        **state,\n",
        "        \"final_answer\": response.content.strip(),\n",
        "        \"context\": results if environment else [],\n",
        "        \"messages\": [AIMessage(content=f\"[DirectAnswer] Generated answer\")]\n",
        "    }\n",
        "\n",
        "print(\"‚úì Direct Answer node defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Final Answer node defined\n"
          ]
        }
      ],
      "source": [
        "# Node 6: Final Answer Synthesis\n",
        "def final_answer_node(state: AgentState) -> AgentState:\n",
        "    \"\"\"\n",
        "    Synthesizes the final answer from all retrieved context.\n",
        "    \"\"\"\n",
        "    print(\"  ‚îú‚îÄ ‚ú® [FinalAnswer] Synthesizing final answer...\")\n",
        "    start = _time.time()\n",
        "    \n",
        "    question = state[\"question\"]\n",
        "    context = state.get(\"context\", [])\n",
        "    \n",
        "    print(f\"  ‚îÇ    ‚îú‚îÄ Using {len(context)} context documents\")\n",
        "    \n",
        "    # Build comprehensive context\n",
        "    context_text = \"\\n\\n\".join([\n",
        "        f\"**{ctx['title']}**:\\n{ctx['content']}\"\n",
        "        for ctx in context\n",
        "    ])\n",
        "    \n",
        "    synthesis_prompt = f\"\"\"You are an expert at synthesizing information to answer complex questions.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Retrieved Information:\n",
        "{context_text}\n",
        "\n",
        "Based on the information above, provide a clear and accurate answer to the question. \n",
        "Be specific and cite relevant facts from the sources.\n",
        "If the information is insufficient, say so honestly.\n",
        "\n",
        "Answer:\"\"\"\n",
        "    \n",
        "    llm_start = _time.time()\n",
        "    response = llm.invoke([HumanMessage(content=synthesis_prompt)])\n",
        "    llm_time = _time.time() - llm_start\n",
        "    \n",
        "    total_time = _time.time() - start\n",
        "    print(f\"  ‚îÇ    ‚îî‚îÄ Final answer synthesized (LLM: {llm_time:.2f}s, total: {total_time:.2f}s)\")\n",
        "    \n",
        "    return {\n",
        "        **state,\n",
        "        \"final_answer\": response.content.strip(),\n",
        "        \"messages\": [AIMessage(content=f\"[FinalAnswer] Synthesized final response\")]\n",
        "    }\n",
        "\n",
        "print(\"‚úì Final Answer node defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Routing functions defined\n"
          ]
        }
      ],
      "source": [
        "# Define routing functions\n",
        "def route_by_complexity(state: AgentState) -> Literal[\"planner\", \"direct_answer\"]:\n",
        "    \"\"\"Route based on question complexity.\"\"\"\n",
        "    if state.get(\"complexity\") == \"SIMPLE\":\n",
        "        return \"direct_answer\"\n",
        "    return \"planner\"\n",
        "\n",
        "def route_after_reflection(state: AgentState) -> Literal[\"planner\", \"final_answer\"]:\n",
        "    \"\"\"Route based on reflection result.\"\"\"\n",
        "    if state.get(\"complexity\") == \"DONE\":\n",
        "        return \"final_answer\"\n",
        "    return \"planner\"\n",
        "\n",
        "print(\"‚úì Routing functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Agent graph compiled\n"
          ]
        }
      ],
      "source": [
        "# Build the LangGraph workflow\n",
        "def build_agent_graph() -> StateGraph:\n",
        "    \"\"\"\n",
        "    Constructs the Reflexive Metacognitive Agent graph.\n",
        "    \n",
        "    Flow:\n",
        "    START -> Router -> [SIMPLE: DirectAnswer | COMPLEX: Planner -> Executor -> Reflector]\n",
        "                                                                    |-> loop back to Planner\n",
        "                                                                    |-> FinalAnswer -> END\n",
        "    \"\"\"\n",
        "    # Create the graph\n",
        "    workflow = StateGraph(AgentState)\n",
        "    \n",
        "    # Add nodes\n",
        "    workflow.add_node(\"router\", router_node)\n",
        "    workflow.add_node(\"planner\", planner_node)\n",
        "    workflow.add_node(\"executor\", executor_node)\n",
        "    workflow.add_node(\"reflector\", reflector_node)\n",
        "    workflow.add_node(\"direct_answer\", direct_answer_node)\n",
        "    workflow.add_node(\"final_answer\", final_answer_node)\n",
        "    \n",
        "    # Add edges\n",
        "    workflow.add_edge(START, \"router\")\n",
        "    workflow.add_conditional_edges(\n",
        "        \"router\",\n",
        "        route_by_complexity,\n",
        "        {\n",
        "            \"planner\": \"planner\",\n",
        "            \"direct_answer\": \"direct_answer\"\n",
        "        }\n",
        "    )\n",
        "    workflow.add_edge(\"planner\", \"executor\")\n",
        "    workflow.add_edge(\"executor\", \"reflector\")\n",
        "    workflow.add_conditional_edges(\n",
        "        \"reflector\",\n",
        "        route_after_reflection,\n",
        "        {\n",
        "            \"planner\": \"planner\",\n",
        "            \"final_answer\": \"final_answer\"\n",
        "        }\n",
        "    )\n",
        "    workflow.add_edge(\"direct_answer\", END)\n",
        "    workflow.add_edge(\"final_answer\", END)\n",
        "    \n",
        "    return workflow.compile()\n",
        "\n",
        "# Build the agent\n",
        "agent = build_agent_graph()\n",
        "print(\"‚úì Agent graph compiled\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Test the Agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: Were Scott Derrickson and Ed Wood of the same nationality?\n",
            "Ground Truth: yes\n",
            "\n",
            "============================================================\n",
            "RUNNING AGENT\n",
            "============================================================\n",
            "  ‚îå‚îÄ üöÄ Agent workflow started\n",
            "  ‚îÇ  Question: Were Scott Derrickson and Ed Wood of the same nationality?\n",
            "  ‚îú‚îÄ üîÄ [Router] Analyzing question complexity...\n",
            "  ‚îÇ    ‚îî‚îÄ Classified as: COMPLEX (LLM: 20.44s, total: 20.44s)\n",
            "  ‚îú‚îÄ üìã [Planner] Generating search queries (iteration 1)...\n",
            "  ‚îÇ    ‚îî‚îÄ Generated 2 queries (LLM: 6.86s, total: 6.86s)\n",
            "  ‚îÇ       [1] Scott Derrickson nationality\n",
            "  ‚îÇ       [2] Ed Wood nationality\n",
            "  ‚îú‚îÄ üîç [Executor] Executing search queries...\n",
            "  ‚îÇ    ‚îú‚îÄ Query 1: found 2 docs, added 2 new (0.6ms)\n",
            "  ‚îÇ    ‚îú‚îÄ Query 2: found 2 docs, added 2 new (0.4ms)\n",
            "  ‚îÇ    ‚îî‚îÄ Total: 4 docs in context (+4 new) (0.00s)\n",
            "  ‚îú‚îÄ ü§î [Reflector] Evaluating information sufficiency (reflection #1/3)...\n",
            "  ‚îÇ    ‚îî‚îÄ ‚úÖ Sufficient (LLM: 12.86s, total: 12.86s)\n",
            "  ‚îú‚îÄ ‚ú® [FinalAnswer] Synthesizing final answer...\n",
            "  ‚îÇ    ‚îú‚îÄ Using 4 context documents\n",
            "  ‚îÇ    ‚îî‚îÄ Final answer synthesized (LLM: 6.96s, total: 6.96s)\n",
            "  ‚îî‚îÄ ‚úÖ Agent workflow completed in 47.12s\n",
            "\n",
            "============================================================\n",
            "AGENT TRACE (message log)\n",
            "============================================================\n",
            "  [Router] Classified as: COMPLEX\n",
            "  [Planner] Generated 2 search queries: ['Scott Derrickson nationality', 'Ed Wood nationality']\n",
            "  [Executor] Retrieved 4 documents\n",
            "  [Reflector] Reflection #1: Sufficient\n",
            "  [FinalAnswer] Synthesized final response\n",
            "\n",
            "============================================================\n",
            "RESULTS\n",
            "============================================================\n",
            "Complexity: DONE\n",
            "Reflections: 1\n",
            "Context Retrieved: 4 documents\n",
            "\n",
            "Agent Answer:\n",
            "Based solely on the retrieved information provided:\n",
            "\n",
            "- **Scott Derrickson** is explicitly identified as an \"American director, screenwriter and producer\" (born July 16, 1966, living in Los Angeles, California).\n",
            "- **Ed Wood** (Edward Davis Wood Jr.) is explicitly identified as an \"American filmmaker, actor, writer, producer, and director\" (born October 10, 1924, died December 10, 1978).\n",
            "\n",
            "Both individuals are consistently described as American in the retrieved sources. There is no conflicting information about their nationalities. Therefore, **Scott Derrickson and Ed Wood were of the same nationality: American**.\n",
            "\n",
            "**Relevant facts cited**:  \n",
            "1. \"Scott Derrickson [...] is an American director, screenwriter and producer.\"  \n",
            "2. \"Edward Davis Wood Jr. [...] was an American filmmaker, actor, writer, producer, and director.\"\n",
            "\n",
            "Ground Truth: yes\n"
          ]
        }
      ],
      "source": [
        "def run_agent(question: str, environment: HotpotQAEnvironment, verbose: bool = True) -> dict:\n",
        "    \"\"\"\n",
        "    Run the agent on a single question.\n",
        "    \n",
        "    Args:\n",
        "        question: The question to answer\n",
        "        environment: The HotpotQA environment with context\n",
        "        verbose: Whether to print detailed progress (default True)\n",
        "    \n",
        "    Returns:\n",
        "        Final state with answer\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(\"  ‚îå‚îÄ üöÄ Agent workflow started\")\n",
        "        print(f\"  ‚îÇ  Question: {question[:70]}{'...' if len(question) > 70 else ''}\")\n",
        "    \n",
        "    start_time = _time.time()\n",
        "    \n",
        "    initial_state = {\n",
        "        \"messages\": [],\n",
        "        \"question\": question,\n",
        "        \"plan\": [],\n",
        "        \"context\": [],\n",
        "        \"reflection_count\": 0,\n",
        "        \"complexity\": \"\",\n",
        "        \"final_answer\": None,\n",
        "        \"environment\": environment\n",
        "    }\n",
        "    \n",
        "    result = agent.invoke(initial_state)\n",
        "    \n",
        "    total_time = _time.time() - start_time\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"  ‚îî‚îÄ ‚úÖ Agent workflow completed in {total_time:.2f}s\")\n",
        "    \n",
        "    return result\n",
        "\n",
        "# Test with first sample\n",
        "test_sample = dataset[0]\n",
        "test_env = HotpotQAEnvironment.from_sample(test_sample)\n",
        "\n",
        "print(f\"Question: {test_sample['question']}\")\n",
        "print(f\"Ground Truth: {test_sample['answer']}\")\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"RUNNING AGENT\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "result = run_agent(test_sample['question'], test_env)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"AGENT TRACE (message log)\")\n",
        "print(f\"{'='*60}\")\n",
        "for msg in result['messages']:\n",
        "    print(f\"  {msg.content}\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"RESULTS\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Complexity: {result['complexity']}\")\n",
        "print(f\"Reflections: {result['reflection_count']}\")\n",
        "print(f\"Context Retrieved: {len(result['context'])} documents\")\n",
        "print(f\"\\nAgent Answer:\")\n",
        "print(result['final_answer'])\n",
        "print(f\"\\nGround Truth: {test_sample['answer']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. MLflow Tracing & Evaluation Setup\n",
        "\n",
        "### Prerequisites: Start the MLflow Tracking Server\n",
        "\n",
        "Before running the benchmark, start the MLflow UI server in a separate terminal to view traces and experiments:\n",
        "\n",
        "```bash\n",
        "# Option 1: Start MLflow UI with default settings (uses ./mlruns directory)\n",
        "mlflow ui\n",
        "\n",
        "# Option 2: Specify a custom port\n",
        "mlflow ui --port 5000\n",
        "\n",
        "# Option 3: Start with a specific backend store\n",
        "mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./mlruns\n",
        "```\n",
        "\n",
        "The MLflow UI will be available at **http://localhost:5000** (or your specified port).\n",
        "\n",
        "### What Gets Traced?\n",
        "\n",
        "With `mlflow.langchain.autolog()` enabled, the following will be automatically captured:\n",
        "- **LLM calls**: Every invocation of `ChatOllama` (router, planner, reflector, etc.)\n",
        "- **Input/Output**: Prompts sent and responses received\n",
        "- **Latency**: Time taken for each LLM call\n",
        "- **Token counts**: Input and output token estimates\n",
        "- **Chain execution**: The full LangGraph workflow execution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/11/29 21:30:47 INFO mlflow.tracking.fluent: Experiment with name 'hotpotqa-qwen3-benchmark' does not exist. Creating a new experiment.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì LangChain autologging enabled\n",
            "\n",
            "‚úì MLflow configured\n",
            "  Tracking URI: file:./mlruns\n",
            "  Experiment: hotpotqa-qwen3-benchmark\n"
          ]
        }
      ],
      "source": [
        "import mlflow\n",
        "\n",
        "# Configure MLflow\n",
        "MLFLOW_TRACKING_URI = os.getenv(\"MLFLOW_TRACKING_URI\", \"file:./mlruns\")\n",
        "EXPERIMENT_NAME = \"hotpotqa-qwen3-benchmark\"\n",
        "\n",
        "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
        "mlflow.set_experiment(EXPERIMENT_NAME)\n",
        "\n",
        "# Enable LangChain autologging for tracing (requires 'langchain' package)\n",
        "try:\n",
        "    mlflow.langchain.autolog()\n",
        "    print(\"‚úì LangChain autologging enabled\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö† LangChain autologging not available: {e}\")\n",
        "    print(\"  MLflow will still track runs and metrics, but without automatic LLM tracing.\")\n",
        "\n",
        "print(f\"\\n‚úì MLflow configured\")\n",
        "print(f\"  Tracking URI: {MLFLOW_TRACKING_URI}\")\n",
        "print(f\"  Experiment: {EXPERIMENT_NAME}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Accessing Traces in the MLflow UI\n",
        "\n",
        "After running experiments, open **http://localhost:5000** in your browser to access the MLflow UI:\n",
        "\n",
        "1. **Select Experiment**: Click on `hotpotqa-qwen3-benchmark` in the left sidebar\n",
        "2. **View Runs**: Each benchmark execution creates a new run with logged metrics\n",
        "3. **Explore Traces**: Click on a run, then navigate to the **\"Traces\"** tab to see:\n",
        "   - Full execution timeline of the agent\n",
        "   - Individual LLM calls with prompts and responses\n",
        "   - Latency breakdown per node (Router ‚Üí Planner ‚Üí Executor ‚Üí Reflector)\n",
        "4. **Compare Runs**: Select multiple runs to compare metrics across different configurations\n",
        "\n",
        "**Key Metrics to Monitor:**\n",
        "| Metric | Description |\n",
        "|--------|-------------|\n",
        "| `avg_score` | Average evaluation score (1-5) from Gemini judge |\n",
        "| `avg_latency` | Average time per question |\n",
        "| `success_rate` | Percentage of successfully answered questions |\n",
        "| `avg_reflections` | Average number of retrieval-reflection cycles |\n",
        "\n",
        "\n",
        "**Tip**: If using `file:./mlruns` (default), run `mlflow ui` from the notebook's directory to see the experiments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì MLflow LLM Judge environment configured\n",
            "  Using model: gemini/gemini-2.5-pro\n",
            "  API: MLflow make_judge (template-based LLM Scorer)\n"
          ]
        }
      ],
      "source": [
        "# Configure environment for MLflow's make_judge API\n",
        "# The make_judge API requires the GEMINI_API_KEY to be set in the environment\n",
        "# Reference: https://mlflow.org/docs/latest/genai/eval-monitor/scorers/llm-judge/make-judge/\n",
        "\n",
        "import os\n",
        "\n",
        "def verify_judge_configuration():\n",
        "    \"\"\"\n",
        "    Verifies that the environment is configured for MLflow's template-based LLM Scorer.\n",
        "    MLflow's make_judge uses the model URI format (e.g., 'gemini/gemini-2.5-pro')\n",
        "    and reads credentials from environment variables.\n",
        "    \"\"\"\n",
        "    if not GEMINI_API_KEY:\n",
        "        print(\"‚ö† GEMINI_API_KEY not set. Evaluation will use fallback scoring.\")\n",
        "        return False\n",
        "    \n",
        "    # Ensure GEMINI_API_KEY is available to MLflow's judge\n",
        "    os.environ[\"GEMINI_API_KEY\"] = GEMINI_API_KEY\n",
        "    \n",
        "    print(f\"‚úì MLflow LLM Judge environment configured\")\n",
        "    print(f\"  Using model: gemini/{GEMINI_MODEL}\")\n",
        "    print(f\"  API: MLflow make_judge (template-based LLM Scorer)\")\n",
        "    return True\n",
        "\n",
        "gemini_judge_ready = verify_judge_configuration()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì MLflow template-based LLM Scorer (make_judge) configured\n",
            "  Model: gemini/gemini-2.5-pro\n",
            "  Feedback categories: perfect, correct, mostly_correct, partial, wrong\n"
          ]
        }
      ],
      "source": [
        "from mlflow.genai.judges import make_judge\n",
        "from typing import Literal\n",
        "\n",
        "# Create template-based LLM Scorer using MLflow's make_judge API\n",
        "# Reference: https://mlflow.org/docs/latest/genai/eval-monitor/scorers/llm-judge/make-judge/\n",
        "\n",
        "# Define the answer quality judge using MLflow's template-based approach\n",
        "answer_quality_judge = make_judge(\n",
        "    name=\"answer_quality\",\n",
        "    instructions=(\n",
        "        \"You are an expert judge evaluating question-answering quality.\\n\\n\"\n",
        "        \"Question: {{ inputs }}\\n\"\n",
        "        \"Ground Truth Answer: {{ expectations }}\\n\"\n",
        "        \"Agent's Answer: {{ outputs }}\\n\\n\"\n",
        "        \"Rate the agent's answer based on how well it matches the ground truth.\\n\"\n",
        "        \"Consider correctness, completeness, and relevance.\\n\"\n",
        "        \"- 'perfect': Matches or exceeds ground truth\\n\"\n",
        "        \"- 'correct': Correct with minor issues\\n\"\n",
        "        \"- 'mostly_correct': Mostly correct but missing key details\\n\"\n",
        "        \"- 'partial': Partially correct but with significant errors\\n\"\n",
        "        \"- 'wrong': Completely wrong or unrelated\\n\"\n",
        "    ),\n",
        "    feedback_value_type=Literal[\"perfect\", \"correct\", \"mostly_correct\", \"partial\", \"wrong\"],\n",
        "    model=f\"gemini:/{GEMINI_MODEL}\",\n",
        ")\n",
        "\n",
        "# Score mapping for metrics\n",
        "SCORE_MAP = {\n",
        "    \"perfect\": 5,\n",
        "    \"correct\": 4,\n",
        "    \"mostly_correct\": 3,\n",
        "    \"partial\": 2,\n",
        "    \"wrong\": 1,\n",
        "}\n",
        "\n",
        "def evaluate_answer(agent_answer: str, ground_truth: str, question: str, judge_model=None) -> dict:\n",
        "    \"\"\"\n",
        "    Evaluate agent answer against ground truth using MLflow's template-based LLM judge.\n",
        "    \n",
        "    Args:\n",
        "        agent_answer: The agent's generated answer\n",
        "        ground_truth: The correct answer\n",
        "        question: The original question\n",
        "        judge_model: Legacy parameter (kept for compatibility, now uses make_judge)\n",
        "    \n",
        "    Returns:\n",
        "        dict with score (1-5) and explanation\n",
        "    \"\"\"\n",
        "    if not GEMINI_API_KEY:\n",
        "        # Fallback: simple string matching\n",
        "        gt_lower = ground_truth.lower().strip()\n",
        "        ans_lower = agent_answer.lower().strip()\n",
        "        if gt_lower in ans_lower or ans_lower in gt_lower:\n",
        "            return {\"score\": 4, \"explanation\": \"Fallback: Answer contains ground truth\"}\n",
        "        return {\"score\": 2, \"explanation\": \"Fallback: Unable to verify without judge\"}\n",
        "    \n",
        "    try:\n",
        "        # Use MLflow's template-based judge\n",
        "        feedback = answer_quality_judge(\n",
        "            inputs={\"question\": question},\n",
        "            outputs={\"answer\": agent_answer},\n",
        "            expectations={\"ground_truth\": ground_truth},\n",
        "        )\n",
        "        \n",
        "        # Convert categorical feedback to numeric score\n",
        "        score = SCORE_MAP.get(feedback.value, 3)\n",
        "        explanation = feedback.rationale or \"No rationale provided\"\n",
        "        \n",
        "        return {\"score\": score, \"explanation\": explanation}\n",
        "    except Exception as e:\n",
        "        return {\"score\": 0, \"explanation\": f\"Evaluation error: {str(e)}\"}\n",
        "\n",
        "print(\"‚úì MLflow template-based LLM Scorer (make_judge) configured\")\n",
        "print(f\"  Model: gemini/{GEMINI_MODEL}\")\n",
        "print(f\"  Feedback categories: perfect, correct, mostly_correct, partial, wrong\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Run Benchmark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Benchmark function defined (with per-trace assessments)\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "import time\n",
        "import pandas as pd\n",
        "from mlflow.entities import Feedback, Expectation, AssessmentSource\n",
        "\n",
        "def run_benchmark(dataset, sample_size: int = 10, verbose: bool = True, random_seed: int = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Run the full benchmark on HotpotQA samples.\n",
        "    \n",
        "    Uses MLflow's template-based LLM Scorer (make_judge) for evaluation.\n",
        "    Logs per-trace assessments (Feedback) for each request so metrics appear at trace level in MLflow UI.\n",
        "    \n",
        "    Reference: https://mlflow.org/docs/latest/genai/eval-monitor/scorers/llm-judge/make-judge/\n",
        "    \n",
        "    Args:\n",
        "        dataset: HotpotQA dataset\n",
        "        sample_size: Number of samples to evaluate\n",
        "        verbose: Whether to print detailed agent progress (default True)\n",
        "        random_seed: Seed for random sampling (default uses RANDOM_SEED from config)\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with results\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    # Use random sampling to select cases from the dataset\n",
        "    seed = random_seed if random_seed is not None else RANDOM_SEED\n",
        "    random.seed(seed)\n",
        "    total_available = len(dataset)\n",
        "    actual_sample_size = min(sample_size, total_available)\n",
        "    sampled_indices = random.sample(range(total_available), actual_sample_size)\n",
        "    \n",
        "    with mlflow.start_run(run_name=f\"benchmark_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"):\n",
        "        # Log parameters\n",
        "        mlflow.log_params({\n",
        "            \"model\": OLLAMA_MODEL,\n",
        "            \"sample_size\": actual_sample_size,\n",
        "            \"max_reflections\": MAX_REFLECTIONS,\n",
        "            \"judge_model\": f\"gemini/{GEMINI_MODEL}\",\n",
        "            \"judge_type\": \"mlflow_make_judge\",\n",
        "            \"random_seed\": seed,\n",
        "            \"sampling_method\": \"random\"\n",
        "        })\n",
        "        \n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"üèÉ BENCHMARK STARTED: {actual_sample_size} samples (randomly selected)\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"Model: {OLLAMA_MODEL} | Max Reflections: {MAX_REFLECTIONS}\")\n",
        "        print(f\"Judge: MLflow make_judge (gemini/{GEMINI_MODEL})\")\n",
        "        print(f\"Random Seed: {seed} | Dataset Size: {total_available}\")\n",
        "        print(f\"{'='*70}\")\n",
        "        \n",
        "        benchmark_start = time.time()\n",
        "        \n",
        "        for i, dataset_idx in enumerate(sampled_indices):\n",
        "            sample = dataset[dataset_idx]\n",
        "            env = HotpotQAEnvironment.from_sample(sample)\n",
        "            \n",
        "            print(f\"\\n{'‚îÄ'*70}\")\n",
        "            print(f\"üìù Question [{i+1}/{actual_sample_size}] (idx={dataset_idx}): {sample['question']}\")\n",
        "            print(f\"   Ground Truth: {sample['answer']}\")\n",
        "            print(f\"{'‚îÄ'*70}\")\n",
        "            \n",
        "            # Run agent\n",
        "            start_time = time.time()\n",
        "            try:\n",
        "                result = run_agent(sample['question'], env, verbose=verbose)\n",
        "                latency = time.time() - start_time\n",
        "                agent_answer = result.get('final_answer', '')\n",
        "                success = True\n",
        "            except Exception as e:\n",
        "                latency = time.time() - start_time\n",
        "                agent_answer = f\"Error: {str(e)}\"\n",
        "                result = {\"complexity\": \"ERROR\", \"reflection_count\": 0, \"context\": []}\n",
        "                success = False\n",
        "                print(f\"  ‚ùå Agent error: {str(e)}\")\n",
        "            \n",
        "            # Get the trace_id for this request (created by langchain autolog)\n",
        "            trace_id = mlflow.get_last_active_trace_id()\n",
        "            \n",
        "            # Evaluate using MLflow's template-based LLM Scorer\n",
        "            print(f\"\\n  üìä Evaluating with MLflow make_judge (gemini/{GEMINI_MODEL})...\")\n",
        "            eval_start = time.time()\n",
        "            evaluation = evaluate_answer(\n",
        "                agent_answer, \n",
        "                sample['answer'], \n",
        "                sample['question']\n",
        "            )\n",
        "            eval_time = time.time() - eval_start\n",
        "            \n",
        "            # Log per-trace assessments (Feedback) so they appear in MLflow UI at trace level\n",
        "            if trace_id:\n",
        "                try:\n",
        "                    # Log score as feedback\n",
        "                    score_feedback = Feedback(\n",
        "                        name=\"score\",\n",
        "                        value=evaluation['score'],\n",
        "                        rationale=evaluation['explanation'][:500] if evaluation['explanation'] else None,\n",
        "                        source=AssessmentSource(source_type=\"LLM_JUDGE\", source_id=f\"gemini/{GEMINI_MODEL}\"),\n",
        "                        metadata={\"max_score\": \"5\", \"question_id\": sample['id']},\n",
        "                    )\n",
        "                    mlflow.log_assessment(trace_id=trace_id, assessment=score_feedback)\n",
        "                    \n",
        "                    # Log latency as feedback\n",
        "                    latency_feedback = Feedback(\n",
        "                        name=\"latency_seconds\",\n",
        "                        value=round(latency, 3),\n",
        "                        source=AssessmentSource(source_type=\"CODE\", source_id=\"benchmark\"),\n",
        "                    )\n",
        "                    mlflow.log_assessment(trace_id=trace_id, assessment=latency_feedback)\n",
        "                    \n",
        "                    # Log success as feedback\n",
        "                    success_feedback = Feedback(\n",
        "                        name=\"success\",\n",
        "                        value=success,\n",
        "                        source=AssessmentSource(source_type=\"CODE\", source_id=\"benchmark\"),\n",
        "                    )\n",
        "                    mlflow.log_assessment(trace_id=trace_id, assessment=success_feedback)\n",
        "                    \n",
        "                    # Log reflections count as feedback\n",
        "                    reflections_feedback = Feedback(\n",
        "                        name=\"reflections\",\n",
        "                        value=result.get('reflection_count', 0),\n",
        "                        source=AssessmentSource(source_type=\"CODE\", source_id=\"benchmark\"),\n",
        "                    )\n",
        "                    mlflow.log_assessment(trace_id=trace_id, assessment=reflections_feedback)\n",
        "                    \n",
        "                    # Log ground truth as expectation\n",
        "                    expectation = Expectation(\n",
        "                        name=\"expected_answer\",\n",
        "                        value=sample['answer'],\n",
        "                        source=AssessmentSource(source_type=\"HUMAN\", source_id=\"hotpotqa_dataset\"),\n",
        "                    )\n",
        "                    mlflow.log_assessment(trace_id=trace_id, assessment=expectation)\n",
        "                    \n",
        "                    print(f\"  üìù Logged assessments to trace: {trace_id[:20]}...\")\n",
        "                except Exception as e:\n",
        "                    print(f\"  ‚ö†Ô∏è Failed to log trace assessments: {e}\")\n",
        "            else:\n",
        "                print(f\"  ‚ö†Ô∏è No trace_id captured for this request\")\n",
        "            \n",
        "            # Store result\n",
        "            results.append({\n",
        "                \"dataset_index\": dataset_idx,\n",
        "                \"question_id\": sample['id'],\n",
        "                \"question\": sample['question'],\n",
        "                \"ground_truth\": sample['answer'],\n",
        "                \"agent_answer\": agent_answer,\n",
        "                \"complexity\": result.get('complexity', 'UNKNOWN'),\n",
        "                \"reflections\": result.get('reflection_count', 0),\n",
        "                \"context_count\": len(result.get('context', [])),\n",
        "                \"latency_seconds\": latency,\n",
        "                \"score\": evaluation['score'],\n",
        "                \"evaluation\": evaluation['explanation'][:200],\n",
        "                \"success\": success,\n",
        "                \"trace_id\": trace_id,\n",
        "            })\n",
        "            \n",
        "            # Summary for this question\n",
        "            score_emoji = \"üåü\" if evaluation['score'] >= 4 else \"‚≠ê\" if evaluation['score'] >= 3 else \"üí´\"\n",
        "            print(f\"\\n  {score_emoji} Score: {evaluation['score']}/5\")\n",
        "            print(f\"  ‚è±Ô∏è  Agent latency: {latency:.2f}s | Evaluation: {eval_time:.2f}s\")\n",
        "            print(f\"  üìÑ Context docs: {len(result.get('context', []))} | Reflections: {result.get('reflection_count', 0)}\")\n",
        "            \n",
        "            # Running stats\n",
        "            df_temp = pd.DataFrame(results)\n",
        "            elapsed = time.time() - benchmark_start\n",
        "            avg_per_q = elapsed / (i + 1)\n",
        "            remaining = avg_per_q * (actual_sample_size - i - 1)\n",
        "            print(f\"  üìà Running avg score: {df_temp['score'].mean():.2f}/5 | Est. remaining: {remaining/60:.1f}min\")\n",
        "        \n",
        "        # Create DataFrame\n",
        "        df = pd.DataFrame(results)\n",
        "        \n",
        "        total_time = time.time() - benchmark_start\n",
        "        \n",
        "        # Log run-level aggregate metrics\n",
        "        mlflow.log_metrics({\n",
        "            \"avg_score\": df['score'].mean(),\n",
        "            \"avg_latency\": df['latency_seconds'].mean(),\n",
        "            \"success_rate\": df['success'].mean(),\n",
        "            \"avg_reflections\": df['reflections'].mean(),\n",
        "            \"total_time_s\": total_time\n",
        "        })\n",
        "        \n",
        "        # Save results\n",
        "        output_file = f\"benchmark_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
        "        df.to_csv(output_file, index=False)\n",
        "        mlflow.log_artifact(output_file)\n",
        "        \n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"üèÜ BENCHMARK COMPLETE\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"  üìä Samples Evaluated: {len(df)}\")\n",
        "        print(f\"  ‚≠ê Average Score: {df['score'].mean():.2f}/5\")\n",
        "        print(f\"  ‚è±Ô∏è  Average Latency: {df['latency_seconds'].mean():.2f}s per question\")\n",
        "        print(f\"  ‚úÖ Success Rate: {df['success'].mean()*100:.1f}%\")\n",
        "        print(f\"  üîÑ Avg Reflections: {df['reflections'].mean():.1f}\")\n",
        "        print(f\"  ‚è≥ Total Time: {total_time/60:.1f} minutes\")\n",
        "        print(f\"  üíæ Results saved to: {output_file}\")\n",
        "        print(f\"  üìã Trace assessments logged for each request (view in MLflow UI ‚Üí Traces tab)\")\n",
        "        print(f\"{'='*70}\")\n",
        "        \n",
        "        return df\n",
        "\n",
        "print(\"‚úì Benchmark function defined (with per-trace assessments)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "üèÉ BENCHMARK STARTED: 10 samples (randomly selected)\n",
            "======================================================================\n",
            "Model: qwen3:30b | Max Reflections: 3\n",
            "Judge: MLflow make_judge (gemini/gemini-2.5-pro)\n",
            "Random Seed: 42 | Dataset Size: 7405\n",
            "======================================================================\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üìù Question [1/10] (idx=5238): In what year was the university where Sergei Aleksandrovich Tokarev was a professor founded?\n",
            "   Ground Truth: 1755\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "  ‚îå‚îÄ üöÄ Agent workflow started\n",
            "  ‚îÇ  Question: In what year was the university where Sergei Aleksandrovich Tokarev wa...\n",
            "  ‚îú‚îÄ üîÄ [Router] Analyzing question complexity...\n",
            "  ‚îÇ    ‚îî‚îÄ Classified as: COMPLEX (LLM: 14.46s, total: 14.46s)\n",
            "  ‚îú‚îÄ üìã [Planner] Generating search queries (iteration 1)...\n",
            "  ‚îÇ    ‚îî‚îÄ Generated 0 queries (LLM: 702.03s, total: 702.03s)\n",
            "  ‚îú‚îÄ üîç [Executor] Executing search queries...\n",
            "  ‚îÇ    ‚îî‚îÄ Total: 0 docs in context (+0 new) (0.00s)\n",
            "  ‚îú‚îÄ ü§î [Reflector] Evaluating information sufficiency (reflection #1/3)...\n",
            "  ‚îÇ    ‚îî‚îÄ üîÑ Need more info (LLM: 6.30s, total: 6.30s)\n",
            "  ‚îú‚îÄ üìã [Planner] Generating search queries (iteration 2)...\n",
            "  ‚îÇ    ‚îî‚îÄ Generated 2 queries (LLM: 53.92s, total: 53.92s)\n",
            "  ‚îÇ       [1] Sergei Aleksandrovich Tokarev professor university\n",
            "  ‚îÇ       [2] Moscow State University founded year\n",
            "  ‚îú‚îÄ üîç [Executor] Executing search queries...\n",
            "  ‚îÇ    ‚îú‚îÄ Query 1: found 2 docs, added 2 new (0.4ms)\n",
            "  ‚îÇ    ‚îú‚îÄ Query 2: found 2 docs, added 1 new (0.2ms)\n",
            "  ‚îÇ    ‚îî‚îÄ Total: 3 docs in context (+3 new) (0.00s)\n",
            "  ‚îú‚îÄ ü§î [Reflector] Evaluating information sufficiency (reflection #2/3)...\n",
            "  ‚îÇ    ‚îî‚îÄ ‚úÖ Sufficient (LLM: 11.13s, total: 11.13s)\n",
            "  ‚îú‚îÄ ‚ú® [FinalAnswer] Synthesizing final answer...\n",
            "  ‚îÇ    ‚îú‚îÄ Using 3 context documents\n",
            "  ‚îÇ    ‚îî‚îÄ Final answer synthesized (LLM: 8.58s, total: 8.58s)\n",
            "  ‚îî‚îÄ ‚úÖ Agent workflow completed in 796.53s\n",
            "\n",
            "  üìä Evaluating with MLflow make_judge (gemini/gemini-2.5-pro)...\n",
            "  üìù Logged assessments to trace: tr-8b9d2434e465e150b...\n",
            "\n",
            "  üåü Score: 5/5\n",
            "  ‚è±Ô∏è  Agent latency: 796.53s | Evaluation: 5.37s\n",
            "  üìÑ Context docs: 3 | Reflections: 2\n",
            "  üìà Running avg score: 5.00/5 | Est. remaining: 120.3min\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üìù Question [2/10] (idx=912): Black Book starred the actress and writer of what heritage?\n",
            "   Ground Truth: Dutch\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "  ‚îå‚îÄ üöÄ Agent workflow started\n",
            "  ‚îÇ  Question: Black Book starred the actress and writer of what heritage?\n",
            "  ‚îú‚îÄ üîÄ [Router] Analyzing question complexity...\n",
            "  ‚îÇ    ‚îî‚îÄ Classified as: SIMPLE (LLM: 23.95s, total: 23.95s)\n",
            "  ‚îú‚îÄ ‚ö° [DirectAnswer] Generating quick answer for simple query...\n",
            "  ‚îÇ    ‚îú‚îÄ Quick search: 3 docs (0.9ms)\n",
            "  ‚îÇ    ‚îî‚îÄ Answer generated (LLM: 24.12s, total: 24.12s)\n",
            "  ‚îî‚îÄ ‚úÖ Agent workflow completed in 48.08s\n",
            "\n",
            "  üìä Evaluating with MLflow make_judge (gemini/gemini-2.5-pro)...\n",
            "  üìù Logged assessments to trace: tr-e2acf72f9e574f7aa...\n",
            "\n",
            "  üåü Score: 5/5\n",
            "  ‚è±Ô∏è  Agent latency: 48.08s | Evaluation: 5.57s\n",
            "  üìÑ Context docs: 3 | Reflections: 0\n",
            "  üìà Running avg score: 5.00/5 | Est. remaining: 57.0min\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üìù Question [3/10] (idx=204): Which actor does American Beauty and American Beauty have in common?\n",
            "   Ground Truth: Kevin Spacey\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "  ‚îå‚îÄ üöÄ Agent workflow started\n",
            "  ‚îÇ  Question: Which actor does American Beauty and American Beauty have in common?\n",
            "  ‚îú‚îÄ üîÄ [Router] Analyzing question complexity...\n",
            "  ‚îÇ    ‚îî‚îÄ Classified as: SIMPLE (LLM: 29.10s, total: 29.10s)\n",
            "  ‚îú‚îÄ ‚ö° [DirectAnswer] Generating quick answer for simple query...\n",
            "  ‚îÇ    ‚îú‚îÄ Quick search: 3 docs (1.0ms)\n",
            "  ‚îÇ    ‚îî‚îÄ Answer generated (LLM: 9.23s, total: 9.23s)\n",
            "  ‚îî‚îÄ ‚úÖ Agent workflow completed in 38.35s\n",
            "\n",
            "  üìä Evaluating with MLflow make_judge (gemini/gemini-2.5-pro)...\n",
            "  üìù Logged assessments to trace: tr-5af305535ec42e082...\n",
            "\n",
            "  üåü Score: 5/5\n",
            "  ‚è±Ô∏è  Agent latency: 38.35s | Evaluation: 6.40s\n",
            "  üìÑ Context docs: 3 | Reflections: 0\n",
            "  üìà Running avg score: 5.00/5 | Est. remaining: 35.0min\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üìù Question [4/10] (idx=6074): Ken Pruitt  was a Republican member of an upper house of the legislature with how many members ?\n",
            "   Ground Truth: 40 members\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "  ‚îå‚îÄ üöÄ Agent workflow started\n",
            "  ‚îÇ  Question: Ken Pruitt  was a Republican member of an upper house of the legislatu...\n",
            "  ‚îú‚îÄ üîÄ [Router] Analyzing question complexity...\n",
            "  ‚îÇ    ‚îî‚îÄ Classified as: COMPLEX (LLM: 9.77s, total: 9.77s)\n",
            "  ‚îú‚îÄ üìã [Planner] Generating search queries (iteration 1)...\n",
            "  ‚îÇ    ‚îî‚îÄ Generated 3 queries (LLM: 15.08s, total: 15.08s)\n",
            "  ‚îÇ       [1] Oklahoma Senate membership size\n",
            "  ‚îÇ       [2] Ken Pruitt Oklahoma Senate member count\n",
            "  ‚îÇ       [3] Oklahoma state senate number of members\n",
            "  ‚îú‚îÄ üîç [Executor] Executing search queries...\n",
            "  ‚îÇ    ‚îú‚îÄ Query 1: found 2 docs, added 2 new (0.7ms)\n",
            "  ‚îÇ    ‚îú‚îÄ Query 2: found 2 docs, added 1 new (0.6ms)\n",
            "  ‚îÇ    ‚îú‚îÄ Query 3: found 2 docs, added 1 new (0.6ms)\n",
            "  ‚îÇ    ‚îî‚îÄ Total: 4 docs in context (+4 new) (0.00s)\n",
            "  ‚îú‚îÄ ü§î [Reflector] Evaluating information sufficiency (reflection #1/3)...\n",
            "  ‚îÇ    ‚îî‚îÄ ‚úÖ Sufficient (LLM: 12.49s, total: 12.49s)\n",
            "  ‚îú‚îÄ ‚ú® [FinalAnswer] Synthesizing final answer...\n",
            "  ‚îÇ    ‚îú‚îÄ Using 4 context documents\n",
            "  ‚îÇ    ‚îî‚îÄ Final answer synthesized (LLM: 11.38s, total: 11.38s)\n",
            "  ‚îî‚îÄ ‚úÖ Agent workflow completed in 48.74s\n",
            "\n",
            "  üìä Evaluating with MLflow make_judge (gemini/gemini-2.5-pro)...\n",
            "  üìù Logged assessments to trace: tr-5304317faf42e12f3...\n",
            "\n",
            "  üåü Score: 5/5\n",
            "  ‚è±Ô∏è  Agent latency: 48.74s | Evaluation: 6.68s\n",
            "  üìÑ Context docs: 4 | Reflections: 1\n",
            "  üìà Running avg score: 5.00/5 | Est. remaining: 23.9min\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üìù Question [5/10] (idx=2253): Between Greyia and Calibanus, which genus contains more species?\n",
            "   Ground Truth: Greyia\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "  ‚îå‚îÄ üöÄ Agent workflow started\n",
            "  ‚îÇ  Question: Between Greyia and Calibanus, which genus contains more species?\n",
            "  ‚îú‚îÄ üîÄ [Router] Analyzing question complexity...\n",
            "  ‚îÇ    ‚îî‚îÄ Classified as: COMPLEX (LLM: 30.19s, total: 30.19s)\n",
            "  ‚îú‚îÄ üìã [Planner] Generating search queries (iteration 1)...\n",
            "  ‚îÇ    ‚îî‚îÄ Generated 2 queries (LLM: 7.43s, total: 7.43s)\n",
            "  ‚îÇ       [1] Greyia genus species count\n",
            "  ‚îÇ       [2] Calibanus genus species count\n",
            "  ‚îú‚îÄ üîç [Executor] Executing search queries...\n",
            "  ‚îÇ    ‚îú‚îÄ Query 1: found 2 docs, added 2 new (0.4ms)\n",
            "  ‚îÇ    ‚îú‚îÄ Query 2: found 2 docs, added 1 new (0.3ms)\n",
            "  ‚îÇ    ‚îî‚îÄ Total: 3 docs in context (+3 new) (0.00s)\n",
            "  ‚îú‚îÄ ü§î [Reflector] Evaluating information sufficiency (reflection #1/3)...\n",
            "  ‚îÇ    ‚îî‚îÄ ‚úÖ Sufficient (LLM: 16.29s, total: 16.29s)\n",
            "  ‚îú‚îÄ ‚ú® [FinalAnswer] Synthesizing final answer...\n",
            "  ‚îÇ    ‚îú‚îÄ Using 3 context documents\n",
            "  ‚îÇ    ‚îî‚îÄ Final answer synthesized (LLM: 7.57s, total: 7.57s)\n",
            "  ‚îî‚îÄ ‚úÖ Agent workflow completed in 61.51s\n",
            "\n",
            "  üìä Evaluating with MLflow make_judge (gemini/gemini-2.5-pro)...\n",
            "  üìù Logged assessments to trace: tr-ff50bde4382567b85...\n",
            "\n",
            "  üåü Score: 5/5\n",
            "  ‚è±Ô∏è  Agent latency: 61.51s | Evaluation: 7.99s\n",
            "  üìÑ Context docs: 3 | Reflections: 1\n",
            "  üìà Running avg score: 5.00/5 | Est. remaining: 17.1min\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üìù Question [6/10] (idx=2006): Did John Updike and Tom Clancy both publish more than 15 bestselling novels?\n",
            "   Ground Truth: yes\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "  ‚îå‚îÄ üöÄ Agent workflow started\n",
            "  ‚îÇ  Question: Did John Updike and Tom Clancy both publish more than 15 bestselling n...\n",
            "  ‚îú‚îÄ üîÄ [Router] Analyzing question complexity...\n",
            "  ‚îÇ    ‚îî‚îÄ Classified as: COMPLEX (LLM: 16.41s, total: 16.41s)\n",
            "  ‚îú‚îÄ üìã [Planner] Generating search queries (iteration 1)...\n",
            "  ‚îÇ    ‚îî‚îÄ Generated 2 queries (LLM: 22.36s, total: 22.36s)\n",
            "  ‚îÇ       [1] John Updike number of bestselling novels\n",
            "  ‚îÇ       [2] Tom Clancy number of bestselling novels\n",
            "  ‚îú‚îÄ üîç [Executor] Executing search queries...\n",
            "  ‚îÇ    ‚îú‚îÄ Query 1: found 2 docs, added 2 new (0.8ms)\n",
            "  ‚îÇ    ‚îú‚îÄ Query 2: found 2 docs, added 1 new (0.7ms)\n",
            "  ‚îÇ    ‚îî‚îÄ Total: 3 docs in context (+3 new) (0.00s)\n",
            "  ‚îú‚îÄ ü§î [Reflector] Evaluating information sufficiency (reflection #1/3)...\n",
            "  ‚îÇ    ‚îî‚îÄ üîÑ Need more info (LLM: 10.87s, total: 10.87s)\n",
            "  ‚îú‚îÄ üìã [Planner] Generating search queries (iteration 2)...\n",
            "  ‚îÇ    ‚îú‚îÄ Prior context: 3 documents\n",
            "  ‚îÇ    ‚îî‚îÄ Generated 2 queries (LLM: 24.40s, total: 24.40s)\n",
            "  ‚îÇ       [1] John Updike total bestselling novels\n",
            "  ‚îÇ       [2] Tom Clancy total bestselling novels\n",
            "  ‚îú‚îÄ üîç [Executor] Executing search queries...\n",
            "  ‚îÇ    ‚îú‚îÄ Query 1: found 2 docs, added 0 new (0.4ms)\n",
            "  ‚îÇ    ‚îú‚îÄ Query 2: found 2 docs, added 2 new (0.3ms)\n",
            "  ‚îÇ    ‚îî‚îÄ Total: 5 docs in context (+2 new) (0.00s)\n",
            "  ‚îú‚îÄ ü§î [Reflector] Evaluating information sufficiency (reflection #2/3)...\n",
            "  ‚îÇ    ‚îî‚îÄ üîÑ Need more info (LLM: 24.15s, total: 24.15s)\n",
            "  ‚îú‚îÄ üìã [Planner] Generating search queries (iteration 3)...\n",
            "  ‚îÇ    ‚îú‚îÄ Prior context: 5 documents\n",
            "  ‚îÇ    ‚îî‚îÄ Generated 2 queries (LLM: 18.60s, total: 18.60s)\n",
            "  ‚îÇ       [1] John Updike number of bestselling novels\n",
            "  ‚îÇ       [2] Tom Clancy number of bestselling novels\n",
            "  ‚îú‚îÄ üîç [Executor] Executing search queries...\n",
            "  ‚îÇ    ‚îú‚îÄ Query 1: found 2 docs, added 0 new (0.8ms)\n",
            "  ‚îÇ    ‚îú‚îÄ Query 2: found 2 docs, added 0 new (0.7ms)\n",
            "  ‚îÇ    ‚îî‚îÄ Total: 5 docs in context (+0 new) (0.00s)\n",
            "  ‚îú‚îÄ ü§î [Reflector] Evaluating information sufficiency (reflection #3/3)...\n",
            "  ‚îÇ    ‚îî‚îÄ ‚ö†Ô∏è Max reflections reached, forcing completion (LLM: 15.62s, total: 15.62s)\n",
            "  ‚îú‚îÄ ‚ú® [FinalAnswer] Synthesizing final answer...\n",
            "  ‚îÇ    ‚îú‚îÄ Using 5 context documents\n",
            "  ‚îÇ    ‚îî‚îÄ Final answer synthesized (LLM: 44.70s, total: 44.70s)\n",
            "  ‚îî‚îÄ ‚úÖ Agent workflow completed in 177.16s\n",
            "\n",
            "  üìä Evaluating with MLflow make_judge (gemini/gemini-2.5-pro)...\n",
            "  üìù Logged assessments to trace: tr-287d06ca6f4cc69a4...\n",
            "\n",
            "  üí´ Score: 1/5\n",
            "  ‚è±Ô∏è  Agent latency: 177.16s | Evaluation: 11.69s\n",
            "  üìÑ Context docs: 5 | Reflections: 3\n",
            "  üìà Running avg score: 4.33/5 | Est. remaining: 13.5min\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üìù Question [7/10] (idx=1828): Who was hung for assisting the attempted surrender of a defector from the American Continental Army to the British Army?\n",
            "   Ground Truth: John Andr√©\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "  ‚îå‚îÄ üöÄ Agent workflow started\n",
            "  ‚îÇ  Question: Who was hung for assisting the attempted surrender of a defector from ...\n",
            "  ‚îú‚îÄ üîÄ [Router] Analyzing question complexity...\n",
            "  ‚îÇ    ‚îî‚îÄ Classified as: SIMPLE (LLM: 17.10s, total: 17.10s)\n",
            "  ‚îú‚îÄ ‚ö° [DirectAnswer] Generating quick answer for simple query...\n",
            "  ‚îÇ    ‚îú‚îÄ Quick search: 3 docs (1.3ms)\n",
            "  ‚îÇ    ‚îî‚îÄ Answer generated (LLM: 5.41s, total: 5.41s)\n",
            "  ‚îî‚îÄ ‚úÖ Agent workflow completed in 22.52s\n",
            "\n",
            "  üìä Evaluating with MLflow make_judge (gemini/gemini-2.5-pro)...\n",
            "  üìù Logged assessments to trace: tr-b09b2a5cbadcc32ac...\n",
            "\n",
            "  üåü Score: 5/5\n",
            "  ‚è±Ô∏è  Agent latency: 22.52s | Evaluation: 7.07s\n",
            "  üìÑ Context docs: 3 | Reflections: 0\n",
            "  üìà Running avg score: 4.43/5 | Est. remaining: 8.9min\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üìù Question [8/10] (idx=1143): which Mexican and American film actress is Ethel Houbiers  French voice of \n",
            "   Ground Truth: Salma Hayek Pinault\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "  ‚îå‚îÄ üöÄ Agent workflow started\n",
            "  ‚îÇ  Question: which Mexican and American film actress is Ethel Houbiers  French voic...\n",
            "  ‚îú‚îÄ üîÄ [Router] Analyzing question complexity...\n",
            "  ‚îÇ    ‚îî‚îÄ Classified as: SIMPLE (LLM: 10.84s, total: 10.84s)\n",
            "  ‚îú‚îÄ ‚ö° [DirectAnswer] Generating quick answer for simple query...\n",
            "  ‚îÇ    ‚îú‚îÄ Quick search: 3 docs (0.4ms)\n",
            "  ‚îÇ    ‚îî‚îÄ Answer generated (LLM: 4.81s, total: 4.81s)\n",
            "  ‚îî‚îÄ ‚úÖ Agent workflow completed in 15.66s\n",
            "\n",
            "  üìä Evaluating with MLflow make_judge (gemini/gemini-2.5-pro)...\n",
            "  üìù Logged assessments to trace: tr-b535106e122c9a560...\n",
            "\n",
            "  üåü Score: 4/5\n",
            "  ‚è±Ô∏è  Agent latency: 15.66s | Evaluation: 9.43s\n",
            "  üìÑ Context docs: 3 | Reflections: 0\n",
            "  üìà Running avg score: 4.38/5 | Est. remaining: 5.3min\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üìù Question [9/10] (idx=6033): Which major international airport in south-east England ranks as the 8th busiest airport in Europe and replaced Croydon Airport?\n",
            "   Ground Truth: Gatwick Airport\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "  ‚îå‚îÄ üöÄ Agent workflow started\n",
            "  ‚îÇ  Question: Which major international airport in south-east England ranks as the 8...\n",
            "  ‚îú‚îÄ üîÄ [Router] Analyzing question complexity...\n",
            "  ‚îÇ    ‚îî‚îÄ Classified as: COMPLEX (LLM: 11.83s, total: 11.83s)\n",
            "  ‚îú‚îÄ üìã [Planner] Generating search queries (iteration 1)...\n",
            "  ‚îÇ    ‚îî‚îÄ Generated 3 queries (LLM: 33.25s, total: 33.25s)\n",
            "  ‚îÇ       [1] Croydon Airport successor airport\n",
            "  ‚îÇ       [2] 8th busiest airport in Europe south-east England 2023\n",
            "  ‚îÇ       [3] Heathrow Airport Europe passenger traffic ranking\n",
            "  ‚îú‚îÄ üîç [Executor] Executing search queries...\n",
            "  ‚îÇ    ‚îú‚îÄ Query 1: found 2 docs, added 2 new (0.4ms)\n",
            "  ‚îÇ    ‚îú‚îÄ Query 2: found 2 docs, added 2 new (0.3ms)\n",
            "  ‚îÇ    ‚îú‚îÄ Query 3: found 2 docs, added 1 new (0.3ms)\n",
            "  ‚îÇ    ‚îî‚îÄ Total: 5 docs in context (+5 new) (0.00s)\n",
            "  ‚îú‚îÄ ü§î [Reflector] Evaluating information sufficiency (reflection #1/3)...\n",
            "  ‚îÇ    ‚îî‚îÄ ‚úÖ Sufficient (LLM: 14.97s, total: 14.97s)\n",
            "  ‚îú‚îÄ ‚ú® [FinalAnswer] Synthesizing final answer...\n",
            "  ‚îÇ    ‚îú‚îÄ Using 5 context documents\n",
            "  ‚îÇ    ‚îî‚îÄ Final answer synthesized (LLM: 15.70s, total: 15.70s)\n",
            "  ‚îî‚îÄ ‚úÖ Agent workflow completed in 75.78s\n",
            "\n",
            "  üìä Evaluating with MLflow make_judge (gemini/gemini-2.5-pro)...\n",
            "  üìù Logged assessments to trace: tr-7914c120c8dcd19f3...\n",
            "\n",
            "  üåü Score: 5/5\n",
            "  ‚è±Ô∏è  Agent latency: 75.78s | Evaluation: 8.22s\n",
            "  üìÑ Context docs: 5 | Reflections: 1\n",
            "  üìà Running avg score: 4.44/5 | Est. remaining: 2.5min\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üìù Question [10/10] (idx=839): Isabella Kelly was born at a ruined castle characterized as one of the most isolated fortifications in Britain by who?\n",
            "   Ground Truth: The Changing Scottish Landscape\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "  ‚îå‚îÄ üöÄ Agent workflow started\n",
            "  ‚îÇ  Question: Isabella Kelly was born at a ruined castle characterized as one of the...\n",
            "  ‚îú‚îÄ üîÄ [Router] Analyzing question complexity...\n",
            "  ‚îÇ    ‚îî‚îÄ Classified as: COMPLEX (LLM: 13.66s, total: 13.66s)\n",
            "  ‚îú‚îÄ üìã [Planner] Generating search queries (iteration 1)...\n",
            "  ‚îÇ    ‚îî‚îÄ Generated 2 queries (LLM: 62.09s, total: 62.09s)\n",
            "  ‚îÇ       [1] Isabella Kelly born at ruined castle characterized as most i...\n",
            "  ‚îÇ       [2] Isabella Kelly birthplace castle described as most isolated ...\n",
            "  ‚îú‚îÄ üîç [Executor] Executing search queries...\n",
            "  ‚îÇ    ‚îú‚îÄ Query 1: found 2 docs, added 2 new (0.9ms)\n",
            "  ‚îÇ    ‚îú‚îÄ Query 2: found 2 docs, added 0 new (0.7ms)\n",
            "  ‚îÇ    ‚îî‚îÄ Total: 2 docs in context (+2 new) (0.00s)\n",
            "  ‚îú‚îÄ ü§î [Reflector] Evaluating information sufficiency (reflection #1/3)...\n",
            "  ‚îÇ    ‚îî‚îÄ ‚úÖ Sufficient (LLM: 11.50s, total: 11.50s)\n",
            "  ‚îú‚îÄ ‚ú® [FinalAnswer] Synthesizing final answer...\n",
            "  ‚îÇ    ‚îú‚îÄ Using 2 context documents\n",
            "  ‚îÇ    ‚îî‚îÄ Final answer synthesized (LLM: 14.20s, total: 14.20s)\n",
            "  ‚îî‚îÄ ‚úÖ Agent workflow completed in 101.47s\n",
            "\n",
            "  üìä Evaluating with MLflow make_judge (gemini/gemini-2.5-pro)...\n",
            "  üìù Logged assessments to trace: tr-db20a56edc815fe7c...\n",
            "\n",
            "  üåü Score: 5/5\n",
            "  ‚è±Ô∏è  Agent latency: 101.47s | Evaluation: 4.15s\n",
            "  üìÑ Context docs: 2 | Reflections: 1\n",
            "  üìà Running avg score: 4.50/5 | Est. remaining: 0.0min\n",
            "\n",
            "======================================================================\n",
            "üèÜ BENCHMARK COMPLETE\n",
            "======================================================================\n",
            "  üìä Samples Evaluated: 10\n",
            "  ‚≠ê Average Score: 4.50/5\n",
            "  ‚è±Ô∏è  Average Latency: 138.58s per question\n",
            "  ‚úÖ Success Rate: 100.0%\n",
            "  üîÑ Avg Reflections: 0.9\n",
            "  ‚è≥ Total Time: 24.3 minutes\n",
            "  üíæ Results saved to: benchmark_results_20251129_215506.csv\n",
            "  üìã Trace assessments logged for each request (view in MLflow UI ‚Üí Traces tab)\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Run the benchmark\n",
        "# Uses MLflow's template-based LLM Scorer (make_judge) for evaluation\n",
        "# Cases are randomly selected from the dataset (controlled by RANDOM_SEED for reproducibility)\n",
        "benchmark_results = run_benchmark(dataset, sample_size=SAMPLE_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>agent_answer</th>\n",
              "      <th>score</th>\n",
              "      <th>latency_seconds</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In what year was the university where Sergei A...</td>\n",
              "      <td>1755</td>\n",
              "      <td>Based solely on the provided retrieved informa...</td>\n",
              "      <td>5</td>\n",
              "      <td>796.530573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Black Book starred the actress and writer of w...</td>\n",
              "      <td>Dutch</td>\n",
              "      <td>Dutch</td>\n",
              "      <td>5</td>\n",
              "      <td>48.082319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Which actor does American Beauty and American ...</td>\n",
              "      <td>Kevin Spacey</td>\n",
              "      <td>Kevin Spacey</td>\n",
              "      <td>5</td>\n",
              "      <td>38.347693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Ken Pruitt  was a Republican member of an uppe...</td>\n",
              "      <td>40 members</td>\n",
              "      <td>Based solely on the provided retrieved informa...</td>\n",
              "      <td>5</td>\n",
              "      <td>48.739338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Between Greyia and Calibanus, which genus cont...</td>\n",
              "      <td>Greyia</td>\n",
              "      <td>Based solely on the provided retrieved informa...</td>\n",
              "      <td>5</td>\n",
              "      <td>61.506555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Did John Updike and Tom Clancy both publish mo...</td>\n",
              "      <td>yes</td>\n",
              "      <td>Based solely on the retrieved information prov...</td>\n",
              "      <td>1</td>\n",
              "      <td>177.159528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Who was hung for assisting the attempted surre...</td>\n",
              "      <td>John Andr√©</td>\n",
              "      <td>John Andr√©</td>\n",
              "      <td>5</td>\n",
              "      <td>22.517682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>which Mexican and American film actress is Eth...</td>\n",
              "      <td>Salma Hayek Pinault</td>\n",
              "      <td>Salma Hayek</td>\n",
              "      <td>4</td>\n",
              "      <td>15.664922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Which major international airport in south-eas...</td>\n",
              "      <td>Gatwick Airport</td>\n",
              "      <td>Based solely on the provided retrieved informa...</td>\n",
              "      <td>5</td>\n",
              "      <td>75.777518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Isabella Kelly was born at a ruined castle cha...</td>\n",
              "      <td>The Changing Scottish Landscape</td>\n",
              "      <td>Based solely on the retrieved information prov...</td>\n",
              "      <td>5</td>\n",
              "      <td>101.473732</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question  \\\n",
              "0  In what year was the university where Sergei A...   \n",
              "1  Black Book starred the actress and writer of w...   \n",
              "2  Which actor does American Beauty and American ...   \n",
              "3  Ken Pruitt  was a Republican member of an uppe...   \n",
              "4  Between Greyia and Calibanus, which genus cont...   \n",
              "5  Did John Updike and Tom Clancy both publish mo...   \n",
              "6  Who was hung for assisting the attempted surre...   \n",
              "7  which Mexican and American film actress is Eth...   \n",
              "8  Which major international airport in south-eas...   \n",
              "9  Isabella Kelly was born at a ruined castle cha...   \n",
              "\n",
              "                      ground_truth  \\\n",
              "0                             1755   \n",
              "1                            Dutch   \n",
              "2                     Kevin Spacey   \n",
              "3                       40 members   \n",
              "4                           Greyia   \n",
              "5                              yes   \n",
              "6                       John Andr√©   \n",
              "7              Salma Hayek Pinault   \n",
              "8                  Gatwick Airport   \n",
              "9  The Changing Scottish Landscape   \n",
              "\n",
              "                                        agent_answer  score  latency_seconds  \n",
              "0  Based solely on the provided retrieved informa...      5       796.530573  \n",
              "1                                              Dutch      5        48.082319  \n",
              "2                                       Kevin Spacey      5        38.347693  \n",
              "3  Based solely on the provided retrieved informa...      5        48.739338  \n",
              "4  Based solely on the provided retrieved informa...      5        61.506555  \n",
              "5  Based solely on the retrieved information prov...      1       177.159528  \n",
              "6                                         John Andr√©      5        22.517682  \n",
              "7                                        Salma Hayek      4        15.664922  \n",
              "8  Based solely on the provided retrieved informa...      5        75.777518  \n",
              "9  Based solely on the retrieved information prov...      5       101.473732  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Display results\n",
        "if 'benchmark_results' in dir() and benchmark_results is not None:\n",
        "    display(benchmark_results[['question', 'ground_truth', 'agent_answer', 'score', 'latency_seconds']])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
